{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af04746",
   "metadata": {},
   "source": [
    "# 机器学习相关\n",
    "\n",
    "## 机器学习\n",
    "- 介绍一个最熟悉的机器学习算法\n",
    "- 决策树怎么建树，基尼系数公式\n",
    "- Adaboost拟合目标是什么\n",
    "- Adaboost介绍一下，每个基学习器的权重怎么得到的\n",
    "- 介绍下GBDT\n",
    "- 介绍XGBoost\n",
    "- 介绍下LightGBM\n",
    "- LightGBM相对于XGBoost的改进\n",
    "- GBDT中的梯度是什么，怎么用\n",
    "- GBDT如何计算特征重要性\n",
    "- GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差\n",
    "- 介绍XGBoost中的并行\n",
    "- 介绍XGBoost中精确算法与近似算法\n",
    "- XGBoost如何处理空缺值，为何要进行行采样、列采样\n",
    "- 讲一下xgboost算法，xgboost是如何处理离散特征的，xgb怎么训练，xgb算法优点，怎么选特征，主要参数有哪些，xgb的特征重要性怎么看\n",
    "- xgboost介绍一下，xgb对目标函数二阶泰勒展开，哪个是x，哪个是delta x, 一阶导和二阶导是对谁求得\n",
    "- 为什么高维稀疏数据，LR比GBDT要好\n",
    "- 随机森林与GBDT采样的区别\n",
    "- 随机森林中列采样的作用\n",
    "- bagging与boosting对比, boosting和bagging的区别及分别适用于什么场景\n",
    "- bagging与boosting分别从什么角度降低过拟合\n",
    "- 逻辑回归如何避免过拟合\n",
    "- 推导逻辑回归损失函数和损失函数求导\n",
    "- 正则化项L1和L2为什么有用\n",
    "- l1正则不可导，如何优化\n",
    "- 什么样的特征容易产生比较小的权重\n",
    "- 随机森林采样n次，n趋于无穷大，oob样本的概率接近于？\n",
    "- 逻辑回归与树模型的优缺点\n",
    "- 对于高维稀疏数据，树模型能训练吗？一般怎么处理\n",
    "- 树模型一般有哪些参数，分别有什么作用\n",
    "- 随机森林如何处理空缺值\n",
    "- 介绍kmeans，与其他聚类算法的对比\n",
    "- 机器学习导致误差的原因？过拟合、欠拟合对应的偏差和方差是怎样的？\n",
    "- 如何解决过拟合问题？哪些角度\n",
    "- LR的原理，问题的假设，为什么用交叉熵作为损失函数\n",
    "- LR损失函数写一下\n",
    "- LR是不是凸优化问题，如何判断LR达到最优值\n",
    "- LR一般用什么数据，一般有什么特点(离散数据，离散化的一堆优点)\n",
    "- LR，SVM, xgboost如何防止过拟合\n",
    "- lr和树模型，离散特征和连续特征分别怎么处理\n",
    "- lr和线性回归的区别\n",
    "- 连续特征可以直接输入到lr中不? （归一化和标准化有什么区别）\n",
    "- 线性回归可以求闭式解，逻辑回归可以吗，为什么，LR用什么求解参数，为什么要用梯度下降算法\n",
    "- SVM和LR的区别\n",
    "- SVM的公式会推导嘛，SVM的损失函数\n",
    "- SVM原理，为什么求最大间隔，为什么用核函数，常见的核函数及区别\n",
    "- SVM支持向量怎么得到的\n",
    "- 写一下SVM的原问题和对偶问题，分别解释一下\n",
    "- SVM核函数有什么性质，写一下SVM核化的形式\n",
    "- 无监督学习，半监督学习，有监督学习的区别\n",
    "- 有哪些无监督学习的方法（kmeans,pca,生成模型，自编码器）\n",
    "- 有哪些回归模型（多项式回归，树模型，svr, 神经网络）\n",
    "- 生成模型、判别模型的区别\n",
    "- 概率和似然的区别\n",
    "- 最大似然估计和后验概率的区别，分别用LR来推导损失函数的话有什么区别（乘以W的先验概率）\n",
    "- 朴素贝叶斯介绍，朴素贝叶斯公式，为什么朴素\n",
    "- l1,l2特性及原理，分别适用于那些场合\n",
    "- 给一个多峰数据场景，为什么l2不适合，可以怎么解决\n",
    "- 讲讲Kmeans、EM算法\n",
    "- 机器学习中怎么解决过拟合，DNN中怎么解决\n",
    "- 说一下SVD怎么降维\n",
    "- 推导softmax做激活函数求导\n",
    "- LR，SVM,xgb哪个对样本不平衡不太敏感，顺便把SVM和xgb介绍了\n",
    "- 降维方法了解嘛，PCA? 为什么取特征值前k大的对应的特征向量组成的矩阵？低秩表示\n",
    "\n",
    "\n",
    "## 深度学习\n",
    "\n",
    "- 梯度是什么，hessian矩阵怎么求\n",
    "- 有没有上过凸优化的课程，如何判断凸函数\n",
    "- 防止过拟合的策略有哪些\n",
    "- dropout怎么防止过拟合, Dropout在训练和测试区别\n",
    "- BN介绍，BN怎么防止过拟合，怎么用的，参数量, 参数怎么得到的\n",
    "- 优化器，SGD与Adam的异同点\n",
    "- 介绍一下你了解的激活函数以及优缺点\n",
    "- 介绍一下深度学习的优化器发展史, 及常见的优化算法\n",
    "- 梯度爆炸和梯度消失问题\n",
    "- SGD缺点，已经有什么改进的优化器\n",
    "- 网络权重初始化为0有什么影响，初始化为一个非0的常数呢？\n",
    "- embedding如何设置维度？越大越好还是越小越好？\n",
    "- transformer中计算attention除于根号d的作用\n",
    "- embedding如何训练\n",
    "- 介绍下attention，相比cnn、lstm的优势\n",
    "- word2vec如何进行负采样\n",
    "- word2vec两种训练方法的区别，具体损失函数\n",
    "- 介绍LSTM每一个门的具体操作，一个LSTM cell的时间复杂度是多少\n",
    "- transformer中encoder和decoder的输入分别是什么\n",
    "- transformer中encoder与decoder的QKV矩阵如何产生\n",
    "- transformer中QKV矩阵是否可以设置成同一个\n",
    "- transformer与bert的位置编码有什么区别\n",
    "- BERT中计算attention的公式\n",
    "- BERT中LayerNorm的作用，为什么不用BN？\n",
    "- BERT中的两种预训练任务介绍\n",
    "- 深度学习中BN的好处？最早提出BN是为了解决什么问题？BN具体怎么实现的\n",
    "- 激活函数中，sigmoid，tanh有什么不好的地方？relu有什么优势？\n",
    "- pagerank相比于tf-idf的优势\n",
    "- 画一下LSTM的结构图\n",
    "- RNN和LSTM的区别，解决了什么问题，为什么解决了梯度消失的问题\n",
    "- 深度模型和传统机器学习模型对数据量的要求，什么场景用什么模型\n",
    "\n",
    "\n",
    "## 特征工程\n",
    "- 特征工程一般怎么做\n",
    "- 特征数值分布比较稀疏如何处理\n",
    "- 正负样本不均衡如何处理\n",
    "- 连续特征离散化的作用\n",
    "- 对id类特征onehot导致维度过高，如何处理？\n",
    "- 如何进行特征筛选\n",
    "- DNN能做特征交叉嘛\n",
    "- 海量类别特征该如何处理，有什么方法\n",
    "- pearson系数\n",
    "- 归一化和标准化有什么区别\n",
    "- 如果不使用最近邻检索的库，你会怎么做最近邻检索\n",
    "\n",
    "\n",
    "## 评估指标\n",
    "\n",
    "- auc的含义和计算方法, 有没有更快的计算方法\n",
    "- AUC会不会出现小于0.5的情况，出现了怎么调bug\n",
    "- AUC为1可能是由什么导致的？\n",
    "- 分类评估指标中，F1和AUC有什么区别\n",
    "- 分类指标用的什么，哪个分类指标对正负样本分布不敏感\n",
    "- 如果对负样本进行采样，auc的计算结果会发生变化吗\n",
    "- 交叉熵跟MSE有什么区别\n",
    "- micro-f1解释\n",
    "- 介绍下排序指标ndcg\n",
    "- 回归指标应该用什么\n",
    "- AUC和precision,recall，F1的区别，不同情况怎么选择指标\n",
    "- Group auc了解嘛\n",
    "- 给数据计算AUC\n",
    "- 分类评价指标，TPR,FPR等的含义\n",
    "\n",
    "\n",
    "## 参考解析\n",
    "\n",
    "（解析仅供参考，如果有错误、补充或者建议欢迎Issue，部分方向题目笔者还不熟悉不确定答案的没有更新）\n",
    "\n",
    "### 机器学习\n",
    "\n",
    "- 介绍一个最熟悉的机器学习算法\n",
    "\n",
    "  - LR：逻辑回归是假设数据服从**伯努利分布**，通过**极大似然估计方法**，使用**梯度下降**来求解参数，达到二分类目的的一个模型。我们在考虑把广义线性模型用于分类的时候，需要如何确定逻辑边界，感知机模型用的是阶跃函数，但是阶跃函数不可导，不能作为广义线性模型的联系函数。逻辑回归对数几率函数代替阶跃函数。因为对数几率函数是单调可微的一个函数，所以可以作为联系函数。所以逻辑回归本质上还是广义线性模型。\n",
    "\n",
    "  - LR的优缺点：\n",
    "    1. 形式简单，可解释性好；\n",
    "    2. 它直接对分类概率进行建模，不需要知道真实数据的分布，这和生成式模型相区别，避免了假设错误带来的问题；\n",
    "    3. 不仅能够预测出类别，还能够预测出概率，能够用于很多场景，比如ctr排序中；\n",
    "    4. 对数几率函数任意阶数可导，能够很容易优化；\n",
    "    5. 可以获得特征权重，方便我们进行特征筛选；\n",
    "    6. 训练速度快；\n",
    "    7. 它对稀疏特征效果比较好，因为使用的是w1 w2 w3本质上的线性模型，稀疏数据能够筛选出不稀疏的重要特征。\n",
    "    8. 模型表达能力有限；\n",
    "    9. 样本不均衡很难处理；\n",
    "    10. 在非线性可分数据集上性能有限；\n",
    "\n",
    "  - LR推导：\n",
    "    $$\n",
    "    \\text{在伯努利分布下，假设样本概率为：}\n",
    "    f(n)=\n",
    "      \\begin{cases}\n",
    "        p, & y = 1\\\\\n",
    "        1-p,& y = 0\n",
    "      \\end{cases}\n",
    "      \\\\ \\Rightarrow p(y_i|xi) = p^{y_i}(1-p)^{1-{y_i}}\n",
    "      \\\\ \\Rightarrow \\text{在总体上有：} P = \\prod_{n=1}^N p^{y_n}(1-p)(1-{y_n})\n",
    "      \\\\ \\Rightarrow \\text{连乘求导很复杂，我们用单调函数ln化为连加：}F(w) = ln(P) = \\sum_{n=1}^N(y_nln(p) + (1-y_n)ln(1-p))\n",
    "      \\\\ \\Rightarrow \\text{其中}p = \\frac {1}{1+e^{-w^Tx}}\n",
    "    $$\n",
    "    \n",
    "- 决策树怎么建树，基尼系数公式\n",
    "  - 决策树建树算法有三种ID3、C4.5、CART，每个算法主要考虑的事情主要有三个问题：\n",
    "    1. 选什么特征来当条件？\n",
    "    2. 条件判断的属性值是什么？\n",
    "    3. 什么时候停止分裂，达到我们需要的决策？\n",
    "\n",
    "  - CART树采用基尼系数进行最优特征的选择，构造过程中假设有K类，则样本属于第K类的概率为pk，则定义样本分布的**基尼系数**为：\n",
    "    $$\n",
    "    Gini(p) = \\sum_{k=1}^mp_k(1-p_k) = 1-\\sum_{k=1}^Kp_k^2\n",
    "    $$    \n",
    "    根据基尼系数定义，可以得到样本集合D的**基尼指数**，其中ck表述样本集合中第k类的子集：\n",
    "    $$\n",
    "    Gini(D) = 1 - \\sum_{k=1}^K(\\frac{|C_k|}{D})^2\n",
    "    $$\n",
    "    如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。\n",
    "    $$\n",
    "    Gain\\_Gini(D, A) = \\frac{D1}{D}Gini(D_1) + \\frac{D2}{D}Gini(D_2)\n",
    "    $$\n",
    "    这里回答前两个问题：    \n",
    "      - 选什么特征作为最优特征分割：当我们计算完所有特征基尼指数后，选择其中最小所在特征的作为分裂特征；\n",
    "      - 条件判断的属性值是什么：判断特征属性是否为为此最指数来分裂；\n",
    "      - 什么时候停止分裂，达到我们需要的决策：分裂的最小收益小于我们的划定的阈值，或者树的深度达到我们的阈值。\n",
    "\n",
    "- Adaboost拟合目标是什么\n",
    "  - Adaboost中每训练完一个弱分类器都就会调整权重，上一轮训练中被误分类的点的权重会增加，在本轮训练中，由于权重影响，本轮的弱分类器将更有可能把上一轮的误分类点分对，如果还是没有分对，那么分错的点的权重将继续增加，下一个弱分类器将更加关注这个点，这是adaboost目标。\n",
    "\n",
    "- Adaboost介绍一下，每个基学习器的权重怎么得到的\n",
    "  - Adaboost采用boosting算法中的一个经典算法。Boosting是集成学习中的一种方法提升是一个逐步优化集成学习器的过程，即每个个体学习器都在弥补集成学习器的欠缺，从而达到整体的优化。Adaboost通过每次降低个体学习器的分类误差，加大效果好的个体学习器的重要性，得到最终的集成学习器，它的损失函数是指数损失函数。\n",
    "  - 算法流程：\n",
    "    1. 初始化训练样本的权值 ![[公式]](https://www.zhihu.com/equation?tex=D_0) 。则每一个训练样本的权重被初始化为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7D) ，其中 m 为样本的数量。\n",
    "    2. 迭代训练弱分类器。在迭代过程中，需要对样本权重 ![[公式]](https://www.zhihu.com/equation?tex=D_i) 进行更新。如果某个样本点已经被准确地分类，那么在训练下一个弱分类器时，就会降低它的权值；相反，如果该个样本点没有被准确地分类，就会提高它的权值。\n",
    "    3. 将各个弱分类器加权平均得到强分类器。误差率 e 低的弱分类器权重 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 较大，误差率 e 高的弱分类器权重 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 较小。\n",
    "\n",
    "  - 数据权重：Adaboost算法中有两种权重，一种是数据的权重，另一种是弱分类器的权重。其中，数据的权重主要用于弱分类器寻找其分类误差最小的决策点，找到之后用这个最小误差计算出该弱分类器的权重（发言权），分类器权重越大说明该弱分类器在最终决策时拥有更大的发言权。如果训练数据保持不变，那么在数据的某个特定维度上单层决策树找到的最佳决策点每一次必然都是一样的，为什么呢？因为单层决策树是把所有可能的决策点都找了一遍然后选择了最好的，如果训练数据不变，那么每次找到的最好的点当然都是同一个点了。\n",
    "\n",
    "  - 基学习器权重：\n",
    "    $$\n",
    "    ak = \\frac{1}{2}log\\frac{1-e_k}{e_k}\n",
    "    $$\n",
    "    其中，k是第k次迭代，ek是第k次迭代的误差，在二分类问题中，每个样本被学习器分为为1，分错为0。ak代表了此学习器在最终集成学习器中的重要性，当分类器ek越大，ak越小，此学习器话语权越小。ak理论上界是正无穷，有些改进的adaboost会加入softmax思想修正权重。\n",
    "\n",
    "- 介绍下GBDT\n",
    "\n",
    "  - gbdt 是通过采用加法模型（即**基函数的线性组合**），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法， gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的**残差**基础上进行训练。\n",
    "  - GBDT中的树是回归树（不是分类树），默认选择CART回归树，GBDT用来做回归预测，调整后也可以用于分类。\n",
    "  - 核心思想是**利用损失函数的负梯度在当前模型的值作为残差的近似值**，本质上是**对损失函数进行一阶泰勒展开**，从而拟合一个回归树。\n",
    "\n",
    "- 介绍XGBoost\n",
    "\n",
    "  - XGBoost是的陈天奇开源一种梯度提升树模型，是GBDT的一种工程实现。与GBDT最大的区别就是**树的生成方式不同**，加快了树的生成过程，以生成最优树。\n",
    "  - XGBT相对于GBDT的优化（sklearn中的GBDT实现和传统的有一定改进，同样支持XGBT的一些特性，这里的对比只针对传统GBDT）：\n",
    "    - **正则项**：XGBT加入了正则项，控制模型复杂度，防止过拟合，加入的有叶子结点个数正则化、叶子结点输出L2正则化；\n",
    "    - **二阶泰勒展开**：XGBT采用牛顿法，对损失函数进行了二阶泰勒展开，加速收敛速度；\n",
    "    - **支持更多基学习器**：GBDT只支持CART树，XGBT支持多种基学习器，比如线性分类器；\n",
    "    - **行采样**：传统GBDT每一轮迭代都使用了全部数据，XGBT使用了行采样；\n",
    "    - **列采样**：传统GBDT同样没有使用列采样，XGBT引入了列采样；\n",
    "    - **缺失值处理**：GBDT没有缺失值处理机制，XGBT支持缺失值处理；\n",
    "    - **Shrinkage**：对每一颗树输出进行衰减，削弱单颗树影响，让后续树有更大学习空间；\n",
    "    - **并行化**：特征粒度的并行化，而非树粒度的，在不同特征上采用多线程并行计算最优分割点；\n",
    "\n",
    "- 介绍下LightGBM\n",
    "\n",
    "  - LightGBM是微软开源的一个梯度Boosting框架，使用基于决策树的学习算法，是GBDT的一种工程实现，特点是**快**。\n",
    "  - LGB相对于XGBT的改进：\n",
    "    - 基于**直方图的决策树算法**：把特征离散到K个bin中构造直方图，遍历一遍特征统计直方图，最后根据直方图寻找最优分割点，这样做的好处是计算速度更快，内存占用更小；\n",
    "    - **直方图做差加速**：计算兄弟节点的直方图，只需要用父节点直方图-本节点直方图。这样做速度提升了一倍；\n",
    "    - **Leaf-wise叶子生长策略**：XGBT的Level-wise每次分裂一层节点，容易并行化，但是更容易过拟合，Leaf-wise值分裂增益最大的节点，相对精度更高，过拟合更小；\n",
    "    - 直接**支持类别特征**：第一个直接支持类别特征的GBDT工具，具体算法《On Grouping For Maximum Homogeneity》；\n",
    "    - **高效并行优化**：数据量小采用特征并行、数据并行，数据量大采用投票并行；\n",
    "    - **Cache优化**：直方图算法天生提高缓存命中，降低内存消耗；\n",
    "    - **单边梯度抽样算法**：过滤梯度小的样本，同时平衡了数据分布的改变，这个算法能够提升计算速度；\n",
    "\n",
    "- LightGBM相对于XGBoost的改进\n",
    "\n",
    "  - 如上\n",
    "\n",
    "- GBDT中的梯度是什么，怎么用\n",
    "\n",
    "  - 在线性模型优化的过程中。利用**梯度下降**我们总是让参数**向负梯度的方向移动**，使损失函数最小。gbdt，假入我们现在有 t 课树，我们需要去学习是第 t+1 颗树，那么如何学习第 t+1 颗树才是最优的树呢？ 这个时候我们参考梯度优化的思想。现在的 t 课树就是我们现在的状态使用这个状态我们可以计算出现在的损失。如何让损失更小呢？我们只需要让 t+1 颗树去拟合损失的负梯度。而**残差** 是梯度在MSE为损失函数下的特例（MSE的导数就是残差）。 \n",
    "\n",
    "- GBDT如何计算特征重要性\n",
    "\n",
    "  - 所有集成树可以通过单颗树的特征重要度平均来计算；\n",
    "  - 单颗树的特征重要度（用CART树举例）有几种算法（重点要和面试官介绍处第一种）：\n",
    "    - 可以通过基尼系数特征分裂收益计算重要性：训练过程中通过记录特征的分裂总次数、总/平均信息增益来对特征重要性进行量化。例如实际工程中我们会用特征在整个GBDT、XgBoost里面被使用的次数或者带来的总/平均信息增益来给特征重要度打分，最后进行排序。由于本身Ensemble模型在选择特征分裂时带有一定随机性，一般会跑多个模型然后把特征重要性求平均后排序。\n",
    "    - 通过OOB数据来计算：训练好模型之后，用Out of Bag（或称Test）数据进行特征重要性的量化计算。具体来说，先用训练好的模型对OOB数据进行打分，计算出AUC或其他业务定义的评估指标；接着对OOB数据中的每个特征：\n",
    "      （1）随机shuffle当前特征的取值；\n",
    "      （2）重新对当前数据进行打分，计算评估指标；\n",
    "      （3）计算指标变化率\n",
    "\n",
    "- GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差\n",
    "\n",
    "  - 核心思想是**利用损失函数的负梯度在当前模型的值作为残差的近似值**，本质上是**对损失函数进行一阶泰勒展开**，从而拟合一个回归树。\n",
    "  - 而**残差** 是梯度在MSE为损失函数下的特例（MSE的导数就是残差）。 \n",
    "\n",
    "- 介绍XGBoost中的并行\n",
    "\n",
    "  - 如上\n",
    "\n",
    "- 介绍XGBoost中精确算法与近似算法\n",
    "\n",
    "  - 指的是在计算特征分裂的时候，XGBT使用了近似算法\n",
    "  - 精确算法：通过列举所有特征的可能划分找到最优划分解来生成树，该方法**需要排序以形成连续的特征，之后计算每个可能的梯度统计值**。\n",
    "    - 缺点：在数据量非常大的情况下，精确基本用不了。一方面在生成树的过程中，每次寻找一个节点最佳分割点时，都需要比较其在所有特征的**所有分割点**上的效果，这么做时间复杂度很高；另一方面，在每次对某个特征进行分割的时候，需要对所有样本根据该特征值进行排序，这需要我们把所有的数据存储在内存中，这会给硬件方面带来很大压力。\n",
    "  - 近似算法：在针对一个特征寻找分割点的时候，我们其实对特征中的值的范围不敏感，只对这些值的顺序敏感。比如数据集中的样本的某一个特征出现的值有12、15、82、107，但是如果我们把这四种值分别替换成1、2、3、4，最后得到的树的结构是不变的。利用这种思想，给出一个数据集中样本的第k个特征和样本点在损失函数上的二阶导数所组成的集合，随后利用数据分布的百分比来定义一个排名函数 ，这个排名函数代表了特征k的值小于z的样本占总样本的比例。我们的目标就是利用这个排名函数来寻找候选分割点集合。\n",
    "\n",
    "- XGBoost如何处理空缺值，为何要进行行采样、列采样\n",
    "\n",
    "  - 将缺失值分别划分到左子树和右子树，分别计算出左子树和右子树的增益 ，选出更大的，将该方向作为缺失值的分裂方向（记录下来，预测阶段将会使用）；LGB使用相同的方法；\n",
    "  - 降低了过拟合；\n",
    "\n",
    "- 讲一下xgboost算法，xgboost是如何处理离散特征的，xgb怎么训练，xgb算法优点，怎么选特征，主要参数有哪些，xgb的特征重要性怎么看\n",
    "\n",
    "  - 如上\n",
    "\n",
    "- xgboost介绍一下，xgb对目标函数二阶泰勒展开，哪个是x，哪个是delta x, 一阶导和二阶导是对谁求得\n",
    "\n",
    "  - （题干没咋看懂，猜测一下问的可能是xgbt使用牛顿法为优化器）\n",
    "\n",
    "- 为什么高维稀疏数据，LR比GBDT要好\n",
    "\n",
    "  - 树模型对稀疏特征，切分的收益非常小，只能从少量非0信息上学习；\n",
    "  - 线性模型的正则项是对权重惩罚，树模型是对深度、叶子个数的惩罚。所以高维稀疏数据中，少量样本会对结果产生非常大的影响，非常容易过拟合，而线性模型的权重惩罚能够很好处理这一点。综上，带正则化的线性模型比较不容易对稀疏特征过拟合；\n",
    "  - 同样的原因可以解释为什么onehot不适合树模型；\n",
    "\n",
    "- 随机森林与GBDT采样的区别\n",
    "\n",
    "  - RF采用了行列采样，传统GBDT算法没有采用（SkLearn中集成了采样）；\n",
    "\n",
    "- 随机森林中列采样的作用\n",
    "\n",
    "  - 随机森林在bagging基础上，进一步在训练过程引入随机属性选择，从全集d中随机选择k个属性的子集，利用这个子集来建立本颗子树，下一轮同理；推荐的k=log2d；\n",
    "  - 能够有效降低过拟合风险，降低方差；\n",
    "\n",
    "- bagging与boosting对比, boosting和bagging的区别及分别适用于什么场景\n",
    "\n",
    "  - boosting：串行的方式训练基分类器，各分类器之间有依赖。每次训练时，对前一层基分类器分错的样本给与更高的权重，更多的关注的是偏差；\n",
    "  - bagging：是Bootstrap aggregating的意思，各分类器之间无强依赖，可以并行，最终结果进行投票（分类），或者平均（回归）；\n",
    "\n",
    "  - 样本选择上：\n",
    "    - Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。\n",
    "    - Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。\n",
    "\n",
    "  - 样例权重：\n",
    "    - Bagging：使用均匀取样，每个样例的权重相等。\n",
    "    - Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。\n",
    "\n",
    "  - 预测函数：\n",
    "    - Bagging：所有预测函数的权重相等。\n",
    "    - Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。\n",
    "\n",
    "  - 并行计算：\n",
    "    - Bagging：各个预测函数可以并行生成。\n",
    "    - Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。\n",
    "\n",
    "- bagging与boosting分别从什么角度降低过拟合\n",
    "\n",
    "  - bagging降低方差，boosting降低偏差\n",
    "\n",
    "- 逻辑回归如何避免过拟合\n",
    "\n",
    "  - 数据层面：更多数据集、数据增强、更多特征、数据中注入噪音；\n",
    "  - 模型层面：权重衰减正则化（L1、L2）；\n",
    "  - 训练层面：提前终止；\n",
    "\n",
    "  - 推导逻辑回归损失函数和损失函数求导\n",
    "\n",
    "  - 推导损失函数：\n",
    "    $$\n",
    "    \\text{在伯努利分布下，假设样本概率为：}\n",
    "    f(n)=\n",
    "    \t\\begin{cases}\n",
    "    \t\tp, & y = 1\\\\\n",
    "    \t\t1-p,& y = 0\n",
    "    \t\\end{cases}\n",
    "    \t\\\\ \\Rightarrow p(y_i|xi) = p^{y_i}(1-p)^{1-{y_i}}\n",
    "    \t\\\\ \\Rightarrow \\text{在总体上有：} P = \\prod_{n=1}^N p^{y_n}(1-p)(1-{y_n})\n",
    "    \t\\\\ \\Rightarrow \\text{连乘求导很复杂，我们用单调函数ln化为连加：}F（w） = ln(P) = \\sum_{n=1}^N(y_nln(p) + (1-y_n)ln(1-p))\n",
    "    \t\\\\ \\Rightarrow \\text{其中}p = \\frac {1}{1+e^{-w^Tx}}\n",
    "    $$\n",
    "    \n",
    "\n",
    "- 正则化项L1和L2为什么有用\n",
    "\n",
    "  - L1正则化，相当于为模型添加了一个先验知识，就是权重矩阵W服从均值为0的拉普拉斯分布，L2正则化，相当于为模型添加了一个正态分布先验知识，就是权重矩阵W服从均值为0的正态分布。\n",
    "  - L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。\n",
    "\n",
    "- l1正则不可导，如何优化\n",
    "\n",
    "  - 在不可导处无法进行梯度下降，此时采用坐标轴下降法：坐标轴下降法是沿着坐标轴的方向，每次固定m-1个数值，对最后一个数值求局部最优解，迭代m次（证明：凸函数在每一个维度都取得最小值，则此处就是全局最小值）；\n",
    "\n",
    "- 什么样的特征容易产生比较小的权重\n",
    "\n",
    "  - （方差较大的权重？）\n",
    "\n",
    "- 随机森林采样n次，n趋于无穷大，oob样本的概率接近于？\n",
    "\n",
    "  - 1/e\n",
    "\n",
    "  - 推到过程根据基本不等式：\n",
    "    $$\n",
    "    lim_{x\\to\\infty}(1+1/x)^x = e\n",
    "    $$\n",
    "\n",
    "- 逻辑回归与树模型的优缺点\n",
    "\n",
    "  - 树模型\n",
    "    - 可解释性强，比线性模型还强\n",
    "    - 拟合能力更强，特别是对非线性数据；\n",
    "    - 但是容易过拟合；\n",
    "\n",
    "- 对于高维稀疏数据，树模型能训练吗？一般怎么处理\n",
    "\n",
    "  - 能训练，但是效果不好，容易过拟合；\n",
    "\n",
    "- 树模型一般有哪些参数，分别有什么作用\n",
    "\n",
    "  - num_leaves: 最大叶子节点个数，控制过拟合\n",
    "  - max_depth：最大深度，控制过拟合\n",
    "  - learning_rate：学习率\n",
    "  - min_split_gain：最小分裂的收益：控制过拟合\n",
    "\n",
    "  - 随机森林如何处理空缺值\n",
    "\n",
    "  - 随机森林本身没有处理空缺值算法，有些实现中附带了处理空缺值算法；\n",
    "  - 数值变量用中位数、类别变量用众数；\n",
    "  - 利用无空缺的变量计算相似度后加权计算，类别变量用加权投票，数值变量加权平均；\n",
    "\n",
    "- 介绍kmeans，与其他聚类算法的对比\n",
    "\n",
    "  - K-means 是我们最常用的基于欧式距离的聚类算法，其认为两个目标的距离越近，相似度越大；\n",
    "  - 所以 K-means 的算法步骤为：\n",
    "    1. 选择初始化的 k 个样本作为初始聚类中心 ![[公式]](https://www.zhihu.com/equation?tex=a%3D%7Ba_1%2Ca_2%2C%E2%80%A6a_k%7D) ；\n",
    "    2. 针对数据集中每个样本 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 计算它到 k 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；\n",
    "    3. 针对每个类别 ![[公式]](https://www.zhihu.com/equation?tex=a_j) ，重新计算它的聚类中心 ![[公式]](https://www.zhihu.com/equation?tex=a_j%3D%5Cfrac%7B1%7D%7B%5Cleft%7C+c_i+%5Cright%7C%7D%5Csum_%7Bx%5Cin+c_i%7Dx) （即属于该类的所有样本的质心）；\n",
    "    4. 重复上面 2 3 两步操作，直到达到某个中止条件（迭代次数、最小误差变化等）。\n",
    "  - 优点：\n",
    "    - 容易理解，聚类效果不错，虽然是局部最优， 但往往局部最优就够了；\n",
    "    - 处理大数据集的时候，该算法可以保证较好的伸缩性；\n",
    "    - 当簇近似高斯分布的时候，效果非常不错；\n",
    "    - 算法复杂度低。\n",
    "\n",
    "  - 缺点：\n",
    "    - K 值需要人为设定，不同 K 值得到的结果不一样；\n",
    "    - 对初始的簇中心敏感，不同选取方式会得到不同结果；\n",
    "    - 对异常值敏感；\n",
    "    - 样本只能归为一类，不适合多分类任务；\n",
    "    - 不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。\n",
    "\n",
    "- 机器学习导致误差的原因？过拟合、欠拟合对应的偏差和方差是怎样的？\n",
    "\n",
    "  - 偏差：模型无法表达数据集的复杂度，模型不够复杂，导致不能学习到基本关系，导致欠拟合；\n",
    "  - 方差：数据量有限，模型对数据过度敏感，导致方差；\n",
    "  - 过拟合容易导致高方差，欠拟合容易导致高偏差；\n",
    "\n",
    "- 如何解决过拟合问题？哪些角度\n",
    "\n",
    "  - 数据层面：更多数据、数据增强；\n",
    "  - 模型层面：更简单模型、更优化的模型；\n",
    "  - 正则化：权重衰减正则化；\n",
    "  - bagging等集成学习方法，深度学习中的dropout；\n",
    "  - early stopping；\n",
    "  - BN、LN等实践中也有助于降低过拟合风险；\n",
    "\n",
    "- LR的原理，问题的假设，为什么用交叉熵作为损失函数\n",
    "\n",
    "  - 如题1，交叉熵事伯努利分布在最大似然估计下的特例；\n",
    "\n",
    "- LR损失函数写一下\n",
    "\n",
    "  - 如题1\n",
    "\n",
    "- LR是不是凸优化问题，如何判断LR达到最优值\n",
    "\n",
    "  - 对于一个优化问题，如果目标函数是凸的且可行域是凸集，那么就是一个凸优化问题。凸优化问题有一个很好的性质，即局部最优解就是全局最优解。凸优化就是研究如何解决凸优化问题，以及对于非凸的问题如何转化为凸优化问题求解。\n",
    "  - 逻辑斯特回归是凸优化问题，凸函数+无约束条件。有最优解是因为存在一阶导数为零的点。\n",
    "\n",
    "- LR一般用什么数据，一般有什么特点(离散数据，离散化的一堆优点)\n",
    "\n",
    "- LR，SVM, xgboost如何防止过拟合\n",
    "\n",
    "  - 具体的细节如上，结合LR、SVM和xgbt说；\n",
    "  - LR一般采用L1和L2正则化（权重衰减）；\n",
    "  - **加入软间隔**：SVM引入了松弛变量，引入松弛变量使SVM能够容忍异常点的存在，很好的缓解了过拟合，SVM的最小化$||w||$本质上是要最大化间隔，但同时也是一种**正则化方法**，可以看作是对模型的结构约束，这样可以筛选掉一部分w很大的模型，削减假设空间，控制求解的模型是我们希望的，从而缓解过拟合。\n",
    "  - XGBT可以控制树的深度、最大叶子结点数目、列采样等方法；\n",
    "\n",
    "- lr和树模型，离散特征和连续特征分别怎么处理\n",
    "\n",
    "  - LabelEncoder和分桶\n",
    "  - lr和线性回归的区别\n",
    "  - 一个是分类模型，一个是回归模型；\n",
    "\n",
    "- 连续特征可以直接输入到LR中不? （归一化和标准化有什么区别）\n",
    "\n",
    "  - 可以，但是容易过拟合，建议可以分桶离散化后输入；\n",
    "  - 归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定；\n",
    "  - 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；\n",
    "  - 在机器学习中，标准化是更常用的手段，归一化的应用场景是有限的。我总结原因有两点：\n",
    "    - 1、标准化更好保持了样本间距。当样本中有异常点时，**归一化有可能将正常的样本“挤”到一起去。比如三个样本，某个特征的值为1，2，10000假设10000这个值是异常值，用归一化的方法后，正常的1,2就会被“挤”到一起去**。如果不幸的是1和2的分类标签还是相反的，那么，当我们用梯度下降来做分类模型训练时，模型会需要更长的时间收敛，因为将样本分开需要更大的努力！而标准化在这方面就做得很好，至少它不会将样本“挤到一起”。\n",
    "    - 2、**标准化更符合统计学假设**\n",
    "      对一个数值特征来说，**很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，**只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。\n",
    "\n",
    "- 线性回归可以求闭式解，逻辑回归可以吗，为什么，LR用什么求解参数，为什么要用梯度下降算法\n",
    "\n",
    "  - 不能：最大似然估计下没有。有近似解，如果是非最大似然估计，那么是可能推导出解析解的，也可以理解为是最大似然估计下的近似解。\n",
    "  - 因为他的目标函数是凸函数，存在极小值，所以通过梯度下降法，或者牛顿迭代法都能计算出最优解。\n",
    "\n",
    "- SVM和LR的区别\n",
    "\n",
    "  1. 都是分类算法，都在找最佳分类超平面，都是监督学习；\n",
    "  2. 都是判别式模型，不是生成式，判别式模型不关心数据是如何生成的，只关心数据差别和结果的关系；\n",
    "  3. LR是统计学方法，SVM是几何学方法；\n",
    "  4. SVM只考虑支撑向量，不考虑距离较远的样本，LR本质上还是计算了所有样本的距离；\n",
    "  5. SVM对样本数目要求较小，只用支撑向量；\n",
    "  6. LR是参数模型，对样本不均衡敏感，SVM是非参数模型，对样本不均衡不敏感；参数模型假设数据服从了某一分布；\n",
    "  7. LR能够获得概率，SVM则不能；\n",
    "  8. LR并行化方便，SVM优化比较困难。\n",
    "\n",
    "- SVM的公式会推导嘛，SVM的损失函数\n",
    "\n",
    "  - SVM推导过程比较长和复杂，建议看西瓜书中的推导\n",
    "  - Hinge损失函数(合页损失函数)：\n",
    "    $$\n",
    "    L(y, f(x))= max(0, 1-yf(x))\n",
    "    $$\n",
    "    特点是不鼓励过度自信，更关注整体的误差，健壮性相对高，对噪声不敏感，缺点是没有好的概率解释。\n",
    "  - SVM原理，为什么求最大间隔，为什么用核函数，常见的核函数及区别\n",
    "  - SVM大概的可以不确切的分为三个程度理解： \n",
    "    - (1) 线性可分情况下的线性分类器，这是最原始的SVM，它最核心的思想就是最大的分类间隔（margin maximization）； \n",
    "    - (2) 线性不可分情况下的线性分类器，引入了软间隔（soft margin）的概念； \n",
    "    - (3) 线性不可分情况下的非线性分类器，是SVM与核函数（kernel function）的结合。 \n",
    "\n",
    "  - 支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；SVM还包括**核技巧**，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。硬间隔最大化不允许任何样本出现错分的情况，哪怕导致过拟合；容许少量样本被错分，从而得到一个次优解，而这个容忍的程度则通过目标函数来调节。或者再极端一点就是，根本找不到一个超平面能够将样本无误的分开（不过拟合的前提下），必须得错分一些点。此时间隔就称之为**软间隔（soft margin）**。\n",
    "\n",
    "  - 最大间隔：SVM最大分类间隔的灵感来自于一个非常符合直觉的观察，如果存在两类数据，数据的特征是二维的，那么我们就可以把数据画在一个二维平面上，此时我想找到一个决策面（决策边界）去将这两类数据分开，理论上这个决策边界有无数种选择，就像图中画出的四条黑色的线，都能实现分类，但是哪一种是最好的分类方式呢？SVM算法认为在上图中靠近决策平边界的点（正负样本）与决策边界的距离最大时，是最好的分类选择。\n",
    "\n",
    "  - 解决在低维空间线性不可分的问题，通过核函数把低维映射到高维，实现线性可分。核函数的作用是进行特征转换或特征生成（升维），比如多项式核函数把输入的低次项特征，转换为高阶特征，即x转为x的平方。\n",
    "\n",
    "  - 常用核函数：\n",
    "    - 线性核函数\n",
    "      - 优点：\n",
    "        方案首选，奥卡姆剃刀定律\n",
    "        简单，可以求解较快一个QP问题\n",
    "        可解释性强：可以轻易知道哪些feature是重要的\n",
    "      - 限制：只能解决线性可分问题\n",
    "    - 多项式核函数\n",
    "      - 基本原理：依靠升维使得原本线性不可分的数据线性可分；\n",
    "      - 升维的意义：使得原本线性不可分的数据线性可分；\n",
    "      - 优点：\n",
    "        可解决非线性问题\n",
    "        可通过主观设置幂数来实现总结的预判\n",
    "      - 缺点：\n",
    "        对于大数量级的幂数，不太适用\n",
    "        比较多的参数要选择\n",
    "    - 径向基（高斯）核函数\n",
    "      - 优点：\n",
    "        可以映射到无限维\n",
    "        决策边界更为多样\n",
    "        只有一个参数，相比多项式核容易选择\n",
    "      - 缺点：\n",
    "        可解释性差(无限多维的转换，无法算w)\n",
    "        计算速度比较慢(解一个对偶问题)\n",
    "        容易过拟合(参数选不好时容易overfitting)\n",
    "    - Sigmoid核函数\n",
    "      - 采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。\n",
    "    - 在实战中更多的是：\n",
    "      - 特征维数高选择线性核\n",
    "      - 样本数量可观、特征少选择高斯核（非线性核）\n",
    "      - 样本数量非常多选择线性核（避免造成庞大的计算量）\n",
    "\n",
    "- SVM支持向量怎么得到的\n",
    "\n",
    "- 写一下SVM的原问题和对偶问题，分别解释一下      \n",
    "  - SVM的目标函数是一个凸二次优化问题，但是可以通过拉格朗日对偶性将原始问题转化为对偶问题，通过对偶问题来求原始问题的解。因为对偶问题更容易求解，二是自然引进核函数，进而推广到非线性分类问题。三是改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关。\n",
    "\n",
    "- SVM核函数有什么性质，写一下SVM核化的形式\n",
    "\n",
    "- 无监督学习，半监督学习，有监督学习的区别\n",
    "  - 有监督学习：通过**已有的训练样本**去训练得到一个**最优模型**，再利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现**预测**和**分类**的目的，也就具有了对未知数据进行预测和分类的能力。\n",
    "  - 无监督学习：缺乏足够的先验知识，因此难以人工标注类别或进行人工类别标注的成本太高。很自然地，我们希望计算机能代我们完成这些工作，或至少提供一些帮助。根据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。\n",
    "  - 半监督学习：是监督学习与无监督学习相结合的一种学习方法。半监督学习使用大量的未标记数据，以及同时使用标记数据，来进行模式识别工作。当使用半监督学习时，将会要求尽量少的人员来从事工作，同时，又能够带来比较高的准确性。\n",
    "\n",
    "- 有哪些无监督学习的方法（kmeans,pca,生成模型，自编码器）\n",
    "\n",
    "- 有哪些回归模型（多项式回归，树模型，svr, 神经网络）\n",
    "\n",
    "- 生成模型、判别模型的区别\n",
    "  - 于有监督学习可以将其分为两类模型：判别式模型和生成式模型。简单地说，判别式模型是针对条件分布建模，而生成式模型则针对联合分布进行建模。对于输入x，类别标签y：生成式模型估计它们的联合概率分布P(x,y)\n",
    "    判别式模型估计条件概率分布P(y|x)；\n",
    "  - 生成式模型：根据训练数据得到分类函数和分界面，比如说根据SVM模型得到一个分界面，然后直接计算条件概率 ![[公式]](https://www.zhihu.com/equation?tex=P%28y%7Cx%29) ，我们将最大的 ![[公式]](https://www.zhihu.com/equation?tex=P%28y%7Cx%29) 作为新样本的分类。判别式模型是对条件概率建模，学习不同类别之间的最优边界，无法反映训练数据本身的特性，能力有限，其只能告诉我们分类的类别。\n",
    "  - 判别式模型：一般会对每一个类建立一个模型，有多少个类别，就建立多少个模型。比如说类别标签有｛猫，狗，猪｝，那首先根据猫的特征学习出一个猫的模型，再根据狗的特征学习出狗的模型，之后分别计算新样本 ![[公式]](https://www.zhihu.com/equation?tex=x) 跟三个类别的联合概率 ![[公式]](https://www.zhihu.com/equation?tex=P%28x%2Cy%29) \n",
    "\n",
    "- 概率和似然的区别\n",
    "  - 在数理统计中，两个术语则有不同的意思。“概率”描述了给定模型参数后，描述结果的合理性，而不涉及任何观察到的数据。而“似然”则描述了给定了特定观测值后，描述模型参数是否合理。\n",
    "  - 概率和似然都是指可能性，但在统计学中，概率和似然有截然不同的用法。\n",
    "    - 概率描述了已知参数时的随机变量的输出结果;\n",
    "    - 似然则用来描述已知随机变量输出结果时，未知参数的可能取值。\n",
    "\n",
    "- 最大似然估计和后验概率的区别，分别用LR来推导损失函数的话有什么区别（乘以W的先验概率）\n",
    "  - 极大似然估计（MLE）\n",
    "    - 是频率学派模型参数估计的常用方法。\n",
    "    - 似然，可以简单理解为概率、可能性，也就是说要最大化该事件发生的可能性\n",
    "    - 它的含义是根据已知样本，希望通过调整模型参数来使得模型能够最大化样本情况出现的概率。\n",
    "  - 最大后验概率估计（MAP）\n",
    "    - 是贝叶斯派模型参数估计的常用方法。\n",
    "    - 就是最大化在给定数据样本的情况下模型参数的后验概率\n",
    "    - 它依然是根据已知样本，来通过调整模型参数使得模型能够产生该数据样本的概率最大，只不过对于模型参数有了一个先验假设，即模型参数可能满足某种分布，不再一味地依赖数据样例（万一数据量少或者数据不靠谱呢）。**故最大后验估计可以看做规则化的最大似然估计。**\n",
    "\n",
    "- 朴素贝叶斯介绍，朴素贝叶斯公式，为什么朴素贝叶斯假设特征之间是相互独立的\n",
    "  - 它之所以称为朴素，是因为它假设特征之间是相互独立的。\n",
    "\n",
    "- l1,l2特性及原理，分别适用于那些场合\n",
    "  - L1正则化：L1正则化，相当于为模型添加了一个先验知识，就是权重矩阵W服从均值为0的拉普拉斯分布（拉普拉斯分布类似正太分布，但是长尾效应更明显，更容易出现极端值）。L1 正则化增加了所有权重 w 参数的绝对值之和逼迫更多 w 为零，也就是变稀疏（ L2 因为其导数也趋 0, 奔向零的速度不如 L1 给力了）。我们对稀疏规则趋之若鹜的一个关键原因在于它能**实现特征的自动选择**。一般来说，大部分特征 x_i 都是和最终的输出 y_i 没有关系或者不提供任何信息的。在最小化目标函数的时候考虑 x_i 这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确 y_i 的预测。L1 正则化的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为 0。\n",
    "  - L2正则化：L2正则化，相当于为模型添加了一个正态分布先验知识，就是权重矩阵W服从均值为0的正态分布。L2 正则化中增加所有权重 w 参数的平方之和，逼迫所有 w 尽可能趋向零但不为零（L2 的导数趋于零）。因为在未加入 L2 正则化发生过拟合时，拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，在某些很小的区间里，函数值的变化很剧烈，也就是某些 w 值非常大。为此，L2 正则化的加入就惩罚了权重变大的趋势。\n",
    "  - L1正则化更容易获得稀疏解，所以需要稀疏参数的话选用L1。L1用棱形的边界去逼近目标，在和坐标轴交点更容易成为随机下降的最优点；我们经过观察可以看到，几乎对于很多原函数等高曲线，和某个菱形相交的时候及其容易相交在坐标轴（比如上图），也就是说最终的结果，解的某些维度及其容易是 0，比如上图最终解是，这也就是我们所说的 L1 更容易得到稀疏解（解向量中 0 比较多）的原因。\n",
    "  - 给一个多峰数据场景，为什么l2不适合，可以怎么解决\n",
    "  - （不理解什么是多峰数据）\n",
    "\n",
    "- 讲讲Kmeans、EM算法\n",
    "\n",
    "  - 如上\n",
    "\n",
    "- 机器学习中怎么解决过拟合，DNN中怎么解决\n",
    "  - 增大数据集、数据增强、加入噪声\n",
    "  - 更换简单模型、更好的模型：模型更加复杂容易过拟合\n",
    "  - 使用权重衰减正则化\n",
    "  - 使用集成学习方法，特别是bagging\n",
    "  - 使用dropout：本质上是深度学习的bagging\n",
    "  - 早停：训练集最优不一定在测试集最优\n",
    "  - LN和BN；\n",
    "\n",
    "- 说一下SVD怎么降维\n",
    "  - 对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。\n",
    "\n",
    "- 推导softmax做激活函数求导\n",
    "  1. $\\text{设输入为}z, z = [z_1, z_2, z_3...z_n]$\n",
    "  2. $\\text{输入Softmax后}: a_i = \\frac{e_{z_u)}}{\\sum_{k=1}^ne^{z_k}}$\n",
    "  3. $\\text{设在logLoss下，损失函数定义为：}L = -\\sum_{i=1}^ny_iln(a_i)$\n",
    "  4. $\\text{真实label只有一个为1，剩余为0，假设第j个label为1，即} y_j=1$\n",
    "  5. $\\text{求}\\frac{\\partial a}{\\partial z}:$\n",
    "  \t$\\text{当} i \\neq j: \\frac{\\partial a_j}{\\partial z_i} = \\frac{-e^{z_j z_i}}{(\\sum_k^ne_{zK})^2}= -a_ja_i$\n",
    "  \t$\\text{当} i = j: \\frac{\\partial a_j}{\\partial z_i} = \\frac{e^{z_j}\\sum_k^ne^z_k - e^{z_j}e^{z_j}}{(\\sum_k^ne_{zk})^2}= aj - aj^2$\n",
    "  6. $\\text{加入损失函数情况下求导，因为是logloss，比较容易写出导数：}$\n",
    "  \t$\\text{当} i \\neq j: \\frac{\\partial L}{\\partial z_i} = a_i$\n",
    "  \t$\\text{当} i = j: \\frac{\\partial L}{\\partial z_i} = a_j - 1$\n",
    "\n",
    "- LR，SVM,xgb哪个对样本不平衡不太敏感，顺便把SVM和xgb介绍了\n",
    "  - SVM不敏感，其他如上\n",
    "\n",
    "- 降维方法了解嘛，PCA? 为什么取特征值前k大的对应的特征向量组成的矩阵？低秩表示\n",
    "  - PCA和SVD\n",
    "  - PCA是为了在尽量保证“信息量不丢失”的情况下，对原始特征进行 降维，也就是尽可能将原始特征往具有最大投影信息量的维度上进行投影。将原特征投影到这些维度上，使降维后信息量损失最小。\n",
    "  - 目标（1）.找到变异最大的新维度，以最大程度地区分不同数据点。（2）.这一新维度应该可以让我们预测和重建原始维度，重建或投影误差（reconstruction/projection error）应该最小化\n",
    "\n",
    "\n",
    "### 深度学习\n",
    "\n",
    "- 梯度是什么，hessian矩阵怎么求\n",
    "  - 梯度的本意是一个向量（矢量）， 表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。\n",
    "  - 梯度是雅可比矩阵的一种特殊形式，当m=1时函数的雅可比矩阵就是梯度，这个概念原是为场论设定的，任何场都可以用来理解梯度，后来被引用到数学中用来指明函数在指定点的变量率最快的方向和大小，是一种变化效率的数字抽象。\n",
    "  - 海森矩阵是一个自变量为向量的实值函数的二阶偏导数组成的方块矩阵\n",
    "\n",
    "- 有没有上过凸优化的课程，如何判断凸函数\n",
    "  - 对于一元函数f(x)，我们可以通过其二阶导数f″(x) 的符号来判断。如果函数的二阶导数总是非负，即f″(x)≥0 ，则f(x)是凸函数\n",
    "\n",
    "- 防止过拟合的策略有哪些\n",
    "  - 如上\n",
    "\n",
    "- dropout怎么防止过拟合, Dropout在训练和测试区别\n",
    "  - 数据层面\n",
    "    对于每一个dropout后的网络，进行训练时，相当于做了Data Augmentation。比如，对于某一层，dropout一些单元后，形成的结果是(1.5，0，2.5，0，1，2，0)，其中0是被drop的单元，那么总能找到一个样本，使得结果也是如此。这样每一次dropout其实都相当于增加了样本。\n",
    "  - 模型层面\n",
    "    1. 在较大程度上减小了网络的大小：**在这个“残缺”的网络中，让神经网络学习数据中的局部特征（即部分分布式特征），但这些特征也足以进行输出正确的结果。**\n",
    "    2. **Dropout思想类似于集成学习中的Bagging思想：**由学习阶段可知，每一次训练都会按keep_probability=p来保留每个神经元，这意味着每次迭代过程中，随机删除一些神经元，这就意味着在多个\"残缺\"的神经网络中，每次都进行随机的特征选择，这要比仅在单个健全网络上进行特征学习，其泛化能力来得更加健壮。\n",
    "  - **区别**：\n",
    "  - **【训练阶段】**以概率p主动临时性地忽略掉部分隐藏节点，算法步骤如下：\n",
    "    1. 首先随机**（临时）**删掉网络中的一些隐藏神经元，一般情况下输入输出神经元保持不变 \n",
    "    2. 把输入x通过**修改后**的网络前向传播，删除的神经元不进行前向传播，传给下一层的值是0，然后把得到的损失结果通过**修改后**的网络反向传播。一小批训练样本执行完这个过程后就按照随机梯度下降法更新**没有被删除的神经元**对应的参数（w，b）\n",
    "    3. 恢复被删掉的神经元，此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新\n",
    "    4. 不断重复上述过程1，2，3；\n",
    "\n",
    "  - **【测试阶段】**将参与学习的节点和那些被隐藏的节点以一定的概率p加权求和，综合计算得到网络的输出。预测的时候，每一个单元的参数要预乘以p。比如一个神经元的输出是x，在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。\n",
    "\n",
    "- BN介绍，BN怎么防止过拟合，怎么用的，参数量, 参数怎么得到的\n",
    "  - **BN 层对数据做了哪些处理？**\n",
    "    如果没有 BN 层，深度神经网络中的每一层的输入数据或大或小、分布情况等都是不可控的。有了 BN 层之后，每层的数据分布都被转换在均值为零，方差为1 的状态，这样每层数据的分布大致是一样的，训练会比较容易收敛。\n",
    "  - **BN 为什么能加快网络训练时的收敛速度？**\n",
    "    一般来说，如果神经网络中每层输入数据集服从同一分布，神经网络在训练时比较容易收敛（因为映射 梯度消失 梯度爆炸\n",
    "  - **BN 层为什么能防止梯度消失和梯度爆炸？**\n",
    "    - **梯度消失**\n",
    "    对于 Sigmoid 激活函数，其导数最大值为 0.25. 神经网络在反向传播更新参数时，执行链式求导。假如网络有 n 层，那么第一层的梯度将会小于 （0.25 * 权重）的 n 次方，学习速率相对较慢，而网络最后一层只需要对自身求导一次，梯度比较大，学习速率快。所以，后面的几层网络在学习，而前面的网络几乎停滞了，训练不动。\n",
    "    - **梯度爆炸**\n",
    "    第一层偏移量的梯度 = 激活层斜率1 * 权值1 * 激活层斜率2 * … 激活层斜率(n-1) * 权值(n-1) * 激活层斜率n，假如激活层斜率均为最大值 0.25，每层权重值大于 4，这样梯度就会呈指数增加。而 BN 能够很好地控制权重值的更新幅度。\n",
    "\n",
    "  - **BN 为什么能够防止过拟合？**\n",
    "    - BN 的使用使得一个 minibatch 中所有样本都被关联在了一起，因此网络不会从某一个具体的训练样本中生成确定的结果。也就是说，即使输入同一个训练样本，每次的输出都是不同的，而且每次网络都是随机取 batch，这样就会使得整个网络不会朝某一个方向使劲学习\n",
    "    - BN 层在线性层或卷积层之后，在非线性激活函数之前，这是为什么呢？首先，全连接和卷积层的输出一般是一个对称,非稀疏的一个分布，更加类似高斯分布，对它们进行归一化会产生更加稳定的分布。\n",
    "    - 其次，如果数据先经过 Relu 等激活函数，那么数据集的分布同样不可控，再对它进行 BN 处理，无法达到我们想要的稳定分布的效果。\n",
    "\n",
    "  - **BN 层的数据集均值怎么计算？**\n",
    "    训练和推理时的计算方法是不一样的。\n",
    "    训练时的数据集是 minibatch，而推理时的数据集是所有的 minibatch，那是不是得记录训练过程中产生的所有 minibatch 的均值呢？答案是否定的，用滑动平均值就能解决该问题。\n",
    "\n",
    "- 优化器，SGD与Adam的异同点\n",
    "  - SGD有两大改进方向：动量上改进、自适应学习率改进\n",
    "  - Adam同时结合了这两者的改进方法：在动量上用了Momentum，自适应学习率上用了RMSprop\n",
    "\n",
    "- 介绍一下你了解的激活函数以及优缺点\n",
    "  - Sigmoid\n",
    "    - 是什么：f=1/(1+e^-y)\n",
    "    - 优点：\n",
    "      - 能够限定输出值为0-1\n",
    "      - 可微，而且梯度平滑，没有跳跃点\n",
    "    - 缺点：\n",
    "      - 容易出现梯度消失：两端的梯度接近于0\n",
    "      - 输出的中心是0.5：不是0，所以梯度下降会更慢\n",
    "      - 有指数运算：计算机内部运行慢，求导慢\n",
    "\n",
    "- 介绍一下深度学习的优化器发展史, 及常见的优化算法\n",
    "  - SGD有两大改进方向：动量上改进、自适应学习率改进\n",
    "  - Momentum：每一步要结合上一步的方向，能够较大程度降低震荡，加速收敛；\n",
    "  - Agegrad：学习速率会参考历史梯度的大小进行放缩，加快收敛速度；\n",
    "  - RMSProp：是Adadelta增强，添加了衰减系数控制历史梯度获得多少\n",
    "  - Adam：结合了momentum和RMSProm\n",
    "\n",
    "- 梯度爆炸和梯度消失问题\n",
    "  - 本质原因：\n",
    "    - 网络加深、参数共享\n",
    "    - 如果参数的特征值不在1附近，反向传播就会发生连乘效应\n",
    "  - 表现：\n",
    "    - 梯度消失导致无法收敛\n",
    "    - 梯度爆炸导致效果不稳定\n",
    "  - 解决方法\n",
    "    - 换激活函数，用relu族的，sigmoid、tanh两端平滑容易梯度消失；\n",
    "    - 用BN和LN，本质上是让参数特征在1附近；\n",
    "    - 换网络，用resnet、lstm能类型网络；\n",
    "    - 用与训练网络：霍顿文章表明欲训练网络；\n",
    "    - 进行梯度裁剪；\n",
    "    - 用正则化的方法；\n",
    "\n",
    "- SGD缺点，已经有什么改进的优化器\n",
    "  - 每次只使用一批样本，导致迭代方向变化很大，容易剧烈震荡；\n",
    "  - 学习率固定，容易在局部下降速度过慢过过快，得到局部最优解或者学习过慢；\n",
    "  - 改进方法就是动量和自适应学习率：momentum、adagrad、Adam、等；\n",
    "\n",
    "- 网络权重初始化为0有什么影响，初始化为一个非0的常数呢？\n",
    "  - 如果W、b初始化为0：每一层前向传播输出都是一致的，反向传播同样就一致，多个神经元（网络的宽度）作用等同于1个；\n",
    "  - 只有W初始化为0、b随机初始化：反向传播过程中，第一次的第一层的dw都是0，只有第二次才能恢复，导致收敛更慢，梯度消失问题严重；\n",
    "  - 只有b初始化为0：可以的\n",
    "\n",
    "- embedding如何设置维度？越大越好还是越小越好？\n",
    "  - 维度越低越粗糙，拟合能力就有限；\n",
    "  - 维度越高越细致，但是需要更多数据集才能训练，但是容易维度灾难，而且容易过拟合；\n",
    "  - 个人经验是需要结合特征的取值和分布、特征实际的业务意义、问题规模、经验参数，反复调参迭代、优化得到；\n",
    "\n",
    "- transformer中计算attention除于根号d的作用\n",
    "  - 因为使用了softmax，如果元素方差很大，会导致softmax把大部分权重分配给大元素，导致反向传播的时候梯度很小，梯度消失。\n",
    "  - 和xvier、bn、he初始化原理一样，在输入输出空间映射的时候，均衡方差，防止梯度消失；\n",
    "\n",
    "- embedding如何训练\n",
    "  - 在推荐系统中embedding有两种：\n",
    "    - 离散特征通过端到端训练，先通过embedding层再和其他特征合并训练；\n",
    "    - 多模态特征或者其他预训练特征，通过NLP、CV等内容理解方法抽取出来后和推荐特征合并训练；\n",
    "\n",
    "  - NLP中的词向量embedding：\n",
    "    - CBOW: 先在句子中选定一个中心词，并把其它词作为这个中心词的上下文。在学习过程中，使用上下文的词向量推理中心词，这样中心词的语义就被传递到上下文的词向量中, 从而达到学习语义信息的目的。\n",
    "    - Skip-gram: 同样先选定一个中心词，并把其他词作为这个中心词的上下文。不同的是，在学习过程中，使用中心词的词向量去推理上下文，这样上下文定义的语义被传入中心词的表示中， 从而达到学习语义信息的目的。\n",
    "    - 一般来说，CBOW比Skip-gram训练速度快，训练过程更加稳定，原因是CBOW使用上下文average的方式进行训练，每个训练step会见到更多样本。而在生僻字（出现频率低的字）处理上，skip-gram比CBOW效果更好，原因是skip-gram不会刻意回避生僻字(CBOW结构中输入中存在生僻字时，生僻字会被其它非生僻字的权重冲淡)。\n",
    "\n",
    "- 介绍下attention，相比cnn、lstm的优势\n",
    "  - 优点：\n",
    "    1. 一步到位获取全局与局部的联系，不会像RNN网络那样对长期依赖的捕捉会收到序列长度的限制。\n",
    "    2. 每步的结果不依赖于上一步，可以做成并行的模式\n",
    "    3. 相比CNN与RNN，参数少，模型复杂度低。(根据attention实现方式不同，复杂度不一）\n",
    "  - 缺点：\n",
    "    1. 没法捕捉位置信息，即没法学习序列中的顺序关系。这点可以通过加入位置信息，如通过位置向量来改善，具体可以参考最近大火的BERT模型。\n",
    "\n",
    "- word2vec如何进行负采样\n",
    "  - 负采样的核心思想是：就是分别计算正负样本的loss，这样负样本就可以选择采样的那几条，而不是除开正样本以外的所有样本。\n",
    "  - 一个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words\n",
    "\n",
    "- word2vec两种训练方法的区别，具体损失函数\n",
    "  - CBOW: 先在句子中选定一个中心词，并把其它词作为这个中心词的上下文。在学习过程中，使用上下文的词向量推理中心词，这样中心词的语义就被传递到上下文的词向量中, 从而达到学习语义信息的目的。\n",
    "  - Skip-gram: 同样先选定一个中心词，并把其他词作为这个中心词的上下文。不同的是，在学习过程中，使用中心词的词向量去推理上下文，这样上下文定义的语义被传入中心词的表示中， 从而达到学习语义信息的目的。\n",
    "  - 一般来说，CBOW比Skip-gram训练速度快，训练过程更加稳定，原因是CBOW使用上下文average的方式进行训练，每个训练step会见到更多样本。而在生僻字（出现频率低的字）处理上，skip-gram比CBOW效果更好，原因是skip-gram不会刻意回避生僻字(CBOW结构中输入中存在生僻字时，生僻字会被其它非生僻字的权重冲淡)\n",
    "\n",
    "- 介绍LSTM每一个门的具体操作，一个LSTM cell的时间复杂度是多少\n",
    "  - 遗忘门（forget gate）\n",
    "    它决定了上一时刻的单元状态 `c_t-1` 有多少保留到当前时刻 `c_t`\n",
    "  - 输入门（input gate）\n",
    "    它决定了当前时刻网络的输入 `x_t` 有多少保存到单元状态 `c_t`\n",
    "  - 输出门（output gate）\n",
    "    控制单元状态 `c_t` 有多少输出到 LSTM 的当前输出值 \n",
    "\n",
    "- transformer中encoder和decoder的输入分别是什么\n",
    "  - encoder的输入：\n",
    "    - 输入x，比如词向量的embediing\n",
    "    - Positional Encoding\n",
    "  - decoder的输入：\n",
    "    - 初始输入：前一时刻Decoder输入+前一时刻Decoder的预测结果 + Positional Encoding\n",
    "    - 中间输入：Encoder Embedding\n",
    "    - Shifted Right：在输出前添加起始符，方便预测第一个Token\n",
    "\n",
    "- transformer中encoder与decoder的QKV矩阵如何产生\n",
    "  - query代表的是当前单词，key代表的是每个单词，value代表的也是当前单词。\n",
    "  - Decoder**每层是有两块Attention**的：\n",
    "    - 一块是正常的Self-Attention，QKV的输入都来自Decoder上一层的输出。\n",
    "    - 另一块是问题中提到的Decoder Encoder Attention，其中的K，V输入来自encoder的输出。\n",
    "\n",
    "- transformer中QKV矩阵是否可以设置成同一个\n",
    "  - kq使用同一个，输入的结果类似于一个单位矩阵了，失去了attention的效果。\n",
    "  - 数学推理上同一个矩阵点乘就是一个单位矩阵。\n",
    "  - 增加了泛化能力：kq是不同初始化的，能够得到输入x在不同空间下的投影，增加表达能力。\n",
    "\n",
    "- transformer与bert的位置编码有什么区别\n",
    "\n",
    "- BERT中计算attention的公式\n",
    "\n",
    "- BERT中LayerNorm的作用，为什么不用BN？\n",
    "\n",
    "- BERT中的两种预训练任务介绍\n",
    "\n",
    "- 深度学习中BN的好处？最早提出BN是为了解决什么问题？BN具体怎么实现的\n",
    "  - 随着训练进行，数据的分布会发生变化，会导致训练困难。如果没有 BN 层，深度神经网络中的每一层的输入数据或大或小、分布情况等都是不可控的。有了 BN 层之后，每层的数据分布都被转换在均值为零，方差为1 的状态，这样每层数据的分布大致是一样的，训练会比较容易收敛。\n",
    "  - 神经网络在训练时比较容易收敛，更容易避免梯度消失、梯度爆炸；\n",
    "\n",
    "- 激活函数中，sigmoid，tanh有什么不好的地方？relu有什么优势？\n",
    "  - sigmoid、tanh缺点\n",
    "    - 两端有梯度消失；\n",
    "    - 有指数等运算，求导更复杂；\n",
    "  - relu优点：\n",
    "    - 求导快，梯度固定；\n",
    "    - 避免了两端梯度消失现象；\n",
    "\n",
    "- pagerank相比于tf-idf的优势\n",
    "\n",
    "- 画一下LSTM的结构图\n",
    "\n",
    "- RNN和LSTM的区别，解决了什么问题，为什么解决了梯度消失的问题\n",
    "\n",
    "- 深度模型和传统机器学习模型对数据量的要求，什么场景用什么模型  \n",
    "\n",
    "### 特征工程\n",
    "\n",
    "- 特征工程一般怎么做：\n",
    "  - 特征工程是从原始数据中提取特征以供算法和模型使用的工程。特征工程是一个反复迭代不断优化的过程，最重要的是提问题、做假设、去验证。做特征工程前思考所有和业务有关的变量数据，思考可行性：数据获取难度、数据规模、覆盖率等信息，获得数据后后进行一些特征处理如下。\n",
    "  - 数据探索（EDA）和预处理：\n",
    "    1. 量纲转换、离散特征编码、连续特征分桶\n",
    "    2. 特征清洗\n",
    "    3. 缺失值处理\n",
    "\n",
    "  - 特征选择：\n",
    "    1. 计算数据是否发散，信息熵为0的特征没有意义，对结果没有区分度；\n",
    "    2. 计算特征重要性选择；\n",
    "    3. 包装法；\n",
    "    4. 计算相关系数或者假设检验等数学方法；\n",
    "  - 特征构造、交叉：\n",
    "    1. 数据降维，PCA或者LDA；\n",
    "    2. 特征交叉。\n",
    "\n",
    "- 特征数值分布比较稀疏如何处理\n",
    "  - embedding\n",
    "  - 用对稀疏特征优化更好的算法，比如FM等；\n",
    "\n",
    "- 正负样本不均衡如何处理\n",
    "  - 采样（欠采样、过采样）\n",
    "  - 集成学习\n",
    "  - Loss：对较少样本分类错误增加更高惩罚\n",
    "\n",
    "- 连续特征离散化的作用\n",
    "  - 增强模型鲁棒性，减少噪声的影响，减少过拟合\n",
    "  - 增强表达能力，引入了非线性表达，减少偏差\n",
    "  - 模型运算速度更快，储存所用空间更少\n",
    "\n",
    "- 对id类特征onehot导致维度过高，如何处理？\n",
    "  - embedding\n",
    "\n",
    "- 如何进行特征筛选\n",
    "  - 过滤法：按照相关性等指标对特征评分，进行特征选择\n",
    "  - 包装法：每次选择部分特征进行训练\n",
    "  - 嵌入法：使用能够计算特征重要性的模型（比如树、线性模型），找到最重要的特征\n",
    "\n",
    "- DNN能做特征交叉嘛\n",
    "  - 可以，很多推荐广告中的模型都有聚焦在这方面，比如PNN、DeepFM、DCN\n",
    "  - 特征交叉有两种，一种是显式交叉，一种是隐式交叉：\n",
    "    直接两个特征embedding向量乘积的显式方式，另外一种是embedding喂给mlp，通过DNN的深度来交叉的隐式交叉，两种都有特征cross的作用。\n",
    "    DNN结构是种低效率地捕获特征组合的结构，DNN进行特征交叉是隐式的，虽然是每个特征（位）在每一层乘以不同权重，但是彼此之间是有关联的，DNN 应该能捕捉到特征的共现。（DNN能够通过一定深度捕捉到隐式特征共现）所以即使是深度模型，目前一样还离不开类似FM这个能够直白地直接去组合特征的部分。\n",
    "  - 特征交叉如何被学习到：\n",
    "    无论显隐，特征corss都是构建在特征共现的基础上，学习方式分为两类，一类类似于LR模型交叉特征，两特征必须共现才能学习；另外一类，类似于FM、DNN，不必须严格共现就可以学习，门槛大大降低，这也是FM或者MLP好于LR的一个很重要的点（但是FM只能二阶交叉，DNN可以深度交叉）。\n",
    "\n",
    "- 海量类别特征该如何处理，有什么方法\n",
    "  - Mutil-Hot\n",
    "\n",
    "- pearson系数\n",
    "  -  两个连续变量(X,Y)的pearson相关性系数(Px,y)等于它们之间的协方差cov(X,Y)除以它们各自标准差的乘积(σX,σY)。系数的取值总是在-1.0到1.0之间，接近0的变量被成为无相关性，接近1或者-1被称为具有强相关性。\n",
    "\n",
    "- 归一化和标准化有什么区别\n",
    "  - 如上\n",
    "\n",
    "- 如果不使用最近邻检索的库，你会怎么做最近邻检索\n",
    "\n",
    "\n",
    "### 评估指标\n",
    "\n",
    "- auc的含义和计算方法, 有没有更快的计算方法\n",
    "  - ROC介绍\n",
    "    很多学习器是为了对测试样本分类产生一个概率预测，然后把概率值与一个分类阈值进行比较的。\n",
    "    ROC的全名叫做Receiver Operating Characteristic（受试者工作曲线） ，ROC 曲线平面的横坐标是false positive rate(FPR)，纵坐标是true positive rate(TPR)。对某个分类器而言，我们可以根据其在测试样本上的表现得到一个TPR和FPR点对。这样，此分类器就可以映射成ROC平面上的一个点。\n",
    "  - AUC介绍\n",
    "    AUC的值就是处于ROC 曲线下方的那部分面积的大小。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的性能。AUC（Area Under roc Curve）是一种用来度量分类模型好坏的一个标准。\n",
    "  - ROC绘制\n",
    "    1. 将样本值按照概率排序，依次选择每个概率作为正负样本分类的阈值；\n",
    "    2. 对于每个阈值，统计FPR和TPR（假阳率和真阳率），标注在图上，得到ROC，阈值越多越平滑；\n",
    "  - AUC计算\n",
    "    - 方法一，面积法：分段计算roc每一个矩形的面积（几乎不用这种方法）；\n",
    "    - 方法二，根据AUC统计学意义（预测正样本>负样本的概率），复杂度为On方：在m正样本，n个负样本的数据集中，对m*n个样本里，统计正样本预测概率大于负样本的数目，然后除以总条目；\n",
    "    - ​\t方法三（通常的做法），Ologn复杂度就是排序复杂度，本质是对上面一种对优化，减少了重复计算，不理解就选两三个概率模拟下：对概率从小到大排序，把所有正样本的排名相加，减去正样本组合对，得到有多少正样本大于负样本的数目，最后处以总样本数目。这里的逻辑是，排名大小代表了当前样本能够和前面样本形成的样本对，比如排名第五的正样本和1-5（包括自身，后面减去）形成了5对，正样本排名之和就是正样本形成的全部样本对，m（m+1）是1+2+3+……+M的结果，因为排名最小的正样本多统计了自身-自身，排名倒数第二多统计了自身-自身+（自身-前一个正，也就是正-正样本对）两对，以此类推。\n",
    "    $$\n",
    "    AUC = \\frac{\\sum_{i\\in positiveClass} Rank_i - M(1+M)/2}{M*N}\n",
    "    $$\n",
    "      \n",
    "  - 和PR曲线对比\n",
    "    1. PR曲线和ROC都采用了TPR（召回率）为横坐标，但是ROC纵坐标用的是FPR；\n",
    "    2. 因为PR曲线两个指标都关注正样本，所以在类别不均衡下，PR曲线更有说服力，ROC会有偏向乐观的估计；类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计；\n",
    "    3. ROC兼顾正负样本，更适合整体性能评估；\n",
    "    4. 如果有多份数据且存在**不同**的类别分布，比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为类别分布改变可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想测试不同类别分布下对分类器的性能的影响，则PR曲线比较适合。\n",
    "\n",
    "- AUC会不会出现小于0.5的情况，出现了怎么调bug\n",
    "  - 不会，如果出现了说明分类器有问题，反转一下输出就好。\n",
    "\n",
    "- AUC为1可能是由什么导致的？\n",
    "  - 特征穿越了，比如泄漏了lable给模型看到了；\n",
    "  - 样本极端不均衡，虽然auc能够相对处理样本不均衡，但是设想正负样本1:100，模型只需要把唯一的正样本概率判断到最大，就可以达到AUC为1；\n",
    "  - batch内分布不均衡；\n",
    "\n",
    "- 分类评估指标中，F1和AUC有什么区别\n",
    "  - AUC的优化目标：TPR和(1-FPR)\n",
    "    F1的优化目标：Recall和Precision\n",
    "  - 区别就在于auc除了希望提高recall之外，另一个优化目标specificity=1-FPR希望提高非真样本在检验中呈阴性的比例，也就是**降低非真样本呈阳性的比例(假阳性)**，也就是检验犯错误的概率。\n",
    "  - 而F1score的另一个优化目标是精确率\n",
    "  - 希望提高检验呈阳性的样本中实际为真的比例，也就是提高检验的准确率/命中率\n",
    "    那么说到这里，大家应该可以理解这里两个指标在recall之外，其实是存在内在矛盾的。如果说召回率衡量我们训练的模型(制造的一个检验)对既有知识的记忆能力(召回率)，那么两个指标都是希望训练一个能够很好拟合样本数据的模型，这一点上两者目标相同。但是auc在此之外，希望训练一个尽量不误报的模型，也就是知识外推的时候倾向保守估计，而f1希望训练一个不放过任何可能的模型，即知识外推的时候倾向激进，这就是这两个指标的核心区别。\n",
    "\n",
    "- 分类指标用的什么，哪个分类指标对正负样本分布不敏感\n",
    "  - AUC\n",
    "\n",
    "- 如果对负样本进行采样，auc的计算结果会发生变化吗\n",
    "  - AUC对正负样本分布不敏感：针对负样本做随机采样，或者针对正样本做随机采样，或者全局做随机采样，保证随机采样后正负样本分布不变，这个时候auc对采样不敏感。\n",
    "\n",
    "- 交叉熵跟MSE有什么区别\n",
    "  - 一个用于分类任务，一个用于回归任务；\n",
    "  - MSE是假设数据符合高斯分布时,模型概率分布的负条件对数似然;\n",
    "  - 交叉熵是假设模型分布为多项式分布时,模型分布的负条件对数似然；\n",
    "  - MSE无差别得关注全部类别上预测概率和真实概率的差；\n",
    "  - 交叉熵关注的是正确类别的预测概率；\n",
    "\n",
    "- micro-f1解释\n",
    "  - micro f1不需要区分类别，直接使用总体样本的准召计算f1 score；\n",
    "  - 在推荐系统中，种类中数量较多的商品会对f1造成更大的影响力；\n",
    "  - Macro F1分类别计算精确率和召回率，求均值后计算f1；\n",
    "\n",
    "- 介绍下排序指标ndcg\n",
    "  - 归一化折损累计增益，NDCG用作排序结果的评价指标，这个指标通常是用来衡量和评价搜索结果算法；\n",
    "  - ndcg@n 只关心前n个排序是否正确，后面的排序正不正确不予考虑。ndcg@n 的计算方式比较特别，要进行两次排序，一次是对预测的结果排序，另一次是对实际的分布排序；\n",
    "\n",
    "- 回归指标应该用什么\n",
    "  - 除了MSE(mean squared error，均方误差)，还可以尝试使用诸如Pearson's linear coefficient，Earth Mover's Distance (Wasserstein distance)，KL-divergence，histogram intersection，cosine distance等等多种标准。注意这些标准有的是距离（数值越大越不相似），有的是相似度（相反）。当两个数据分布真正相似或不相似时，其在不同的距离度量下应该有一致的表现。\n",
    "  从另一个看，这个问题其实也是度量学习(metrics learning)的研究方向之一。定义一个针对特定高维数据的有效度量总是困难的。对于非专门研究者，同时使用多种简单的度量来说明问题应该是一个成本最低、相对可靠性高的选择。\n",
    "\n",
    "- AUC和precision,recall，F1的区别，不同情况怎么选择指标\n",
    "  - 如上\n",
    "\n",
    "- Group auc了解嘛\n",
    "  - 而在推荐广告领域，我们实际要衡量的是不同用户对不同item之间的排序能力，\n",
    "    **因此实际应该更关注的是同一个用户对不同广告间的排序能力。**\n",
    "    GAUC（group auc）实际是计算每个用户的auc，然后加权平均，最后得到**group auc**，这样就能**减少不同用户间的排序结果不太好比较**这一影响。\n",
    "\n",
    "- 给数据计算AUC\n",
    "  - 用上文LogN或者N方的方法\n",
    "\n",
    "- 分类评价指标，TPR,FPR等的含义\n",
    "  - TPR=TP/(TP+FN)\n",
    "    FPR=FP/(FP+TN)\n",
    "\n",
    "  - 真阳性(True Positive Rate)：分类器正确分类且本身为正例\n",
    "    真阴性(True Negative Rate)：分类器正确分类且本身为负例\n",
    "    假阳性(False Positive Rate)：分类器错误分类本身为负例\n",
    "    假阴性(False Negative Rate)：分类器错误分类本身为正例"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}