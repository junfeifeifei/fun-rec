{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e4903c",
   "metadata": {},
   "source": [
    "# 排序模型\n",
    "\n",
    "通过召回的操作， 我们已经进行了问题规模的缩减， 对于每个用户， 选择出了N篇文章作为了候选集，并基于召回的候选集构建了与用户历史相关的特征，以及用户本身的属性特征，文章本省的属性特征，以及用户与文章之间的特征，下面就是使用机器学习模型来对构造好的特征进行学习，然后对测试集进行预测，得到测试集中的每个候选集用户点击的概率，返回点击概率最大的topk个文章，作为最终的结果。\n",
    "\n",
    "排序阶段选择了三个比较有代表性的排序模型，它们分别是：\n",
    "\n",
    "1. LGB的排序模型\n",
    "2. LGB的分类模型\n",
    "3. 深度学习的分类模型DIN\n",
    "\n",
    "得到了最终的排序模型输出的结果之后，还选择了两种比较经典的模型集成的方法：\n",
    "\n",
    "1. 输出结果加权融合\n",
    "2. Staking（将模型的输出结果再使用一个简单模型进行预测）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8d37a",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gc, os\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca59b5",
   "metadata": {},
   "source": [
    "## 读取排序特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e377ec6c",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "base_path = Path('../tmp/projects/news_recommendation')\n",
    "data_path = base_path / 'data_raw'\n",
    "save_path = base_path / 'temp_results'\n",
    "if not save_path.exists():\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "offline = False\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723d87f",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 重新读取数据的时候，发现click_article_id是一个浮点数，所以将其转换成int类型\n",
    "trn_user_item_feats_df = pd.read_csv(save_path / 'trn_user_item_feats_df.csv')\n",
    "trn_user_item_feats_df['click_article_id'] = trn_user_item_feats_df['click_article_id'].astype(int)\n",
    "\n",
    "if offline:\n",
    "    val_user_item_feats_df = pd.read_csv(save_path / 'val_user_item_feats_df.csv')\n",
    "    val_user_item_feats_df['click_article_id'] = val_user_item_feats_df['click_article_id'].astype(int)\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "    \n",
    "tst_user_item_feats_df = pd.read_csv(save_path / 'tst_user_item_feats_df.csv')\n",
    "tst_user_item_feats_df['click_article_id'] = tst_user_item_feats_df['click_article_id'].astype(int)\n",
    "\n",
    "# 做特征的时候为了方便，给测试集也打上了一个无效的标签，这里直接删掉就行\n",
    "del tst_user_item_feats_df['label']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2ab6b",
   "metadata": {},
   "source": [
    "## 返回排序后的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35cf708",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "def submit(recall_df, topk=5, model_name=None):\n",
    "    recall_df = recall_df.sort_values(by=['user_id', 'pred_score'])\n",
    "    recall_df['rank'] = recall_df.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "    \n",
    "    # 判断是不是每个用户都有5篇文章及以上\n",
    "    tmp = recall_df.groupby('user_id').apply(lambda x: x['rank'].max())\n",
    "    assert tmp.min() >= topk\n",
    "    \n",
    "    del recall_df['pred_score']\n",
    "    submit = recall_df[recall_df['rank'] <= topk].set_index(['user_id', 'rank']).unstack(-1).reset_index()\n",
    "    \n",
    "    submit.columns = [int(col) if isinstance(col, int) else col for col in submit.columns.droplevel(0)]\n",
    "    # 按照提交格式定义列名\n",
    "    submit = submit.rename(columns={'': 'user_id', 1: 'article_1', 2: 'article_2', \n",
    "                                                  3: 'article_3', 4: 'article_4', 5: 'article_5'})\n",
    "    \n",
    "    save_name = save_path / (model_name + '_' + datetime.today().strftime('%m-%d') + '.csv')\n",
    "    submit.to_csv(save_name, index=False, header=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f043a1",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 排序结果归一化\n",
    "def norm_sim(sim_df, weight=0.0):\n",
    "    # print(sim_df.head())\n",
    "    min_sim = sim_df.min()\n",
    "    max_sim = sim_df.max()\n",
    "    if max_sim == min_sim:\n",
    "        sim_df = sim_df.apply(lambda sim: 1.0)\n",
    "    else:\n",
    "        sim_df = sim_df.apply(lambda sim: 1.0 * (sim - min_sim) / (max_sim - min_sim))\n",
    "\n",
    "    sim_df = sim_df.apply(lambda sim: sim + weight)  # plus one\n",
    "    return sim_df\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6aa2f1",
   "metadata": {},
   "source": [
    "## LGB排序模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd67c81",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 防止中间出错之后重新读取数据\n",
    "trn_user_item_feats_df_rank_model = trn_user_item_feats_df.copy()\n",
    "\n",
    "if offline:\n",
    "    val_user_item_feats_df_rank_model = val_user_item_feats_df.copy()\n",
    "    \n",
    "tst_user_item_feats_df_rank_model = tst_user_item_feats_df.copy()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdebba9",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 定义特征列\n",
    "lgb_cols = ['sim0', 'time_diff0', 'word_diff0','sim_max', 'sim_min', 'sim_sum', \n",
    "            'sim_mean', 'score','click_size', 'time_diff_mean', 'active_level',\n",
    "            'click_environment','click_deviceGroup', 'click_os', 'click_country', \n",
    "            'click_region','click_referrer_type', 'user_time_hob1', 'user_time_hob2',\n",
    "            'words_hbo', 'category_id', 'created_at_ts','words_count']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e174d",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 排序模型分组\n",
    "trn_user_item_feats_df_rank_model.sort_values(by=['user_id'], inplace=True)\n",
    "g_train = trn_user_item_feats_df_rank_model.groupby(['user_id'], as_index=False).count()[\"label\"].values\n",
    "\n",
    "if offline:\n",
    "    val_user_item_feats_df_rank_model.sort_values(by=['user_id'], inplace=True)\n",
    "    g_val = val_user_item_feats_df_rank_model.groupby(['user_id'], as_index=False).count()[\"label\"].values\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9622e47c",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 排序模型定义\n",
    "lgb_ranker = lgb.LGBMRanker(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n",
    "                            max_depth=-1, n_estimators=100, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16)  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e861b7",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 排序模型训练\n",
    "if offline:\n",
    "    lgb_ranker.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model['label'], group=g_train,\n",
    "                eval_set=[(val_user_item_feats_df_rank_model[lgb_cols], val_user_item_feats_df_rank_model['label'])], \n",
    "                eval_group= [g_val], eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', ])\n",
    "else:\n",
    "    lgb_ranker.fit(trn_user_item_feats_df[lgb_cols], trn_user_item_feats_df['label'], group=g_train)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce474cd",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 模型预测\n",
    "tst_user_item_feats_df['pred_score'] = lgb_ranker.predict(tst_user_item_feats_df[lgb_cols], num_iteration=lgb_ranker.best_iteration_)\n",
    "\n",
    "# 将这里的排序结果保存一份，用户后面的模型融合\n",
    "tst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']].to_csv(save_path / 'lgb_ranker_score.csv', index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf708291",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 预测结果重新排序, 及生成提交结果\n",
    "rank_results = tst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']]\n",
    "rank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\n",
    "submit(rank_results, topk=5, model_name='lgb_ranker')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbce78",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 五折交叉验证，这里的五折交叉是以用户为目标进行五折划分\n",
    "#  这一部分与前面的单独训练和验证是分开的\n",
    "def get_kfold_users(trn_df, n=5):\n",
    "    user_ids = trn_df['user_id'].unique()\n",
    "    user_set = [user_ids[i::n] for i in range(n)]\n",
    "    return user_set\n",
    "\n",
    "k_fold = 5\n",
    "trn_df = trn_user_item_feats_df_rank_model\n",
    "user_set = get_kfold_users(trn_df, n=k_fold)\n",
    "\n",
    "score_list = []\n",
    "score_df = trn_df[['user_id', 'click_article_id','label']]\n",
    "sub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[0])\n",
    "\n",
    "# 五折交叉验证，并将中间结果保存用于staking\n",
    "for n_fold, valid_user in enumerate(user_set):\n",
    "    train_idx = trn_df[~trn_df['user_id'].isin(valid_user)] # add slide user\n",
    "    valid_idx = trn_df[trn_df['user_id'].isin(valid_user)]\n",
    "    \n",
    "    # 训练集与验证集的用户分组\n",
    "    train_idx.sort_values(by=['user_id'], inplace=True)\n",
    "    g_train = train_idx.groupby(['user_id'], as_index=False).count()[\"label\"].values\n",
    "    \n",
    "    valid_idx.sort_values(by=['user_id'], inplace=True)\n",
    "    g_val = valid_idx.groupby(['user_id'], as_index=False).count()[\"label\"].values\n",
    "    \n",
    "    # 定义模型\n",
    "    lgb_ranker = lgb.LGBMRanker(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n",
    "                            max_depth=-1, n_estimators=100, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16)  \n",
    "    # 训练模型\n",
    "    lgb_ranker.fit(train_idx[lgb_cols], train_idx['label'], group=g_train,\n",
    "                   eval_set=[(valid_idx[lgb_cols], valid_idx['label'])], eval_group= [g_val], \n",
    "                   eval_at=[1, 2, 3, 4, 5], eval_metric=['ndcg', ])\n",
    "    \n",
    "    # 预测验证集结果\n",
    "    valid_idx['pred_score'] = lgb_ranker.predict(valid_idx[lgb_cols], num_iteration=lgb_ranker.best_iteration_)\n",
    "    \n",
    "    # 对输出结果进行归一化\n",
    "    valid_idx['pred_score'] = valid_idx[['pred_score']].transform(lambda x: norm_sim(x))\n",
    "    \n",
    "    valid_idx.sort_values(by=['user_id', 'pred_score'])\n",
    "    valid_idx['pred_rank'] = valid_idx.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "    \n",
    "    # 将验证集的预测结果放到一个列表中，后面进行拼接\n",
    "    score_list.append(valid_idx[['user_id', 'click_article_id', 'pred_score', 'pred_rank']])\n",
    "    \n",
    "    # 如果是线上测试，需要计算每次交叉验证的结果相加，最后求平均\n",
    "    if not offline:\n",
    "        sub_preds += lgb_ranker.predict(tst_user_item_feats_df_rank_model[lgb_cols], lgb_ranker.best_iteration_)\n",
    "    \n",
    "score_df_ = pd.concat(score_list, axis=0)\n",
    "score_df = score_df.merge(score_df_, how='left', on=['user_id', 'click_article_id'])\n",
    "# 保存训练集交叉验证产生的新特征\n",
    "score_df[['user_id', 'click_article_id', 'pred_score', 'pred_rank', 'label']].to_csv(save_path / 'trn_lgb_ranker_feats.csv', index=False)\n",
    "    \n",
    "# 测试集的预测结果，多次交叉验证求平均,将预测的score和对应的rank特征保存，可以用于后面的staking，这里还可以构造其他更多的特征\n",
    "tst_user_item_feats_df_rank_model['pred_score'] = sub_preds / k_fold\n",
    "tst_user_item_feats_df_rank_model['pred_score'] = tst_user_item_feats_df_rank_model['pred_score'].transform(lambda x: norm_sim(x))\n",
    "tst_user_item_feats_df_rank_model.sort_values(by=['user_id', 'pred_score'])\n",
    "tst_user_item_feats_df_rank_model['pred_rank'] = tst_user_item_feats_df_rank_model.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "# 保存测试集交叉验证的新特征\n",
    "tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score', 'pred_rank']].to_csv(save_path / 'tst_lgb_ranker_feats.csv', index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c8c39",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 预测结果重新排序, 及生成提交结果\n",
    "# 单模型生成提交结果\n",
    "rank_results = tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score']]\n",
    "rank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\n",
    "submit(rank_results, topk=5, model_name='lgb_ranker')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3795b",
   "metadata": {},
   "source": [
    "## LGB分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2bb54",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 模型及参数的定义\n",
    "lgb_Classfication = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n",
    "                            max_depth=-1, n_estimators=500, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16, verbose=10)  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b39dc9",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 模型训练\n",
    "if offline:\n",
    "    lgb_Classfication.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model['label'],\n",
    "                    eval_set=[(val_user_item_feats_df_rank_model[lgb_cols], val_user_item_feats_df_rank_model['label'])], \n",
    "                    eval_metric=['auc', ])\n",
    "else:\n",
    "    lgb_Classfication.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model['label'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632dd52",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 模型预测\n",
    "tst_user_item_feats_df['pred_score'] = lgb_Classfication.predict_proba(tst_user_item_feats_df[lgb_cols])[:,1]\n",
    "\n",
    "# 将这里的排序结果保存一份，用户后面的模型融合\n",
    "tst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']].to_csv(save_path / 'lgb_cls_score.csv', index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2e662",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 预测结果重新排序, 及生成提交结果\n",
    "rank_results = tst_user_item_feats_df[['user_id', 'click_article_id', 'pred_score']]\n",
    "rank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\n",
    "submit(rank_results, topk=5, model_name='lgb_cls')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47534015",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 五折交叉验证，这里的五折交叉是以用户为目标进行五折划分\n",
    "#  这一部分与前面的单独训练和验证是分开的\n",
    "def get_kfold_users(trn_df, n=5):\n",
    "    user_ids = trn_df['user_id'].unique()\n",
    "    user_set = [user_ids[i::n] for i in range(n)]\n",
    "    return user_set\n",
    "\n",
    "k_fold = 5\n",
    "trn_df = trn_user_item_feats_df_rank_model\n",
    "user_set = get_kfold_users(trn_df, n=k_fold)\n",
    "\n",
    "score_list = []\n",
    "score_df = trn_df[['user_id', 'click_article_id', 'label']]\n",
    "sub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[0])\n",
    "\n",
    "# 五折交叉验证，并将中间结果保存用于staking\n",
    "for n_fold, valid_user in enumerate(user_set):\n",
    "    train_idx = trn_df[~trn_df['user_id'].isin(valid_user)] # add slide user\n",
    "    valid_idx = trn_df[trn_df['user_id'].isin(valid_user)]\n",
    "    \n",
    "    # 模型及参数的定义\n",
    "    lgb_Classfication = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n",
    "                            max_depth=-1, n_estimators=100, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "                            learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs= 16, verbose=10)  \n",
    "    # 训练模型\n",
    "    lgb_Classfication.fit(train_idx[lgb_cols], train_idx['label'],eval_set=[(valid_idx[lgb_cols], valid_idx['label'])], \n",
    "                          eval_metric=['auc', ])\n",
    "    \n",
    "    # 预测验证集结果\n",
    "    valid_idx['pred_score'] = lgb_Classfication.predict_proba(valid_idx[lgb_cols], \n",
    "                                                              num_iteration=lgb_Classfication.best_iteration_)[:,1]\n",
    "    \n",
    "    # 对输出结果进行归一化 分类模型输出的值本身就是一个概率值不需要进行归一化\n",
    "    # valid_idx['pred_score'] = valid_idx[['pred_score']].transform(lambda x: norm_sim(x))\n",
    "    \n",
    "    valid_idx.sort_values(by=['user_id', 'pred_score'])\n",
    "    valid_idx['pred_rank'] = valid_idx.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "    \n",
    "    # 将验证集的预测结果放到一个列表中，后面进行拼接\n",
    "    score_list.append(valid_idx[['user_id', 'click_article_id', 'pred_score', 'pred_rank']])\n",
    "    \n",
    "    # 如果是线上测试，需要计算每次交叉验证的结果相加，最后求平均\n",
    "    if not offline:\n",
    "        sub_preds += lgb_Classfication.predict_proba(tst_user_item_feats_df_rank_model[lgb_cols], \n",
    "                                                     num_iteration=lgb_Classfication.best_iteration_)[:,1]\n",
    "    \n",
    "score_df_ = pd.concat(score_list, axis=0)\n",
    "score_df = score_df.merge(score_df_, how='left', on=['user_id', 'click_article_id'])\n",
    "# 保存训练集交叉验证产生的新特征\n",
    "score_df[['user_id', 'click_article_id', 'pred_score', 'pred_rank', 'label']].to_csv(save_path / 'trn_lgb_cls_feats.csv', index=False)\n",
    "    \n",
    "# 测试集的预测结果，多次交叉验证求平均,将预测的score和对应的rank特征保存，可以用于后面的staking，这里还可以构造其他更多的特征\n",
    "tst_user_item_feats_df_rank_model['pred_score'] = sub_preds / k_fold\n",
    "tst_user_item_feats_df_rank_model['pred_score'] = tst_user_item_feats_df_rank_model['pred_score'].transform(lambda x: norm_sim(x))\n",
    "tst_user_item_feats_df_rank_model.sort_values(by=['user_id', 'pred_score'])\n",
    "tst_user_item_feats_df_rank_model['pred_rank'] = tst_user_item_feats_df_rank_model.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "# 保存测试集交叉验证的新特征\n",
    "tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score', 'pred_rank']].to_csv(save_path / 'tst_lgb_cls_feats.csv', index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437e6bf",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 预测结果重新排序, 及生成提交结果\n",
    "rank_results = tst_user_item_feats_df_rank_model[['user_id', 'click_article_id', 'pred_score']]\n",
    "rank_results['click_article_id'] = rank_results['click_article_id'].astype(int)\n",
    "submit(rank_results, topk=5, model_name='lgb_cls')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630fd45",
   "metadata": {},
   "source": [
    "## DIN模型\n",
    "\n",
    "### 用户的历史点击行为列表\n",
    "\n",
    "这个是为后面的DIN模型服务的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82070357",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "if offline:\n",
    "    all_data = pd.read_csv(data_path / 'train_click_log.csv')\n",
    "else:\n",
    "    trn_data = pd.read_csv(data_path / 'train_click_log.csv')\n",
    "    tst_data = pd.read_csv(data_path / 'testA_click_log.csv')\n",
    "    all_data = pd.concat([trn_data, tst_data]).reset_index(drop=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d264a5",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "hist_click =all_data[['user_id', 'click_article_id']].groupby('user_id').agg({list}).reset_index()\n",
    "his_behavior_df = pd.DataFrame()\n",
    "his_behavior_df['user_id'] = hist_click['user_id']\n",
    "his_behavior_df['hist_click_article_id'] = hist_click['click_article_id']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2b0ff",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "trn_user_item_feats_df_din_model = trn_user_item_feats_df.copy()\n",
    "\n",
    "if offline:\n",
    "    val_user_item_feats_df_din_model = val_user_item_feats_df.copy()\n",
    "else: \n",
    "    val_user_item_feats_df_din_model = None\n",
    "    \n",
    "tst_user_item_feats_df_din_model = tst_user_item_feats_df.copy()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25f35f",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "trn_user_item_feats_df_din_model = trn_user_item_feats_df_din_model.merge(his_behavior_df, on='user_id')\n",
    "\n",
    "if offline:\n",
    "    val_user_item_feats_df_din_model = val_user_item_feats_df_din_model.merge(his_behavior_df, on='user_id')\n",
    "else:\n",
    "    val_user_item_feats_df_din_model = None\n",
    "\n",
    "tst_user_item_feats_df_din_model = tst_user_item_feats_df_din_model.merge(his_behavior_df, on='user_id')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d305ff7",
   "metadata": {},
   "source": [
    "### DIN模型简介\n",
    "\n",
    "我们下面尝试使用DIN模型， DIN的全称是Deep Interest Network， 这是阿里2018年基于前面的深度学习模型无法表达用户多样化的兴趣而提出的一个模型， 它可以通过考虑【给定的候选广告】和【用户的历史行为】的相关性，来计算用户兴趣的表示向量。具体来说就是通过引入局部激活单元，通过软搜索历史行为的相关部分来关注相关的用户兴趣，并采用加权和来获得有关候选广告的用户兴趣的表示。与候选广告相关性较高的行为会获得较高的激活权重，并支配着用户兴趣。该表示向量在不同广告上有所不同，大大提高了模型的表达能力。所以该模型对于此次新闻推荐的任务也比较适合， 我们在这里通过当前的候选文章与用户历史点击文章的相关性来计算用户对于文章的兴趣。 该模型的结构如下：\n",
    "\n",
    "![DIN模型结构](../img/din_architecture.png)\n",
    "\n",
    "我们这里直接调包来使用这个模型， 关于这个模型的详细细节部分我们会在下一期的推荐系统组队学习中给出。下面说一下该模型如何具体使用：deepctr的函数原型如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0a525",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "def DIN(dnn_feature_columns, history_feature_list, dnn_use_bn=False,\n",
    "       dnn_hidden_units=(200, 80), dnn_activation='relu', att_hidden_size=(80, 40), att_activation=\"dice\",\n",
    "      att_weight_normalization=False, l2_reg_dnn=0, l2_reg_embedding=1e-6, dnn_dropout=0, seed=1024,\n",
    "       task='binary'):\n",
    "    pass\n",
    "    # * dnn_feature_columns: 特征列， 包含数据所有特征的列表\n",
    "    # * history_feature_list: 用户历史行为列， 反应用户历史行为的特征的列表\n",
    "    # * dnn_use_bn: 是否使用BatchNormalization\n",
    "    # * dnn_hidden_units: 全连接层网络的层数和每一层神经元的个数， 一个列表或者元组\n",
    "    # * dnn_activation_relu: 全连接网络的激活单元类型\n",
    "    # * att_hidden_size: 注意力层的全连接网络的层数和每一层神经元的个数\n",
    "    # * att_activation: 注意力层的激活单元类型\n",
    "    # * att_weight_normalization: 是否归一化注意力得分\n",
    "    # * l2_reg_dnn: 全连接网络的正则化系数\n",
    "    # * l2_reg_embedding: embedding向量的正则化稀疏\n",
    "    # * dnn_dropout: 全连接网络的神经元的失活概率\n",
    "    # * task: 任务， 可以是分类， 也可是是回归\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de4c9a",
   "metadata": {},
   "source": [
    "在具体使用的时候， 我们必须要传入特征列和历史行为列， 但是再传入之前， 我们需要进行一下特征列的预处理。具体如下：\n",
    "\n",
    "1. 首先，我们要处理数据集， 得到数据， 由于我们是基于用户过去的行为去预测用户是否点击当前文章， 所以我们需要把数据的特征列划分成数值型特征， 离散型特征和历史行为特征列三部分， 对于每一部分， DIN模型的处理会有不同\n",
    "    1. 对于离散型特征， 在我们的数据集中就是那些类别型的特征， 比如user_id这种， 这种类别型特征， 我们首先要经过embedding处理得到每个特征的低维稠密型表示， 既然要经过embedding， 那么我们就需要为每一列的类别特征的取值建立一个字典，并指明embedding维度， 所以在使用deepctr的DIN模型准备数据的时候， 我们需要通过SparseFeat函数指明这些类别型特征, 这个函数的传入参数就是列名， 列的唯一取值(建立字典用)和embedding维度。\n",
    "    2. 对于用户历史行为特征列， 比如文章id， 文章的类别等这种， 同样的我们需要先经过embedding处理， 只不过和上面不一样的地方是，对于这种特征， 我们在得到每个特征的embedding表示之后， 还需要通过一个Attention_layer计算用户的历史行为和当前候选文章的相关性以此得到当前用户的embedding向量， 这个向量就可以基于当前的候选文章与用户过去点击过得历史文章的相似性的程度来反应用户的兴趣， 并且随着用户的不同的历史点击来变化，去动态的模拟用户兴趣的变化过程。这类特征对于每个用户都是一个历史行为序列， 对于每个用户， 历史行为序列长度会不一样， 可能有的用户点击的历史文章多，有的点击的历史文章少， 所以我们还需要把这个长度统一起来， 在为DIN模型准备数据的时候， 我们首先要通过SparseFeat函数指明这些类别型特征， 然后还需要通过VarLenSparseFeat函数再进行序列填充， 使得每个用户的历史序列一样长， 所以这个函数参数中会有个maxlen，来指明序列的最大长度是多少。\n",
    "    3. 对于连续型特征列， 我们只需要用DenseFeat函数来指明列名和维度即可。\n",
    "2. 处理完特征列之后， 我们把相应的数据与列进行对应，就得到了最后的数据。\n",
    "\n",
    "下面根据具体的代码感受一下， 逻辑是这样， 首先我们需要写一个数据准备函数， 在这里面就是根据上面的具体步骤准备数据， 得到数据和特征列， 然后就是建立DIN模型并训练， 最后基于模型进行测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99acaeab",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import * \n",
    "import tensorflow as tf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a1c08",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 数据准备函数\n",
    "def get_din_feats_columns(df, dense_fea, sparse_fea, behavior_fea, his_behavior_fea, emb_dim=32, max_len=100):\n",
    "    \"\"\"\n",
    "    数据准备函数:\n",
    "    df: 数据集\n",
    "    dense_fea: 数值型特征列\n",
    "    sparse_fea: 离散型特征列\n",
    "    behavior_fea: 用户的候选行为特征列\n",
    "    his_behavior_fea: 用户的历史行为特征列\n",
    "    embedding_dim: embedding的维度， 这里为了简单， 统一把离散型特征列采用一样的隐向量维度\n",
    "    max_len: 用户序列的最大长度\n",
    "    \"\"\"\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, vocabulary_size=df[feat].nunique() + 1, embedding_dim=emb_dim) for feat in sparse_fea]\n",
    "    \n",
    "    dense_feature_columns = [DenseFeat(feat, 1, ) for feat in dense_fea]\n",
    "    \n",
    "    var_feature_columns = [VarLenSparseFeat(SparseFeat(feat, vocabulary_size=df['click_article_id'].nunique() + 1,\n",
    "                                    embedding_dim=emb_dim, embedding_name='click_article_id'), maxlen=max_len) for feat in hist_behavior_fea]\n",
    "    \n",
    "    dnn_feature_columns = sparse_feature_columns + dense_feature_columns + var_feature_columns\n",
    "    \n",
    "    # 建立x, x是一个字典的形式\n",
    "    x = {}\n",
    "    for name in get_feature_names(dnn_feature_columns):\n",
    "        if name in his_behavior_fea:\n",
    "            # 这是历史行为序列\n",
    "            his_list = [l for l in df[name]]\n",
    "            x[name] = pad_sequences(his_list, maxlen=max_len, padding='post')      # 二维数组\n",
    "        else:\n",
    "            x[name] = df[name].values\n",
    "    \n",
    "    return x, dnn_feature_columns\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa081d",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "def get_din_input_data(df, dense_fea, sparse_fea, behavior_fea, his_behavior_fea, emb_dim=32, max_len=100):\n",
    "    dnn_feature_columns = sparse_fea + dense_fea + his_behavior_fea\n",
    "    x = {}\n",
    "    # Sparse features (map if sparse_maps available)\n",
    "    for name in sparse_fea:\n",
    "        if 'sparse_maps' in globals() and name in sparse_maps:\n",
    "            x[name] = df[name].astype('int64').map(sparse_maps[name]).fillna(0).astype('int32').values\n",
    "        else:\n",
    "            x[name] = df[name].astype('int32').values\n",
    "    # Dense features\n",
    "    for name in dense_fea:\n",
    "        x[name] = df[name].astype('float32').values\n",
    "    # History sequence (map with click_article_id map if available)\n",
    "    for name in his_behavior_fea:\n",
    "        seq_list = df[name].tolist()\n",
    "        if 'sparse_maps' in globals() and 'click_article_id' in sparse_maps:\n",
    "            cmap = sparse_maps['click_article_id']\n",
    "            mapped_seq = []\n",
    "            for seq in seq_list:\n",
    "                if isinstance(seq, (list, np.ndarray)):\n",
    "                    mapped_seq.append([int(cmap.get(int(v), 0)) if int(v) != 0 else 0 for v in seq])\n",
    "                else:\n",
    "                    mapped_seq.append([0])\n",
    "            x[name] = pad_sequences(mapped_seq, maxlen=max_len, padding='post')\n",
    "        else:\n",
    "            x[name] = pad_sequences(seq_list, maxlen=max_len, padding='post')\n",
    "    return x, dnn_feature_columns\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e5765",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 把特征分开\n",
    "sparse_fea = ['user_id', 'click_article_id', 'category_id', 'click_environment', 'click_deviceGroup', \n",
    "              'click_os', 'click_country', 'click_region', 'click_referrer_type', 'is_cat_hab']\n",
    "\n",
    "behavior_fea = ['click_article_id']\n",
    "\n",
    "hist_behavior_fea = ['hist_click_article_id']\n",
    "\n",
    "dense_fea = ['sim0', 'time_diff0', 'word_diff0', 'sim_max', 'sim_min', 'sim_sum', 'sim_mean', 'score',\n",
    "             'rank','click_size','time_diff_mean','active_level','user_time_hob1','user_time_hob2',\n",
    "             'words_hbo','words_count']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fab7c9",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# dense特征进行归一化, 神经网络训练都需要将数值进行归一化处理\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "# 下面是做一些特殊处理，当在其他的地方出现无效值的时候，不处理无法进行归一化，刚开始可以先把他注释掉，在运行了下面的代码\n",
    "# 之后如果发现报错，应该先去想办法处理如何不出现inf之类的值\n",
    "# trn_user_item_feats_df_din_model.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "# tst_user_item_feats_df_din_model.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "for feat in dense_fea:\n",
    "    trn_user_item_feats_df_din_model[feat] = mm.fit_transform(trn_user_item_feats_df_din_model[[feat]])\n",
    "    \n",
    "    if val_user_item_feats_df_din_model is not None:\n",
    "        val_user_item_feats_df_din_model[feat] = mm.fit_transform(val_user_item_feats_df_din_model[[feat]])\n",
    "    \n",
    "    tst_user_item_feats_df_din_model[feat] = mm.fit_transform(tst_user_item_feats_df_din_model[[feat]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43412db5",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# funrec DIN 排序\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# FunRec 导入\n",
    "from funrec.features.feature_column import FeatureColumn\n",
    "from funrec.training.trainer import train_model as funrec_train_model\n",
    "\n",
    "# 1) 配置 (mirrors funrec/config/config_din.yaml)\n",
    "funrec_config = {\n",
    "    'training': {\n",
    "        'build_function': 'funrec.models.din.build_din_model',\n",
    "        'model_params': {\n",
    "            'dnn_units': [128, 64, 1],\n",
    "            'linear_logits': True,\n",
    "        },\n",
    "        'optimizer': 'adam',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['binary_accuracy'],\n",
    "        'batch_size': 128,\n",
    "        'epochs': 2,\n",
    "        'validation_split': 0.2,\n",
    "        'verbose': 1,\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2) 构建特征列\n",
    "emb_dim = 8\n",
    "max_len = 50\n",
    "\n",
    "def _nunique(series_list):\n",
    "    return int(pd.concat(series_list, ignore_index=True).nunique())\n",
    "\n",
    "# 使用已有的splits\n",
    "df_trn = trn_user_item_feats_df_din_model\n",
    "df_val = val_user_item_feats_df_din_model if val_user_item_feats_df_din_model is not None else None\n",
    "df_tst = tst_user_item_feats_df_din_model\n",
    "\n",
    "all_frames = [df_trn, df_tst] + ([df_val] if df_val is not None else [])\n",
    "\n",
    "# 为所有稀疏特征构建连续id映射\n",
    "sparse_maps = {}\n",
    "for feat in sparse_fea:\n",
    "    vals = pd.concat([f[feat].astype('int64') for f in all_frames], ignore_index=True)\n",
    "    if feat == 'click_article_id':        \n",
    "        hist_vals = []\n",
    "        for f in all_frames:            \n",
    "            for seq in f['hist_click_article_id']:\n",
    "                if isinstance(seq, (list, np.ndarray)):\n",
    "                    hist_vals.extend([int(x) for x in seq])\n",
    "        if len(hist_vals) > 0:\n",
    "            vals = pd.concat([vals, pd.Series(hist_vals, dtype='int64')], ignore_index=True)\n",
    "    codes, uniques = pd.factorize(vals, sort=False)\n",
    "    mapping = {int(val): int(idx) for idx, val in enumerate(uniques)}\n",
    "    if feat == 'click_article_id':\n",
    "        # 保留0用于序列填充\n",
    "        mapping = {k: v + 1 for k, v in mapping.items()}\n",
    "    sparse_maps[feat] = mapping\n",
    "\n",
    "def _vocab_size(mapping: dict) -> int:\n",
    "    return (max(mapping.values()) + 1) if mapping else 1\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# 稀疏特征 (dnn)\n",
    "for feat in sparse_fea:\n",
    "    vocab = _vocab_size(sparse_maps[feat])\n",
    "    feature_columns.append(\n",
    "        FeatureColumn(\n",
    "            name=feat,\n",
    "            emb_name=feat,\n",
    "            emb_dim=emb_dim,\n",
    "            vocab_size=vocab,\n",
    "            group=['dnn'],\n",
    "            type='sparse',\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 稠密特征\n",
    "for feat in dense_fea:\n",
    "    feature_columns.append(\n",
    "        FeatureColumn(\n",
    "            name=feat,\n",
    "            emb_name=None,\n",
    "            type='dense',\n",
    "            dimension=1,\n",
    "            group=['dnn'],\n",
    "            dtype='float32',\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 变长稀疏特征: hist_click_article_id使用click_article_id embedding\n",
    "click_vocab = _vocab_size(sparse_maps['click_article_id'])\n",
    "feature_columns.append(\n",
    "    FeatureColumn(\n",
    "        name='hist_click_article_id',\n",
    "        emb_name='click_article_id',\n",
    "        emb_dim=emb_dim,\n",
    "        vocab_size=click_vocab,\n",
    "        group=['dnn'],\n",
    "        type='varlen_sparse',\n",
    "        max_len=max_len,\n",
    "        combiner='mean,din',\n",
    "        att_key_name='click_article_id',\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3) 构建模型输入\n",
    "def build_model_input(df: pd.DataFrame) -> dict:\n",
    "    model_input = {}\n",
    "    # 稀疏特征\n",
    "    for feat in sparse_fea:\n",
    "        mapped = df[feat].astype('int64').map(sparse_maps[feat]).fillna(0).astype('int32').values\n",
    "        model_input[feat] = mapped\n",
    "    # 稠密特征\n",
    "    for feat in dense_fea:\n",
    "        model_input[feat] = df[feat].astype('float32').values\n",
    "    # 变长稀疏特征 (右填充)\n",
    "    raw_seq_list = df['hist_click_article_id'].tolist()\n",
    "    mapped_seq = []\n",
    "    cmap = sparse_maps['click_article_id']\n",
    "    for seq in raw_seq_list:\n",
    "        if isinstance(seq, (list, np.ndarray)):\n",
    "            mapped_seq.append([int(cmap.get(int(x), 0)) if int(x) != 0 else 0 for x in seq])\n",
    "        else:\n",
    "            mapped_seq.append([0])\n",
    "    model_input['hist_click_article_id'] = pad_sequences(mapped_seq, maxlen=max_len, padding='post').astype('int32')\n",
    "    return model_input\n",
    "\n",
    "x_trn_funrec = build_model_input(df_trn)\n",
    "y_trn_funrec = df_trn['label'].values.astype('float32')\n",
    "\n",
    "if offline and df_val is not None:\n",
    "    x_val_funrec = build_model_input(df_val)\n",
    "    y_val_funrec = df_val['label'].values.astype('float32')\n",
    "\n",
    "x_tst_funrec = build_model_input(df_tst)\n",
    "\n",
    "processed_data = {\n",
    "    'train': {\n",
    "        'features': x_trn_funrec,\n",
    "        'labels': y_trn_funrec,\n",
    "    },\n",
    "    'test': {\n",
    "        'features': x_tst_funrec,\n",
    "        'labels': {},\n",
    "    },\n",
    "}\n",
    "\n",
    "# 4) 训练FunRec DIN\n",
    "main_model, _, _ = funrec_train_model(funrec_config['training'], feature_columns, processed_data)\n",
    "\n",
    "# 5) 预测和保存结果\n",
    "tst_user_item_feats_df_din_model['pred_score'] = main_model.predict(x_tst_funrec, batch_size=256, verbose=1)\n",
    "tst_user_item_feats_df_din_model[['user_id', 'click_article_id', 'pred_score']].to_csv(save_path / 'din_rank_score.csv', index=False)\n",
    "\n",
    "rank_results = tst_user_item_feats_df_din_model[['user_id', 'click_article_id', 'pred_score']]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f2b7b",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 预测结果重新排序, 及生成提交结果\n",
    "submit(rank_results, topk=5, model_name='din')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3aac4e",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 五折交叉验证，这里的五折交叉是以用户为目标进行五折划分\n",
    "#  这一部分与前面的单独训练和验证是分开的\n",
    "def get_kfold_users(trn_df, n=5):\n",
    "    user_ids = trn_df['user_id'].unique()\n",
    "    user_set = [user_ids[i::n] for i in range(n)]\n",
    "    return user_set\n",
    "\n",
    "k_fold = 5\n",
    "trn_df = trn_user_item_feats_df_din_model\n",
    "user_set = get_kfold_users(trn_df, n=k_fold)\n",
    "\n",
    "score_list = []\n",
    "score_df = trn_df[['user_id', 'click_article_id', 'label']]\n",
    "sub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[0])\n",
    "\n",
    "dense_fea = [x for x in dense_fea if x != 'label']\n",
    "# x_tst, dnn_feature_columns = get_din_feats_columns(tst_user_item_feats_df_din_model, dense_fea, \n",
    "#                                                    sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "\n",
    "x_tst, _ = get_din_input_data(tst_user_item_feats_df_din_model, dense_fea, sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "\n",
    "# 五折交叉验证，并将中间结果保存用于staking\n",
    "for n_fold, valid_user in enumerate(user_set):\n",
    "    train_idx = trn_df[~trn_df['user_id'].isin(valid_user)] # add slide user\n",
    "    valid_idx = trn_df[trn_df['user_id'].isin(valid_user)]\n",
    "    \n",
    "    # 准备训练数据\n",
    "    # x_trn, dnn_feature_columns = get_din_feats_columns(train_idx, dense_fea, \n",
    "    #                                                    sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "    x_trn, _ = get_din_input_data(train_idx, dense_fea, sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "    y_trn = train_idx['label'].values\n",
    "    \n",
    "    # 准备验证数据\n",
    "    # x_val, dnn_feature_columns = get_din_feats_columns(valid_idx, dense_fea, \n",
    "    #                                                sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "    x_val, _ = get_din_input_data(valid_idx, dense_fea, sparse_fea, behavior_fea, hist_behavior_fea, max_len=50)\n",
    "    y_val = valid_idx['label'].values\n",
    "    \n",
    "    history = main_model.fit(x_trn, y_trn, verbose=1, epochs=2, validation_data=(x_val, y_val) , batch_size=256)\n",
    "    \n",
    "    # 预测验证集结果\n",
    "    valid_idx['pred_score'] = main_model.predict(x_val, verbose=1, batch_size=256).flatten()   \n",
    "    \n",
    "    valid_idx.sort_values(by=['user_id', 'pred_score'])\n",
    "    valid_idx['pred_rank'] = valid_idx.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "    \n",
    "    # 将验证集的预测结果放到一个列表中，后面进行拼接\n",
    "    score_list.append(valid_idx[['user_id', 'click_article_id', 'pred_score', 'pred_rank']])\n",
    "    \n",
    "    # 如果是线上测试，需要计算每次交叉验证的结果相加，最后求平均\n",
    "    if not offline:\n",
    "        sub_preds += main_model.predict(x_tst, verbose=1, batch_size=256).flatten()\n",
    "    \n",
    "score_df_ = pd.concat(score_list, axis=0)\n",
    "score_df = score_df.merge(score_df_, how='left', on=['user_id', 'click_article_id'])\n",
    "# 保存训练集交叉验证产生的新特征\n",
    "score_df[['user_id', 'click_article_id', 'pred_score', 'pred_rank', 'label']].to_csv(save_path / 'trn_din_cls_feats.csv', index=False)\n",
    "    \n",
    "# 测试集的预测结果，多次交叉验证求平均,将预测的score和对应的rank特征保存，可以用于后面的staking，这里还可以构造其他更多的特征\n",
    "tst_user_item_feats_df_din_model['pred_score'] = sub_preds / k_fold\n",
    "tst_user_item_feats_df_din_model['pred_score'] = tst_user_item_feats_df_din_model['pred_score'].transform(lambda x: norm_sim(x))\n",
    "tst_user_item_feats_df_din_model.sort_values(by=['user_id', 'pred_score'])\n",
    "tst_user_item_feats_df_din_model['pred_rank'] = tst_user_item_feats_df_din_model.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "\n",
    "# 保存测试集交叉验证的新特征\n",
    "tst_user_item_feats_df_din_model[['user_id', 'click_article_id', 'pred_score', 'pred_rank']].to_csv(save_path / 'tst_din_cls_feats.csv', index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9d9de",
   "metadata": {},
   "source": [
    "## 模型融合\n",
    "\n",
    "### 加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84437621",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 读取多个模型的排序结果文件\n",
    "lgb_ranker = pd.read_csv(save_path / 'lgb_ranker_score.csv')\n",
    "lgb_cls = pd.read_csv(save_path / 'lgb_cls_score.csv')\n",
    "din_ranker = pd.read_csv(save_path / 'din_rank_score.csv')\n",
    "\n",
    "# 这里也可以换成交叉验证输出的测试结果进行加权融合\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0236dca3",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "rank_model = {'lgb_ranker': lgb_ranker, \n",
    "              'lgb_cls': lgb_cls, \n",
    "              'din_ranker': din_ranker}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734dbed",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "def get_ensumble_predict_topk(rank_model, topk=5):\n",
    "    # final_recall = rank_model['lgb_cls'].append(rank_model['din_ranker'])\n",
    "    final_recall = pd.concat([rank_model['lgb_cls'], rank_model['din_ranker']]).reset_index(drop=True)\n",
    "    rank_model['lgb_ranker']['pred_score'] = rank_model['lgb_ranker']['pred_score'].transform(lambda x: norm_sim(x))\n",
    "    \n",
    "    # final_recall = final_recall.append(rank_model['lgb_ranker'])\n",
    "    final_recall = pd.concat([final_recall, rank_model['lgb_ranker']]).reset_index(drop=True)\n",
    "    final_recall = final_recall.groupby(['user_id', 'click_article_id'])['pred_score'].sum().reset_index()\n",
    "    \n",
    "    submit(final_recall, topk=topk, model_name='ensemble_fuse')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3844874",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "get_ensumble_predict_topk(rank_model)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68087854",
   "metadata": {},
   "source": [
    "### Staking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80879286",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 读取多个模型的交叉验证生成的结果文件\n",
    "# 训练集\n",
    "trn_lgb_ranker_feats = pd.read_csv(save_path / 'trn_lgb_ranker_feats.csv')\n",
    "trn_lgb_cls_feats = pd.read_csv(save_path / 'trn_lgb_cls_feats.csv')\n",
    "trn_din_cls_feats = pd.read_csv(save_path / 'trn_din_cls_feats.csv')\n",
    "\n",
    "# 测试集\n",
    "tst_lgb_ranker_feats = pd.read_csv(save_path / 'tst_lgb_ranker_feats.csv')\n",
    "tst_lgb_cls_feats = pd.read_csv(save_path / 'tst_lgb_cls_feats.csv')\n",
    "tst_din_cls_feats = pd.read_csv(save_path / 'tst_din_cls_feats.csv')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ac020",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 将多个模型输出的特征进行拼接\n",
    "\n",
    "finall_trn_ranker_feats = trn_lgb_ranker_feats[['user_id', 'click_article_id', 'label']]\n",
    "finall_tst_ranker_feats = tst_lgb_ranker_feats[['user_id', 'click_article_id']]\n",
    "\n",
    "for idx, trn_model in enumerate([trn_lgb_ranker_feats, trn_lgb_cls_feats, trn_din_cls_feats]):\n",
    "    for feat in [ 'pred_score', 'pred_rank']:\n",
    "        col_name = feat + '_' + str(idx)\n",
    "        finall_trn_ranker_feats[col_name] = trn_model[feat]\n",
    "\n",
    "for idx, tst_model in enumerate([tst_lgb_ranker_feats, tst_lgb_cls_feats, tst_din_cls_feats]):\n",
    "    for feat in [ 'pred_score', 'pred_rank']:\n",
    "        col_name = feat + '_' + str(idx)\n",
    "        finall_tst_ranker_feats[col_name] = tst_model[feat]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff51eae",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "# 定义一个逻辑回归模型再次拟合交叉验证产生的特征对测试集进行预测\n",
    "# 这里需要注意的是，在做交叉验证的时候可以构造多一些与输出预测值相关的特征，来丰富这里简单模型的特征\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "feat_cols = ['pred_score_0', 'pred_rank_0', 'pred_score_1', 'pred_rank_1', 'pred_score_2', 'pred_rank_2']\n",
    "\n",
    "trn_x = finall_trn_ranker_feats[feat_cols]\n",
    "trn_y = finall_trn_ranker_feats['label']\n",
    "\n",
    "tst_x = finall_tst_ranker_feats[feat_cols]\n",
    "\n",
    "# 采样50000行数据 因为全量数据太大了\n",
    "sample_indices = trn_x.sample(n=50000, random_state=42).index\n",
    "trn_x_sample = trn_x.loc[sample_indices]\n",
    "trn_y_sample = trn_y.loc[sample_indices]\n",
    "\n",
    "print(f\"Original training data shape: {trn_x.shape}\")\n",
    "print(f\"Sampled training data shape: {trn_x_sample.shape}\")\n",
    "\n",
    "# 定义模型\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# 模型训练\n",
    "lr.fit(trn_x_sample, trn_y_sample)\n",
    "\n",
    "# 模型预测\n",
    "test_score = []\n",
    "test_batch_size = 10000\n",
    "for i in tqdm(range(0, len(tst_x), test_batch_size), total=len(tst_x)//test_batch_size, desc=\"Predicting test score\"):\n",
    "    test_score.append(lr.predict_proba(tst_x.iloc[i:i+test_batch_size])[:, 1])\n",
    "\n",
    "finall_tst_ranker_feats['pred_score'] = np.concatenate(test_score)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4243d6",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本章主要学习了三个排序模型，包括LGB的Rank， LGB的Classifier还有深度学习的DIN模型， 当然，对于这三个模型的原理部分，我们并没有给出详细的介绍， 请大家课下自己探索原理，也欢迎大家把自己的探索与所学分享出来，我们一块学习和进步。最后，我们进行了简单的模型融合策略，包括简单的加权和Stacking。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}