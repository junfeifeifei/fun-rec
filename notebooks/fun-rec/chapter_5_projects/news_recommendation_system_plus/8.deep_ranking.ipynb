{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. 深度排序（DeepFM / DIN）\n",
        "\n",
        "这一节把基础版的 `LGBMRanker` 排序升级为深度学习排序（工业常见）：\n",
        "\n",
        "- **DeepFM**：FM 二阶交叉 + DNN 高阶交叉\n",
        "- **DIN**：对用户历史序列做注意力（兴趣与候选 item 相关）\n",
        "\n",
        "并补齐更贴近生产的流程：\n",
        "\n",
        "1) 召回候选集（可融合 ItemCF / 双塔 / 热门）\n",
        "\n",
        "2) 特征工程（复用基础版逻辑）\n",
        "\n",
        "3) 深度排序训练与评估（HitRate@K / NDCG@K）\n",
        "\n",
        "## 面试要点\n",
        "\n",
        "- 为什么要两阶段（Recall→Rank）：效率 vs 效果\n",
        "- DeepFM vs DIN：特征交叉 vs 序列兴趣\n",
        "- 训练样本怎么构造：候选集 + 正负样本\n",
        "- 评估：按 user 分组的 TopK 指标（避免把样本当独立）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "from tqdm import tqdm\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    cur = start\n",
        "    for _ in range(10):\n",
        "        if (cur / 'pyproject.toml').exists() or (cur / '.git').exists():\n",
        "            return cur\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "dotenv_path = find_dotenv(usecwd=True)\n",
        "if dotenv_path:\n",
        "    load_dotenv(dotenv_path)\n",
        "os.environ.setdefault('FUNREC_RAW_DATA_PATH', str(REPO_ROOT / 'data'))\n",
        "os.environ.setdefault('FUNREC_PROCESSED_DATA_PATH', str(REPO_ROOT / 'tmp'))\n",
        "\n",
        "RAW_DATA_PATH = Path(os.getenv('FUNREC_RAW_DATA_PATH'))\n",
        "PROCESSED_DATA_PATH = Path(os.getenv('FUNREC_PROCESSED_DATA_PATH'))\n",
        "\n",
        "DATA_PATH = RAW_DATA_PATH / 'dataset' / 'news_recommendation'\n",
        "if not DATA_PATH.exists():\n",
        "    DATA_PATH = RAW_DATA_PATH / 'news_recommendation'\n",
        "\n",
        "PROJECT_PATH = PROCESSED_DATA_PATH / 'projects' / 'news_recommendation_system'\n",
        "ARTIFACTS_DIR = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models'\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_PATH, PROJECT_PATH, ARTIFACTS_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 读取基础数据与离线切分 ====================\n",
        "train_hist = pd.read_pickle(PROJECT_PATH / 'train_hist.pkl')\n",
        "valid_last = pd.read_pickle(PROJECT_PATH / 'valid_last.pkl')\n",
        "articles = pd.read_csv(DATA_PATH / 'articles.csv')\n",
        "\n",
        "# 用户历史序列（用于 DIN）\n",
        "train_hist_sorted = train_hist.sort_values(['user_id', 'click_timestamp'])\n",
        "user_hist: Dict[int, List[int]] = train_hist_sorted.groupby('user_id')['click_article_id'].apply(list).to_dict()\n",
        "\n",
        "len(train_hist), len(valid_last), len(user_hist)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 召回候选集（可融合多路 recall）\n",
        "\n",
        "你可以把候选集理解为“精排的检索空间”。本 notebook 支持：\n",
        "\n",
        "- 基础版候选：`recall_candidates.pkl`（来自 4.recall）\n",
        "- 双塔候选：`recall_candidates_two_tower.pkl`（来自 7.two_tower_recall）\n",
        "\n",
        "默认用 **RRF（倒数排名融合）** 合并。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_BASELINE_RECALL = True\n",
        "USE_TWO_TOWER_RECALL = True\n",
        "MAX_CANDIDATES = 100\n",
        "\n",
        "\n",
        "def load_recall_df(path: Path, name: str) -> pd.DataFrame:\n",
        "    if not path.exists():\n",
        "        print(f'[skip] {name}: {path} not found')\n",
        "        return pd.DataFrame(columns=['user_id', 'article_id', 'recall_score', 'source'])\n",
        "    df = pd.read_pickle(path)\n",
        "    df = df[['user_id', 'article_id', 'recall_score']].copy()\n",
        "    df['source'] = name\n",
        "    return df\n",
        "\n",
        "\n",
        "def rrf_merge(dfs: List[pd.DataFrame], weights: Dict[str, float], topk: int = 100) -> pd.DataFrame:\n",
        "    # Reciprocal Rank Fusion: sum(w / (rank+1))\n",
        "    merged = []\n",
        "    for df in dfs:\n",
        "        if df.empty:\n",
        "            continue\n",
        "        name = df['source'].iloc[0]\n",
        "        w = float(weights.get(name, 1.0))\n",
        "        tmp = df.copy()\n",
        "        tmp['rank'] = tmp.groupby('user_id')['recall_score'].rank(ascending=False, method='first')\n",
        "        tmp['rrf'] = w / (tmp['rank'] + 1.0)\n",
        "        merged.append(tmp[['user_id', 'article_id', 'rrf']])\n",
        "\n",
        "    if not merged:\n",
        "        raise ValueError('No recall sources available.')\n",
        "\n",
        "    merged = pd.concat(merged, axis=0, ignore_index=True)\n",
        "    merged = merged.groupby(['user_id', 'article_id'], as_index=False)['rrf'].sum()\n",
        "    merged = merged.rename(columns={'rrf': 'recall_score'})\n",
        "    merged['recall_rank'] = merged.groupby('user_id')['recall_score'].rank(ascending=False, method='first')\n",
        "    merged = merged[merged['recall_rank'] <= topk]\n",
        "    return merged\n",
        "\n",
        "\n",
        "sources = []\n",
        "weights = {}\n",
        "\n",
        "if USE_BASELINE_RECALL:\n",
        "    df_base = load_recall_df(PROJECT_PATH / 'recall_candidates.pkl', 'baseline')\n",
        "    if not df_base.empty:\n",
        "        sources.append(df_base)\n",
        "        weights['baseline'] = 1.0\n",
        "\n",
        "if USE_TWO_TOWER_RECALL:\n",
        "    df_tt = load_recall_df(PROJECT_PATH / 'recall_candidates_two_tower.pkl', 'two_tower')\n",
        "    if not df_tt.empty:\n",
        "        sources.append(df_tt)\n",
        "        weights['two_tower'] = 1.0\n",
        "\n",
        "recall_df = rrf_merge(sources, weights=weights, topk=MAX_CANDIDATES)\n",
        "recall_df.head(), recall_df['user_id'].nunique(), len(recall_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 特征工程（与基础版一致）\n",
        "\n",
        "深度排序与树模型一样，需要构造用户/物品/交互/召回特征。\n",
        "\n",
        "为了保持可复现与可对比，这里尽量复用基础版 `5.feature_engineering.ipynb` 的逻辑。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 用户特征\n",
        "user_click_count = train_hist.groupby('user_id').size().rename('user_click_count')\n",
        "user_unique_items = train_hist.groupby('user_id')['click_article_id'].nunique().rename('user_unique_items')\n",
        "user_last_click_ts = train_hist.groupby('user_id')['click_timestamp'].max().rename('user_last_click_ts')\n",
        "\n",
        "click_with_cat = train_hist.merge(articles, left_on='click_article_id', right_on='article_id', how='left')\n",
        "user_top_category = click_with_cat.groupby('user_id')['category_id'].agg(lambda x: x.value_counts().idxmax()).rename('user_top_category')\n",
        "\n",
        "user_features = pd.concat([user_click_count, user_unique_items, user_last_click_ts, user_top_category], axis=1).reset_index()\n",
        "\n",
        "# 物品特征\n",
        "item_click_count = train_hist.groupby('click_article_id').size().rename('item_click_count')\n",
        "item_last_click_ts = train_hist.groupby('click_article_id')['click_timestamp'].max().rename('item_last_click_ts')\n",
        "\n",
        "item_features = (\n",
        "    articles.merge(item_click_count, left_on='article_id', right_index=True, how='left')\n",
        "    .merge(item_last_click_ts, left_on='article_id', right_index=True, how='left')\n",
        ")\n",
        "item_features['item_click_count'] = item_features['item_click_count'].fillna(0)\n",
        "item_features['item_last_click_ts'] = item_features['item_last_click_ts'].fillna(0)\n",
        "\n",
        "# 用户最后一次点击（基于 train_hist）\n",
        "user_last_click = (\n",
        "    train_hist.sort_values(['user_id', 'click_timestamp'])\n",
        "    .groupby('user_id')\n",
        "    .tail(1)[['user_id', 'click_article_id', 'click_timestamp']]\n",
        "    .rename(columns={'click_article_id': 'last_click_article_id', 'click_timestamp': 'last_click_timestamp'})\n",
        ")\n",
        "\n",
        "# 合并成候选样本表\n",
        "candidates = (\n",
        "    recall_df.merge(user_features, on='user_id', how='left')\n",
        "    .merge(user_last_click, on='user_id', how='left')\n",
        "    .merge(item_features, left_on='article_id', right_on='article_id', how='left')\n",
        ")\n",
        "\n",
        "candidates['is_same_category'] = (candidates['category_id'] == candidates['user_top_category']).astype(int)\n",
        "candidates['item_age_hours'] = (candidates['last_click_timestamp'] - candidates['created_at_ts']) / 3600_000\n",
        "candidates['time_gap_hours'] = (candidates['last_click_timestamp'] - candidates['item_last_click_ts']) / 3600_000\n",
        "candidates[['item_age_hours', 'time_gap_hours']] = candidates[['item_age_hours', 'time_gap_hours']].fillna(0)\n",
        "\n",
        "# label：valid_last 里每个 user 的目标 item\n",
        "target = valid_last[['user_id', 'click_article_id']].rename(columns={'click_article_id': 'target_article_id'})\n",
        "candidates = candidates.merge(target, on='user_id', how='left')\n",
        "candidates['label'] = (candidates['article_id'] == candidates['target_article_id']).astype(int)\n",
        "\n",
        "candidates[['user_id', 'article_id', 'recall_score', 'recall_rank', 'label']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可选：内容向量相似度特征（emb_sim_last）\n",
        "USE_CONTENT_EMB_SIM = True\n",
        "\n",
        "if USE_CONTENT_EMB_SIM:\n",
        "    article_emb = pd.read_csv(DATA_PATH / 'articles_emb.csv')\n",
        "    emb_cols = [c for c in article_emb.columns if c.startswith('emb_')]\n",
        "    emb_matrix = article_emb[emb_cols].values.astype('float32')\n",
        "    emb_matrix /= np.linalg.norm(emb_matrix, axis=1, keepdims=True) + 1e-12\n",
        "    article_ids = article_emb['article_id'].values\n",
        "    id2idx = {int(a): int(i) for i, a in enumerate(article_ids)}\n",
        "\n",
        "    cand_idx = candidates['article_id'].map(id2idx)\n",
        "    last_idx = candidates['last_click_article_id'].map(id2idx)\n",
        "    mask = cand_idx.notna() & last_idx.notna()\n",
        "\n",
        "    sim = np.zeros(len(candidates), dtype='float32')\n",
        "    sim[mask.values] = (\n",
        "        emb_matrix[cand_idx[mask].astype(int)]\n",
        "        * emb_matrix[last_idx[mask].astype(int)]\n",
        "    ).sum(axis=1)\n",
        "    candidates['emb_sim_last'] = sim\n",
        "else:\n",
        "    candidates['emb_sim_last'] = 0.0\n",
        "\n",
        "candidates[['emb_sim_last']].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) 训练/验证划分（按 user 划分）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "users = candidates['user_id'].unique()\n",
        "rng.shuffle(users)\n",
        "split = int(len(users) * 0.8)\n",
        "train_users = set(users[:split])\n",
        "\n",
        "train_df = candidates[candidates['user_id'].isin(train_users)].copy()\n",
        "valid_df = candidates[~candidates['user_id'].isin(train_users)].copy()\n",
        "\n",
        "# 可选：限制训练规模（避免 notebook 太慢）\n",
        "MAX_TRAIN_ROWS = 300000 if USE_CONTENT_EMB_SIM else 500000\n",
        "if len(train_df) > MAX_TRAIN_ROWS:\n",
        "    train_df = train_df.sample(MAX_TRAIN_ROWS, random_state=42)\n",
        "\n",
        "train_df[['label']].value_counts(), valid_df[['label']].value_counts(), len(train_df), len(valid_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) DeepFM\n",
        "\n",
        "输入：\n",
        "\n",
        "- sparse：user_id, article_id, category_id, user_top_category, last_click_article_id\n",
        "- dense：recall_score, recall_rank, user/item 统计特征, 时间特征, emb_sim_last\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SPARSE_COLS = ['user_id', 'article_id', 'category_id', 'user_top_category', 'last_click_article_id']\n",
        "DENSE_COLS = [\n",
        "    'recall_score',\n",
        "    'recall_rank',\n",
        "    'user_click_count',\n",
        "    'user_unique_items',\n",
        "    'item_click_count',\n",
        "    'words_count',\n",
        "    'item_age_hours',\n",
        "    'time_gap_hours',\n",
        "    'emb_sim_last',\n",
        "    'is_same_category',\n",
        "]\n",
        "\n",
        "\n",
        "def encode_categorical(train_s: pd.Series, valid_s: pd.Series) -> Tuple[np.ndarray, np.ndarray, int, List[str]]:\n",
        "    all_s = pd.concat([train_s, valid_s], axis=0)\n",
        "    codes, uniques = pd.factorize(all_s.astype(str), sort=True)\n",
        "    codes = codes.astype(np.int32) + 1  # 0 reserved\n",
        "    train_codes = codes[: len(train_s)]\n",
        "    valid_codes = codes[len(train_s) :]\n",
        "    vocab_size = int(len(uniques) + 1)\n",
        "    return train_codes, valid_codes, vocab_size, list(uniques)\n",
        "\n",
        "\n",
        "# 编码 sparse\n",
        "train_sparse = {}\n",
        "valid_sparse = {}\n",
        "vocab_sizes = {}\n",
        "factor_uniques = {}\n",
        "for col in SPARSE_COLS:\n",
        "    tr, va, vs, uniq = encode_categorical(train_df[col].fillna(0), valid_df[col].fillna(0))\n",
        "    train_sparse[col] = tr\n",
        "    valid_sparse[col] = va\n",
        "    vocab_sizes[col] = vs\n",
        "    factor_uniques[col] = uniq\n",
        "\n",
        "# 归一化 dense\n",
        "scaler = StandardScaler()\n",
        "X_train_dense = scaler.fit_transform(train_df[DENSE_COLS].fillna(0).values.astype('float32'))\n",
        "X_valid_dense = scaler.transform(valid_df[DENSE_COLS].fillna(0).values.astype('float32'))\n",
        "\n",
        "y_train = train_df['label'].values.astype('float32')\n",
        "y_valid = valid_df['label'].values.astype('float32')\n",
        "\n",
        "len(y_train), len(y_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_deepfm(sparse_vocab_sizes: Dict[str, int], dense_dim: int, emb_dim: int = 16, dnn_units: List[int] = [128, 64]):\n",
        "    inputs = {}\n",
        "    embed_vecs = []\n",
        "    linear_terms = []\n",
        "\n",
        "    for feat, vocab_size in sparse_vocab_sizes.items():\n",
        "        inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name=feat)\n",
        "        inputs[feat] = inp\n",
        "\n",
        "        emb = tf.keras.layers.Embedding(vocab_size, emb_dim, name=f'emb_{feat}')(inp)\n",
        "        emb = tf.keras.layers.Flatten()(emb)\n",
        "        embed_vecs.append(emb)\n",
        "\n",
        "        lin = tf.keras.layers.Embedding(vocab_size, 1, name=f'lin_{feat}')(inp)\n",
        "        lin = tf.keras.layers.Flatten()(lin)\n",
        "        linear_terms.append(lin)\n",
        "\n",
        "    dense_inp = tf.keras.layers.Input(shape=(dense_dim,), dtype=tf.float32, name='dense')\n",
        "    inputs['dense'] = dense_inp\n",
        "\n",
        "    # FM 二阶交叉\n",
        "    stack = tf.keras.layers.Lambda(lambda x: tf.stack(x, axis=1))(embed_vecs)  # [B, F, D]\n",
        "    summed = tf.reduce_sum(stack, axis=1)  # [B, D]\n",
        "    sum_square = tf.square(summed)  # [B, D]\n",
        "    square_sum = tf.reduce_sum(tf.square(stack), axis=1)  # [B, D]\n",
        "    fm = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # [B, 1]\n",
        "\n",
        "    # Deep 部分\n",
        "    dnn_inp = tf.keras.layers.Concatenate()(embed_vecs + [dense_inp])\n",
        "    x = dnn_inp\n",
        "    for units in dnn_units:\n",
        "        x = tf.keras.layers.Dense(units, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    deep_logit = tf.keras.layers.Dense(1, activation=None)(x)\n",
        "\n",
        "    # Linear 部分\n",
        "    linear_sparse = tf.keras.layers.Add()(linear_terms) if len(linear_terms) > 1 else linear_terms[0]\n",
        "    linear_dense = tf.keras.layers.Dense(1, activation=None)(dense_inp)\n",
        "\n",
        "    logit = tf.keras.layers.Add()([linear_sparse, linear_dense, fm, deep_logit])\n",
        "    out = tf.keras.layers.Activation('sigmoid')(logit)\n",
        "    out = tf.keras.layers.Flatten()(out)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=out, name='DeepFM')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[tf.keras.metrics.AUC(name='auc')],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "deepfm = build_deepfm({k: vocab_sizes[k] for k in SPARSE_COLS}, dense_dim=X_train_dense.shape[1])\n",
        "deepfm.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_inputs = {**{k: v for k, v in train_sparse.items()}, 'dense': X_train_dense}\n",
        "valid_inputs = {**{k: v for k, v in valid_sparse.items()}, 'dense': X_valid_dense}\n",
        "\n",
        "deepfm.fit(\n",
        "    train_inputs,\n",
        "    y_train,\n",
        "    batch_size=2048,\n",
        "    epochs=2,\n",
        "    validation_data=(valid_inputs, y_valid),\n",
        "    verbose=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hit_ndcg_at_k(df: pd.DataFrame, preds: np.ndarray, k: int = 5) -> Dict[str, float]:\n",
        "    tmp = df[['user_id', 'label']].copy()\n",
        "    tmp['pred'] = preds\n",
        "    hit = 0\n",
        "    ndcg = 0.0\n",
        "    total = 0\n",
        "    for _, g in tmp.groupby('user_id'):\n",
        "        g = g.sort_values('pred', ascending=False).head(k)\n",
        "        if g['label'].max() > 0:\n",
        "            hit += 1\n",
        "            rank = int(g.reset_index(drop=True).index[g['label'] == 1][0])\n",
        "            ndcg += 1.0 / math.log2(rank + 2)\n",
        "        total += 1\n",
        "    return {f'hit_rate@{k}': hit / max(1, total), f'ndcg@{k}': ndcg / max(1, total), 'num_users': total}\n",
        "\n",
        "\n",
        "pred_valid_deepfm = deepfm.predict(valid_inputs, batch_size=4096, verbose=0)\n",
        "hit_ndcg_at_k(valid_df, pred_valid_deepfm, k=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) DIN（带序列注意力）\n",
        "\n",
        "DIN 的关键是 attention pooling：对同一个用户的历史序列，会根据“当前候选 item”不同而产生不同的兴趣聚合。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_HIST_LEN = 30\n",
        "\n",
        "# 为了让 DIN 能用到完整的历史序列，这里对全量 article_id 做编码（raw->enc）\n",
        "# vocab_size 大约 36 万，embedding 参数量仍可控（36w * 16 ≈ 576 万）\n",
        "all_article_ids = articles['article_id'].astype(int).unique()\n",
        "raw_to_item_enc = {int(v): int(i + 1) for i, v in enumerate(np.sort(all_article_ids))}\n",
        "item_vocab_size = int(len(raw_to_item_enc) + 1)\n",
        "\n",
        "def encode_item_seq(seq: List[int], max_len: int) -> np.ndarray:\n",
        "    enc = [raw_to_item_enc.get(int(x), 0) for x in seq][-max_len:]\n",
        "    out = np.zeros(max_len, dtype=np.int32)\n",
        "    if enc:\n",
        "        out[-len(enc):] = np.asarray(enc, dtype=np.int32)\n",
        "    return out\n",
        "\n",
        "\n",
        "user_hist_enc = {int(u): encode_item_seq(seq, MAX_HIST_LEN) for u, seq in user_hist.items()}\n",
        "\n",
        "def build_hist_matrix(df: pd.DataFrame) -> np.ndarray:\n",
        "    return np.vstack([user_hist_enc.get(int(u), np.zeros(MAX_HIST_LEN, dtype=np.int32)) for u in df['user_id'].values])\n",
        "\n",
        "\n",
        "train_hist_mat = build_hist_matrix(train_df)\n",
        "valid_hist_mat = build_hist_matrix(valid_df)\n",
        "\n",
        "# target item 编码\n",
        "train_target_item = np.asarray([raw_to_item_enc.get(int(x), 0) for x in train_df['article_id'].values], dtype=np.int32)\n",
        "valid_target_item = np.asarray([raw_to_item_enc.get(int(x), 0) for x in valid_df['article_id'].values], dtype=np.int32)\n",
        "\n",
        "print('item_vocab_size:', item_vocab_size)\n",
        "train_hist_mat.shape, train_target_item.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DinAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_units: List[int] = [80, 40], **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_units = hidden_units\n",
        "        self.mlp = [tf.keras.layers.Dense(u, activation='relu') for u in hidden_units]\n",
        "        self.out = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, keys, mask = inputs  # query: [B, D], keys: [B, L, D], mask: [B, L]\n",
        "        q = tf.expand_dims(query, axis=1)  # [B, 1, D]\n",
        "        q = tf.tile(q, [1, tf.shape(keys)[1], 1])  # [B, L, D]\n",
        "        x = tf.concat([q, keys, q - keys, q * keys], axis=-1)  # [B, L, 4D]\n",
        "        for dense in self.mlp:\n",
        "            x = dense(x)\n",
        "        scores = tf.squeeze(self.out(x), axis=-1)  # [B, L]\n",
        "        paddings = tf.ones_like(scores) * (-1e9)\n",
        "        scores = tf.where(mask > 0, scores, paddings)\n",
        "        weights = tf.nn.softmax(scores, axis=-1)  # [B, L]\n",
        "        weights = tf.expand_dims(weights, axis=-1)  # [B, L, 1]\n",
        "        return tf.reduce_sum(weights * keys, axis=1)  # [B, D]\n",
        "\n",
        "\n",
        "def build_din(item_vocab_size: int, user_vocab_size: int, dense_dim: int, emb_dim: int = 16, dnn_units: List[int] = [128, 64]):\n",
        "    user_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='user_id')\n",
        "    hist_inp = tf.keras.layers.Input(shape=(MAX_HIST_LEN,), dtype=tf.int32, name='hist_items')\n",
        "    item_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='target_item')\n",
        "    dense_inp = tf.keras.layers.Input(shape=(dense_dim,), dtype=tf.float32, name='dense')\n",
        "\n",
        "    user_emb = tf.keras.layers.Embedding(user_vocab_size, emb_dim)(user_inp)\n",
        "    user_emb = tf.keras.layers.Flatten()(user_emb)\n",
        "\n",
        "    item_emb_layer = tf.keras.layers.Embedding(item_vocab_size, emb_dim, mask_zero=True)\n",
        "    target_emb = tf.keras.layers.Flatten()(item_emb_layer(item_inp))  # [B, D]\n",
        "    hist_emb = item_emb_layer(hist_inp)  # [B, L, D]\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(hist_inp, 0), tf.int32)  # [B, L]\n",
        "    att_out = DinAttention()([target_emb, hist_emb, mask])\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()([user_emb, target_emb, att_out, dense_inp])\n",
        "    for units in dnn_units:\n",
        "        x = tf.keras.layers.Dense(units, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    out = tf.keras.layers.Flatten()(out)\n",
        "\n",
        "    model = tf.keras.Model(inputs={'user_id': user_inp, 'hist_items': hist_inp, 'target_item': item_inp, 'dense': dense_inp}, outputs=out, name='DIN')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[tf.keras.metrics.AUC(name='auc')],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# user_id 也做一个编码（用于 embedding）\n",
        "all_user_ids = pd.unique(pd.concat([train_df['user_id'], valid_df['user_id']]).astype(int))\n",
        "raw_to_user_enc = {int(v): int(i + 1) for i, v in enumerate(np.sort(all_user_ids))}\n",
        "user_vocab_size = int(len(raw_to_user_enc) + 1)\n",
        "train_user_enc = np.asarray([raw_to_user_enc.get(int(x), 0) for x in train_df['user_id'].values], dtype=np.int32)\n",
        "valid_user_enc = np.asarray([raw_to_user_enc.get(int(x), 0) for x in valid_df['user_id'].values], dtype=np.int32)\n",
        "\n",
        "din = build_din(item_vocab_size=item_vocab_size, user_vocab_size=user_vocab_size, dense_dim=X_train_dense.shape[1])\n",
        "din.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "din_train_inputs = {\n",
        "    'user_id': train_user_enc,\n",
        "    'hist_items': train_hist_mat,\n",
        "    'target_item': train_target_item,\n",
        "    'dense': X_train_dense,\n",
        "}\n",
        "din_valid_inputs = {\n",
        "    'user_id': valid_user_enc,\n",
        "    'hist_items': valid_hist_mat,\n",
        "    'target_item': valid_target_item,\n",
        "    'dense': X_valid_dense,\n",
        "}\n",
        "\n",
        "din.fit(\n",
        "    din_train_inputs,\n",
        "    y_train,\n",
        "    batch_size=2048,\n",
        "    epochs=2,\n",
        "    validation_data=(din_valid_inputs, y_valid),\n",
        "    verbose=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_valid_din = din.predict(din_valid_inputs, batch_size=4096, verbose=0)\n",
        "hit_ndcg_at_k(valid_df, pred_valid_din, k=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) 保存产物\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "deepfm.save(ARTIFACTS_DIR / 'deepfm.keras')\n",
        "din.save(ARTIFACTS_DIR / 'din.keras')\n",
        "\n",
        "import pickle\n",
        "with open(ARTIFACTS_DIR / 'scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "with open(ARTIFACTS_DIR / 'deepfm_factorizers.pkl', 'wb') as f:\n",
        "    pickle.dump({'sparse_cols': SPARSE_COLS, 'dense_cols': DENSE_COLS, 'uniques': factor_uniques}, f)\n",
        "with open(ARTIFACTS_DIR / 'din_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump({'raw_to_item_enc': raw_to_item_enc, 'raw_to_user_enc': raw_to_user_enc}, f)\n",
        "\n",
        "print('saved to:', ARTIFACTS_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
