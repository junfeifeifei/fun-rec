{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. 深度排序（DeepFM / DIN）\n",
        "\n",
        "这一节把基础版的 `LGBMRanker` 排序升级为深度学习排序（工业常见）：\n",
        "\n",
        "- **DeepFM**：FM 二阶交叉 + DNN 高阶交叉\n",
        "- **DIN**：对用户历史序列做注意力（兴趣与候选 item 相关）\n",
        "\n",
        "并补齐更贴近生产的流程：\n",
        "\n",
        "1) 召回候选集（可融合 ItemCF / 双塔 / 热门）\n",
        "\n",
        "2) 特征工程（复用基础版逻辑）\n",
        "\n",
        "3) 深度排序训练与评估（HitRate@K / NDCG@K）\n",
        "\n",
        "## 面试要点\n",
        "\n",
        "- 为什么要两阶段（Recall→Rank）：效率 vs 效果\n",
        "- DeepFM vs DIN：特征交叉 vs 序列兴趣\n",
        "- 训练样本怎么构造：候选集 + 正负样本\n",
        "- 评估：按 user 分组的 TopK 指标（避免把样本当独立）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ac31014a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('/Users/wangjunfei/Desktop/fun-rec/data/dataset/news_recommendation'),\n",
              " PosixPath('/Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system'),\n",
              " PosixPath('/Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system/artifacts/ranking/deep_models'))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "from tqdm import tqdm\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    cur = start\n",
        "    for _ in range(10):\n",
        "        if (cur / 'pyproject.toml').exists() or (cur / '.git').exists():\n",
        "            return cur\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "dotenv_path = find_dotenv(usecwd=True)\n",
        "if dotenv_path:\n",
        "    load_dotenv(dotenv_path)\n",
        "os.environ.setdefault('FUNREC_RAW_DATA_PATH', str(REPO_ROOT / 'data'))\n",
        "os.environ.setdefault('FUNREC_PROCESSED_DATA_PATH', str(REPO_ROOT / 'tmp'))\n",
        "\n",
        "RAW_DATA_PATH = Path(os.getenv('FUNREC_RAW_DATA_PATH'))\n",
        "PROCESSED_DATA_PATH = Path(os.getenv('FUNREC_PROCESSED_DATA_PATH'))\n",
        "\n",
        "DATA_PATH = RAW_DATA_PATH / 'dataset' / 'news_recommendation'\n",
        "if not DATA_PATH.exists():\n",
        "    DATA_PATH = RAW_DATA_PATH / 'news_recommendation'\n",
        "\n",
        "PROJECT_PATH = PROCESSED_DATA_PATH / 'projects' / 'news_recommendation_system'\n",
        "ARTIFACTS_DIR = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models'\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_PATH, PROJECT_PATH, ARTIFACTS_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "64af3825",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(912623, 200000, 200000)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==================== 读取基础数据与离线切分 ====================\n",
        "train_hist = pd.read_pickle(PROJECT_PATH / 'train_hist.pkl')\n",
        "valid_last = pd.read_pickle(PROJECT_PATH / 'valid_last.pkl')\n",
        "articles = pd.read_csv(DATA_PATH / 'articles.csv')\n",
        "\n",
        "# 用户历史序列（用于 DIN）\n",
        "train_hist_sorted = train_hist.sort_values(['user_id', 'click_timestamp'])\n",
        "user_hist: Dict[int, List[int]] = train_hist_sorted.groupby('user_id')['click_article_id'].apply(list).to_dict()\n",
        "\n",
        "len(train_hist), len(valid_last), len(user_hist)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c98c97",
      "metadata": {},
      "source": [
        "## 1) 召回候选集（可融合多路 recall）\n",
        "\n",
        "你可以把候选集理解为“精排的检索空间”。本 notebook 支持：\n",
        "\n",
        "- 基础版候选：`recall_candidates.pkl`（来自 4.recall）\n",
        "- 双塔候选：`recall_candidates_two_tower.pkl`（来自 7.two_tower_recall）\n",
        "\n",
        "默认用 **RRF（倒数排名融合）** 合并。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cd94d92f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(   user_id  article_id  recall_score  recall_rank\n",
              " 2        0       16346      0.031250         61.0\n",
              " 3        0       18187      0.142857         11.0\n",
              " 4        0       22019      0.033333         57.0\n",
              " 5        0       22024      0.200000          7.0\n",
              " 6        0       22529      0.020833         93.0,\n",
              " 20000,\n",
              " 2000000)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "USE_BASELINE_RECALL = True\n",
        "USE_TWO_TOWER_RECALL = True\n",
        "MAX_CANDIDATES = 100\n",
        "\n",
        "\n",
        "def load_recall_df(path: Path, name: str) -> pd.DataFrame:\n",
        "    if not path.exists():\n",
        "        print(f'[skip] {name}: {path} not found')\n",
        "        return pd.DataFrame(columns=['user_id', 'article_id', 'recall_score', 'source'])\n",
        "    df = pd.read_pickle(path)\n",
        "    df = df[['user_id', 'article_id', 'recall_score']].copy()\n",
        "    df['source'] = name\n",
        "    return df\n",
        "\n",
        "\n",
        "def rrf_merge(dfs: List[pd.DataFrame], weights: Dict[str, float], topk: int = 100) -> pd.DataFrame:\n",
        "    # Reciprocal Rank Fusion: sum(w / (rank+1))\n",
        "    merged = []\n",
        "    for df in dfs:\n",
        "        if df.empty:\n",
        "            continue\n",
        "        name = df['source'].iloc[0]\n",
        "        w = float(weights.get(name, 1.0))\n",
        "        tmp = df.copy()\n",
        "        tmp['rank'] = tmp.groupby('user_id')['recall_score'].rank(ascending=False, method='first')\n",
        "        tmp['rrf'] = w / (tmp['rank'] + 1.0)\n",
        "        merged.append(tmp[['user_id', 'article_id', 'rrf']])\n",
        "\n",
        "    if not merged:\n",
        "        raise ValueError('No recall sources available.')\n",
        "\n",
        "    merged = pd.concat(merged, axis=0, ignore_index=True)\n",
        "    merged = merged.groupby(['user_id', 'article_id'], as_index=False)['rrf'].sum()\n",
        "    merged = merged.rename(columns={'rrf': 'recall_score'})\n",
        "    merged['recall_rank'] = merged.groupby('user_id')['recall_score'].rank(ascending=False, method='first')\n",
        "    merged = merged[merged['recall_rank'] <= topk]\n",
        "    return merged\n",
        "\n",
        "\n",
        "sources = []\n",
        "weights = {}\n",
        "\n",
        "if USE_BASELINE_RECALL:\n",
        "    df_base = load_recall_df(PROJECT_PATH / 'recall_candidates.pkl', 'baseline')\n",
        "    if not df_base.empty:\n",
        "        sources.append(df_base)\n",
        "        weights['baseline'] = 1.0\n",
        "\n",
        "if USE_TWO_TOWER_RECALL:\n",
        "    df_tt = load_recall_df(PROJECT_PATH / 'recall_candidates_two_tower.pkl', 'two_tower')\n",
        "    if not df_tt.empty:\n",
        "        sources.append(df_tt)\n",
        "        weights['two_tower'] = 1.0\n",
        "\n",
        "recall_df = rrf_merge(sources, weights=weights, topk=MAX_CANDIDATES)\n",
        "recall_df.head(), recall_df['user_id'].nunique(), len(recall_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3067c949",
      "metadata": {},
      "source": [
        "## 2) 特征工程（与基础版一致）\n",
        "\n",
        "深度排序与树模型一样，需要构造用户/物品/交互/召回特征。\n",
        "\n",
        "为了保持可复现与可对比，这里尽量复用基础版 `5.feature_engineering.ipynb` 的逻辑。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3569fb01",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>article_id</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>recall_rank</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>16346</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>18187</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>22019</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>22024</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>22529</td>\n",
              "      <td>0.020833</td>\n",
              "      <td>93.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  article_id  recall_score  recall_rank  label\n",
              "0        0       16346      0.031250         61.0      0\n",
              "1        0       18187      0.142857         11.0      0\n",
              "2        0       22019      0.033333         57.0      0\n",
              "3        0       22024      0.200000          7.0      0\n",
              "4        0       22529      0.020833         93.0      0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 用户特征\n",
        "user_click_count = train_hist.groupby('user_id').size().rename('user_click_count')\n",
        "user_unique_items = train_hist.groupby('user_id')['click_article_id'].nunique().rename('user_unique_items')\n",
        "user_last_click_ts = train_hist.groupby('user_id')['click_timestamp'].max().rename('user_last_click_ts')\n",
        "\n",
        "click_with_cat = train_hist.merge(articles, left_on='click_article_id', right_on='article_id', how='left')\n",
        "user_top_category = click_with_cat.groupby('user_id')['category_id'].agg(lambda x: x.value_counts().idxmax()).rename('user_top_category')\n",
        "\n",
        "user_features = pd.concat([user_click_count, user_unique_items, user_last_click_ts, user_top_category], axis=1).reset_index()\n",
        "\n",
        "# 物品特征\n",
        "item_click_count = train_hist.groupby('click_article_id').size().rename('item_click_count')\n",
        "item_last_click_ts = train_hist.groupby('click_article_id')['click_timestamp'].max().rename('item_last_click_ts')\n",
        "\n",
        "item_features = (\n",
        "    articles.merge(item_click_count, left_on='article_id', right_index=True, how='left')\n",
        "    .merge(item_last_click_ts, left_on='article_id', right_index=True, how='left')\n",
        ")\n",
        "item_features['item_click_count'] = item_features['item_click_count'].fillna(0)\n",
        "item_features['item_last_click_ts'] = item_features['item_last_click_ts'].fillna(0)\n",
        "\n",
        "# 用户最后一次点击（基于 train_hist）\n",
        "user_last_click = (\n",
        "    train_hist.sort_values(['user_id', 'click_timestamp'])\n",
        "    .groupby('user_id')\n",
        "    .tail(1)[['user_id', 'click_article_id', 'click_timestamp']]\n",
        "    .rename(columns={'click_article_id': 'last_click_article_id', 'click_timestamp': 'last_click_timestamp'})\n",
        ")\n",
        "\n",
        "# 合并成候选样本表\n",
        "candidates = (\n",
        "    recall_df.merge(user_features, on='user_id', how='left')\n",
        "    .merge(user_last_click, on='user_id', how='left')\n",
        "    .merge(item_features, left_on='article_id', right_on='article_id', how='left')\n",
        ")\n",
        "\n",
        "candidates['is_same_category'] = (candidates['category_id'] == candidates['user_top_category']).astype(int)\n",
        "candidates['item_age_hours'] = (candidates['last_click_timestamp'] - candidates['created_at_ts']) / 3600_000\n",
        "candidates['time_gap_hours'] = (candidates['last_click_timestamp'] - candidates['item_last_click_ts']) / 3600_000\n",
        "candidates[['item_age_hours', 'time_gap_hours']] = candidates[['item_age_hours', 'time_gap_hours']].fillna(0)\n",
        "\n",
        "# label：valid_last 里每个 user 的目标 item\n",
        "target = valid_last[['user_id', 'click_article_id']].rename(columns={'click_article_id': 'target_article_id'})\n",
        "candidates = candidates.merge(target, on='user_id', how='left')\n",
        "candidates['label'] = (candidates['article_id'] == candidates['target_article_id']).astype(int)\n",
        "\n",
        "candidates[['user_id', 'article_id', 'recall_score', 'recall_rank', 'label']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6498659b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emb_sim_last</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2.000000e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.432449e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.977172e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-5.161761e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.268502e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.505311e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.093505e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       emb_sim_last\n",
              "count  2.000000e+06\n",
              "mean   3.432449e-01\n",
              "std    2.977172e-01\n",
              "min   -5.161761e-01\n",
              "25%    1.268502e-01\n",
              "50%    2.505311e-01\n",
              "75%    5.093505e-01\n",
              "max    1.000000e+00"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 可选：内容向量相似度特征（emb_sim_last）\n",
        "USE_CONTENT_EMB_SIM = True\n",
        "\n",
        "if USE_CONTENT_EMB_SIM:\n",
        "    article_emb = pd.read_csv(DATA_PATH / 'articles_emb.csv')\n",
        "    emb_cols = [c for c in article_emb.columns if c.startswith('emb_')]\n",
        "    emb_matrix = article_emb[emb_cols].values.astype('float32')\n",
        "    emb_matrix /= np.linalg.norm(emb_matrix, axis=1, keepdims=True) + 1e-12\n",
        "    article_ids = article_emb['article_id'].values\n",
        "    id2idx = {int(a): int(i) for i, a in enumerate(article_ids)}\n",
        "\n",
        "    cand_idx = candidates['article_id'].map(id2idx)\n",
        "    last_idx = candidates['last_click_article_id'].map(id2idx)\n",
        "    mask = cand_idx.notna() & last_idx.notna()\n",
        "\n",
        "    sim = np.zeros(len(candidates), dtype='float32')\n",
        "    sim[mask.values] = (\n",
        "        emb_matrix[cand_idx[mask].astype(int)]\n",
        "        * emb_matrix[last_idx[mask].astype(int)]\n",
        "    ).sum(axis=1)\n",
        "    candidates['emb_sim_last'] = sim\n",
        "else:\n",
        "    candidates['emb_sim_last'] = 0.0\n",
        "\n",
        "candidates[['emb_sim_last']].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b625cf82",
      "metadata": {},
      "source": [
        "## 3) 训练/验证划分（按 user 划分）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4ab1b192",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(label\n",
              " 0        298809\n",
              " 1          1191\n",
              " Name: count, dtype: int64,\n",
              " label\n",
              " 0        398455\n",
              " 1          1545\n",
              " Name: count, dtype: int64,\n",
              " 300000,\n",
              " 400000)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "users = candidates['user_id'].unique()\n",
        "rng.shuffle(users)\n",
        "split = int(len(users) * 0.8)\n",
        "train_users = set(users[:split])\n",
        "\n",
        "train_df = candidates[candidates['user_id'].isin(train_users)].copy()\n",
        "valid_df = candidates[~candidates['user_id'].isin(train_users)].copy()\n",
        "\n",
        "# 可选：限制训练规模（避免 notebook 太慢）\n",
        "MAX_TRAIN_ROWS = 300000 if USE_CONTENT_EMB_SIM else 500000\n",
        "if len(train_df) > MAX_TRAIN_ROWS:\n",
        "    train_df = train_df.sample(MAX_TRAIN_ROWS, random_state=42)\n",
        "\n",
        "train_df[['label']].value_counts(), valid_df[['label']].value_counts(), len(train_df), len(valid_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34633692",
      "metadata": {},
      "source": [
        "## 4) DeepFM\n",
        "\n",
        "输入：\n",
        "\n",
        "- sparse：user_id, article_id, category_id, user_top_category, last_click_article_id\n",
        "- dense：recall_score, recall_rank, user/item 统计特征, 时间特征, emb_sim_last\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "08501dff",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300000, 400000)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SPARSE_COLS = ['user_id', 'article_id', 'category_id', 'user_top_category', 'last_click_article_id']\n",
        "DENSE_COLS = [\n",
        "    'recall_score',\n",
        "    'recall_rank',\n",
        "    'user_click_count',\n",
        "    'user_unique_items',\n",
        "    'item_click_count',\n",
        "    'words_count',\n",
        "    'item_age_hours',\n",
        "    'time_gap_hours',\n",
        "    'emb_sim_last',\n",
        "    'is_same_category',\n",
        "]\n",
        "\n",
        "\n",
        "def encode_categorical(train_s: pd.Series, valid_s: pd.Series) -> Tuple[np.ndarray, np.ndarray, int, List[str]]:\n",
        "    all_s = pd.concat([train_s, valid_s], axis=0)\n",
        "    codes, uniques = pd.factorize(all_s.astype(str), sort=True)\n",
        "    codes = codes.astype(np.int32) + 1  # 0 reserved\n",
        "    train_codes = codes[: len(train_s)]\n",
        "    valid_codes = codes[len(train_s) :]\n",
        "    vocab_size = int(len(uniques) + 1)\n",
        "    return train_codes, valid_codes, vocab_size, list(uniques)\n",
        "\n",
        "\n",
        "# 编码 sparse\n",
        "train_sparse = {}\n",
        "valid_sparse = {}\n",
        "vocab_sizes = {}\n",
        "factor_uniques = {}\n",
        "for col in SPARSE_COLS:\n",
        "    tr, va, vs, uniq = encode_categorical(train_df[col].fillna(0), valid_df[col].fillna(0))\n",
        "    train_sparse[col] = tr\n",
        "    valid_sparse[col] = va\n",
        "    vocab_sizes[col] = vs\n",
        "    factor_uniques[col] = uniq\n",
        "\n",
        "# 归一化 dense\n",
        "scaler = StandardScaler()\n",
        "X_train_dense = scaler.fit_transform(train_df[DENSE_COLS].fillna(0).values.astype('float32'))\n",
        "X_valid_dense = scaler.transform(valid_df[DENSE_COLS].fillna(0).values.astype('float32'))\n",
        "\n",
        "y_train = train_df['label'].values.astype('float32')\n",
        "y_valid = valid_df['label'].values.astype('float32')\n",
        "\n",
        "len(y_train), len(y_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "67d5448c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"DeepFM\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " user_id (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " article_id (InputLayer)     [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " category_id (InputLayer)    [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " user_top_category (InputLa  [(None,)]                    0         []                            \n",
            " yer)                                                                                             \n",
            "                                                                                                  \n",
            " last_click_article_id (Inp  [(None,)]                    0         []                            \n",
            " utLayer)                                                                                         \n",
            "                                                                                                  \n",
            " emb_user_id (Embedding)     (None, 16)                   320016    ['user_id[0][0]']             \n",
            "                                                                                                  \n",
            " emb_article_id (Embedding)  (None, 16)                   573104    ['article_id[0][0]']          \n",
            "                                                                                                  \n",
            " emb_category_id (Embedding  (None, 16)                   4544      ['category_id[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " emb_user_top_category (Emb  (None, 16)                   2416      ['user_top_category[0][0]']   \n",
            " edding)                                                                                          \n",
            "                                                                                                  \n",
            " emb_last_click_article_id   (None, 16)                   56176     ['last_click_article_id[0][0]'\n",
            " (Embedding)                                                        ]                             \n",
            "                                                                                                  \n",
            " dense (InputLayer)          [(None, 10)]                 0         []                            \n",
            "                                                                                                  \n",
            " flatten_11 (Flatten)        (None, 16)                   0         ['emb_user_id[0][0]']         \n",
            "                                                                                                  \n",
            " flatten_13 (Flatten)        (None, 16)                   0         ['emb_article_id[0][0]']      \n",
            "                                                                                                  \n",
            " flatten_15 (Flatten)        (None, 16)                   0         ['emb_category_id[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_17 (Flatten)        (None, 16)                   0         ['emb_user_top_category[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " flatten_19 (Flatten)        (None, 16)                   0         ['emb_last_click_article_id[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 90)                   0         ['flatten_11[0][0]',          \n",
            " )                                                                   'flatten_13[0][0]',          \n",
            "                                                                     'flatten_15[0][0]',          \n",
            "                                                                     'flatten_17[0][0]',          \n",
            "                                                                     'flatten_19[0][0]',          \n",
            "                                                                     'dense[0][0]']               \n",
            "                                                                                                  \n",
            " deep_dense_0 (Dense)        (None, 128)                  11648     ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " fm_stack (Lambda)           (None, 5, 16)                0         ['flatten_11[0][0]',          \n",
            "                                                                     'flatten_13[0][0]',          \n",
            "                                                                     'flatten_15[0][0]',          \n",
            "                                                                     'flatten_17[0][0]',          \n",
            "                                                                     'flatten_19[0][0]']          \n",
            "                                                                                                  \n",
            " deep_dropout_0 (Dropout)    (None, 128)                  0         ['deep_dense_0[0][0]']        \n",
            "                                                                                                  \n",
            " lin_user_id (Embedding)     (None, 1)                    20001     ['user_id[0][0]']             \n",
            "                                                                                                  \n",
            " lin_article_id (Embedding)  (None, 1)                    35819     ['article_id[0][0]']          \n",
            "                                                                                                  \n",
            " lin_category_id (Embedding  (None, 1)                    284       ['category_id[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " lin_user_top_category (Emb  (None, 1)                    151       ['user_top_category[0][0]']   \n",
            " edding)                                                                                          \n",
            "                                                                                                  \n",
            " lin_last_click_article_id   (None, 1)                    3511      ['last_click_article_id[0][0]'\n",
            " (Embedding)                                                        ]                             \n",
            "                                                                                                  \n",
            " fm_sum (Lambda)             (None, 16)                   0         ['fm_stack[0][0]']            \n",
            "                                                                                                  \n",
            " deep_dense_1 (Dense)        (None, 64)                   8256      ['deep_dropout_0[0][0]']      \n",
            "                                                                                                  \n",
            " flatten_12 (Flatten)        (None, 1)                    0         ['lin_user_id[0][0]']         \n",
            "                                                                                                  \n",
            " flatten_14 (Flatten)        (None, 1)                    0         ['lin_article_id[0][0]']      \n",
            "                                                                                                  \n",
            " flatten_16 (Flatten)        (None, 1)                    0         ['lin_category_id[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_18 (Flatten)        (None, 1)                    0         ['lin_user_top_category[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " flatten_20 (Flatten)        (None, 1)                    0         ['lin_last_click_article_id[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " fm_sum_square (Lambda)      (None, 16)                   0         ['fm_sum[0][0]']              \n",
            "                                                                                                  \n",
            " fm_square_sum (Lambda)      (None, 16)                   0         ['fm_stack[0][0]']            \n",
            "                                                                                                  \n",
            " deep_dropout_1 (Dropout)    (None, 64)                   0         ['deep_dense_1[0][0]']        \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 1)                    0         ['flatten_12[0][0]',          \n",
            "                                                                     'flatten_14[0][0]',          \n",
            "                                                                     'flatten_16[0][0]',          \n",
            "                                                                     'flatten_18[0][0]',          \n",
            "                                                                     'flatten_20[0][0]']          \n",
            "                                                                                                  \n",
            " linear_dense (Dense)        (None, 1)                    11        ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " fm_logit (Lambda)           (None, 1)                    0         ['fm_sum_square[0][0]',       \n",
            "                                                                     'fm_square_sum[0][0]']       \n",
            "                                                                                                  \n",
            " deep_logit (Dense)          (None, 1)                    65        ['deep_dropout_1[0][0]']      \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 1)                    0         ['add_2[0][0]',               \n",
            "                                                                     'linear_dense[0][0]',        \n",
            "                                                                     'fm_logit[0][0]',            \n",
            "                                                                     'deep_logit[0][0]']          \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 1)                    0         ['add_3[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_21 (Flatten)        (None, 1)                    0         ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1036002 (3.95 MB)\n",
            "Trainable params: 1036002 (3.95 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_deepfm(sparse_vocab_sizes: Dict[str, int], dense_dim: int, emb_dim: int = 16, dnn_units: List[int] = [128, 64]):\n",
        "    inputs = {}\n",
        "    embed_vecs = []\n",
        "    linear_terms = []\n",
        "\n",
        "    for feat, vocab_size in sparse_vocab_sizes.items():\n",
        "        inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name=feat)\n",
        "        inputs[feat] = inp\n",
        "\n",
        "        emb = tf.keras.layers.Embedding(vocab_size, emb_dim, name=f'emb_{feat}')(inp)\n",
        "        emb = tf.keras.layers.Flatten()(emb)\n",
        "        embed_vecs.append(emb)\n",
        "\n",
        "        lin = tf.keras.layers.Embedding(vocab_size, 1, name=f'lin_{feat}')(inp)\n",
        "        lin = tf.keras.layers.Flatten()(lin)\n",
        "        linear_terms.append(lin)\n",
        "\n",
        "    dense_inp = tf.keras.layers.Input(shape=(dense_dim,), dtype=tf.float32, name='dense')\n",
        "    inputs['dense'] = dense_inp\n",
        "\n",
        "    # FM 二阶交叉\n",
        "    # 注意：在新版本 Keras 中，不能直接对 KerasTensor 调用 tf.reduce_sum/tf.square；用 Lambda 包一下更稳。\n",
        "    stack = tf.keras.layers.Lambda(lambda x: tf.stack(x, axis=1), name='fm_stack')(embed_vecs)  # [B, F, D]\n",
        "    summed = tf.keras.layers.Lambda(lambda t: tf.reduce_sum(t, axis=1), name='fm_sum')(stack)  # [B, D]\n",
        "    sum_square = tf.keras.layers.Lambda(lambda t: tf.square(t), name='fm_sum_square')(summed)  # [B, D]\n",
        "    square_sum = tf.keras.layers.Lambda(lambda t: tf.reduce_sum(tf.square(t), axis=1), name='fm_square_sum')(stack)  # [B, D]\n",
        "    fm = tf.keras.layers.Lambda(lambda z: 0.5 * tf.reduce_sum(z[0] - z[1], axis=1, keepdims=True), name='fm_logit')([sum_square, square_sum])  # [B, 1]\n",
        "\n",
        "    # Deep 部分\n",
        "    dnn_inp = tf.keras.layers.Concatenate()(embed_vecs + [dense_inp])\n",
        "    x = dnn_inp\n",
        "    for i, units in enumerate(dnn_units):\n",
        "        x = tf.keras.layers.Dense(units, activation='relu', name=f'deep_dense_{i}')(x)\n",
        "        x = tf.keras.layers.Dropout(0.2, name=f'deep_dropout_{i}')(x)\n",
        "    deep_logit = tf.keras.layers.Dense(1, activation=None, name='deep_logit')(x)\n",
        "\n",
        "    # Linear 部分\n",
        "    linear_sparse = tf.keras.layers.Add()(linear_terms) if len(linear_terms) > 1 else linear_terms[0]\n",
        "    # 这里必须显式命名，避免和 Input(name='dense') 冲突导致 ValueError\n",
        "    linear_dense = tf.keras.layers.Dense(1, activation=None, name='linear_dense')(dense_inp)\n",
        "\n",
        "    logit = tf.keras.layers.Add()([linear_sparse, linear_dense, fm, deep_logit])\n",
        "    out = tf.keras.layers.Activation('sigmoid')(logit)\n",
        "    out = tf.keras.layers.Flatten()(out)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=out, name='DeepFM')\n",
        "    try:\n",
        "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
        "    except Exception:\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "\n",
        "deepfm = build_deepfm({k: vocab_sizes[k] for k in SPARSE_COLS}, dense_dim=X_train_dense.shape[1])\n",
        "deepfm.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "553c4ba1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "147/147 [==============================] - 2s 7ms/step - loss: 0.0706 - auc: 0.6535 - val_loss: 0.0198 - val_auc: 0.8626\n",
            "Epoch 2/2\n",
            "147/147 [==============================] - 1s 7ms/step - loss: 0.0193 - auc: 0.8857 - val_loss: 0.0188 - val_auc: 0.8769\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x30b1470d0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_inputs = {**{k: v for k, v in train_sparse.items()}, 'dense': X_train_dense}\n",
        "valid_inputs = {**{k: v for k, v in valid_sparse.items()}, 'dense': X_valid_dense}\n",
        "\n",
        "deepfm.fit(\n",
        "    train_inputs,\n",
        "    y_train,\n",
        "    batch_size=2048,\n",
        "    epochs=2,\n",
        "    validation_data=(valid_inputs, y_valid),\n",
        "    verbose=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "65d7e96b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hit_rate@5': 0.2175, 'ndcg@5': 0.1475663304779215, 'num_users': 4000}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def hit_ndcg_at_k(df: pd.DataFrame, preds: np.ndarray, k: int = 5) -> Dict[str, float]:\n",
        "    tmp = df[['user_id', 'label']].copy()\n",
        "    tmp['pred'] = preds\n",
        "    hit = 0\n",
        "    ndcg = 0.0\n",
        "    total = 0\n",
        "    for _, g in tmp.groupby('user_id'):\n",
        "        g = g.sort_values('pred', ascending=False).head(k)\n",
        "        if g['label'].max() > 0:\n",
        "            hit += 1\n",
        "            rank = int(g.reset_index(drop=True).index[g['label'] == 1][0])\n",
        "            ndcg += 1.0 / math.log2(rank + 2)\n",
        "        total += 1\n",
        "    return {f'hit_rate@{k}': hit / max(1, total), f'ndcg@{k}': ndcg / max(1, total), 'num_users': total}\n",
        "\n",
        "\n",
        "pred_valid_deepfm = deepfm.predict(valid_inputs, batch_size=4096, verbose=0)\n",
        "hit_ndcg_at_k(valid_df, pred_valid_deepfm, k=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb9e306",
      "metadata": {},
      "source": [
        "## 5) DIN（带序列注意力）\n",
        "\n",
        "DIN 的关键是 attention pooling：对同一个用户的历史序列，会根据“当前候选 item”不同而产生不同的兴趣聚合。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2d82035b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "item_vocab_size: 364048\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((300000, 30), (300000,))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MAX_HIST_LEN = 30\n",
        "\n",
        "# 为了让 DIN 能用到完整的历史序列，这里对全量 article_id 做编码（raw->enc）\n",
        "# vocab_size 大约 36 万，embedding 参数量仍可控（36w * 16 ≈ 576 万）\n",
        "all_article_ids = articles['article_id'].astype(int).unique()\n",
        "raw_to_item_enc = {int(v): int(i + 1) for i, v in enumerate(np.sort(all_article_ids))}\n",
        "item_vocab_size = int(len(raw_to_item_enc) + 1)\n",
        "\n",
        "def encode_item_seq(seq: List[int], max_len: int) -> np.ndarray:\n",
        "    enc = [raw_to_item_enc.get(int(x), 0) for x in seq][-max_len:]\n",
        "    out = np.zeros(max_len, dtype=np.int32)\n",
        "    if enc:\n",
        "        out[-len(enc):] = np.asarray(enc, dtype=np.int32)\n",
        "    return out\n",
        "\n",
        "\n",
        "user_hist_enc = {int(u): encode_item_seq(seq, MAX_HIST_LEN) for u, seq in user_hist.items()}\n",
        "\n",
        "def build_hist_matrix(df: pd.DataFrame) -> np.ndarray:\n",
        "    return np.vstack([user_hist_enc.get(int(u), np.zeros(MAX_HIST_LEN, dtype=np.int32)) for u in df['user_id'].values])\n",
        "\n",
        "\n",
        "train_hist_mat = build_hist_matrix(train_df)\n",
        "valid_hist_mat = build_hist_matrix(valid_df)\n",
        "\n",
        "# target item 编码\n",
        "train_target_item = np.asarray([raw_to_item_enc.get(int(x), 0) for x in train_df['article_id'].values], dtype=np.int32)\n",
        "valid_target_item = np.asarray([raw_to_item_enc.get(int(x), 0) for x in valid_df['article_id'].values], dtype=np.int32)\n",
        "\n",
        "print('item_vocab_size:', item_vocab_size)\n",
        "train_hist_mat.shape, train_target_item.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7a7a45d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"DIN\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " target_item (InputLayer)    [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " hist_items (InputLayer)     [(None, 30)]                 0         []                            \n",
            "                                                                                                  \n",
            " user_id (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     multiple                     5824768   ['target_item[0][0]',         \n",
            "                                                                     'hist_items[0][0]']          \n",
            "                                                                                                  \n",
            " tf.math.not_equal (TFOpLam  (None, 30)                   0         ['hist_items[0][0]']          \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 16)                   320016    ['user_id[0][0]']             \n",
            "                                                                                                  \n",
            " flatten_23 (Flatten)        (None, 16)                   0         ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " tf.cast (TFOpLambda)        (None, 30)                   0         ['tf.math.not_equal[0][0]']   \n",
            "                                                                                                  \n",
            " flatten_22 (Flatten)        (None, 16)                   0         ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " din_attention (DinAttentio  (None, 16)                   8481      ['flatten_23[0][0]',          \n",
            " n)                                                                  'embedding_1[1][0]',         \n",
            "                                                                     'tf.cast[0][0]']             \n",
            "                                                                                                  \n",
            " dense (InputLayer)          [(None, 10)]                 0         []                            \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 58)                   0         ['flatten_22[0][0]',          \n",
            " )                                                                   'flatten_23[0][0]',          \n",
            "                                                                     'din_attention[0][0]',       \n",
            "                                                                     'dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 128)                  7552      ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 128)                  0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 64)                   8256      ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 64)                   0         ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 1)                    65        ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_24 (Flatten)        (None, 1)                    0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6169138 (23.53 MB)\n",
            "Trainable params: 6169138 (23.53 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "class DinAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_units: List[int] = [80, 40], **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_units = hidden_units\n",
        "        self.mlp = [tf.keras.layers.Dense(u, activation='relu') for u in hidden_units]\n",
        "        self.out = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, keys, mask = inputs  # query: [B, D], keys: [B, L, D], mask: [B, L]\n",
        "        q = tf.expand_dims(query, axis=1)  # [B, 1, D]\n",
        "        q = tf.tile(q, [1, tf.shape(keys)[1], 1])  # [B, L, D]\n",
        "        x = tf.concat([q, keys, q - keys, q * keys], axis=-1)  # [B, L, 4D]\n",
        "        for dense in self.mlp:\n",
        "            x = dense(x)\n",
        "        scores = tf.squeeze(self.out(x), axis=-1)  # [B, L]\n",
        "        paddings = tf.ones_like(scores) * (-1e9)\n",
        "        scores = tf.where(mask > 0, scores, paddings)\n",
        "        weights = tf.nn.softmax(scores, axis=-1)  # [B, L]\n",
        "        weights = tf.expand_dims(weights, axis=-1)  # [B, L, 1]\n",
        "        return tf.reduce_sum(weights * keys, axis=1)  # [B, D]\n",
        "\n",
        "\n",
        "def build_din(item_vocab_size: int, user_vocab_size: int, dense_dim: int, emb_dim: int = 16, dnn_units: List[int] = [128, 64]):\n",
        "    user_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='user_id')\n",
        "    hist_inp = tf.keras.layers.Input(shape=(MAX_HIST_LEN,), dtype=tf.int32, name='hist_items')\n",
        "    item_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='target_item')\n",
        "    dense_inp = tf.keras.layers.Input(shape=(dense_dim,), dtype=tf.float32, name='dense')\n",
        "\n",
        "    user_emb = tf.keras.layers.Embedding(user_vocab_size, emb_dim)(user_inp)\n",
        "    user_emb = tf.keras.layers.Flatten()(user_emb)\n",
        "\n",
        "    item_emb_layer = tf.keras.layers.Embedding(item_vocab_size, emb_dim, mask_zero=True)\n",
        "    target_emb = tf.keras.layers.Flatten()(item_emb_layer(item_inp))  # [B, D]\n",
        "    hist_emb = item_emb_layer(hist_inp)  # [B, L, D]\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(hist_inp, 0), tf.int32)  # [B, L]\n",
        "    att_out = DinAttention()([target_emb, hist_emb, mask])\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()([user_emb, target_emb, att_out, dense_inp])\n",
        "    for units in dnn_units:\n",
        "        x = tf.keras.layers.Dense(units, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    out = tf.keras.layers.Flatten()(out)\n",
        "\n",
        "    model = tf.keras.Model(inputs={'user_id': user_inp, 'hist_items': hist_inp, 'target_item': item_inp, 'dense': dense_inp}, outputs=out, name='DIN')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[tf.keras.metrics.AUC(name='auc')],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# user_id 也做一个编码（用于 embedding）\n",
        "all_user_ids = pd.unique(pd.concat([train_df['user_id'], valid_df['user_id']]).astype(int))\n",
        "raw_to_user_enc = {int(v): int(i + 1) for i, v in enumerate(np.sort(all_user_ids))}\n",
        "user_vocab_size = int(len(raw_to_user_enc) + 1)\n",
        "train_user_enc = np.asarray([raw_to_user_enc.get(int(x), 0) for x in train_df['user_id'].values], dtype=np.int32)\n",
        "valid_user_enc = np.asarray([raw_to_user_enc.get(int(x), 0) for x in valid_df['user_id'].values], dtype=np.int32)\n",
        "\n",
        "din = build_din(item_vocab_size=item_vocab_size, user_vocab_size=user_vocab_size, dense_dim=X_train_dense.shape[1])\n",
        "din.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a0d3d8b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "147/147 [==============================] - 6s 42ms/step - loss: 0.0740 - auc: 0.6111 - val_loss: 0.0203 - val_auc: 0.8682\n",
            "Epoch 2/2\n",
            "147/147 [==============================] - 6s 43ms/step - loss: 0.0195 - auc: 0.8790 - val_loss: 0.0190 - val_auc: 0.8947\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x17ff630d0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "din_train_inputs = {\n",
        "    'user_id': train_user_enc,\n",
        "    'hist_items': train_hist_mat,\n",
        "    'target_item': train_target_item,\n",
        "    'dense': X_train_dense,\n",
        "}\n",
        "din_valid_inputs = {\n",
        "    'user_id': valid_user_enc,\n",
        "    'hist_items': valid_hist_mat,\n",
        "    'target_item': valid_target_item,\n",
        "    'dense': X_valid_dense,\n",
        "}\n",
        "\n",
        "din.fit(\n",
        "    din_train_inputs,\n",
        "    y_train,\n",
        "    batch_size=2048,\n",
        "    epochs=2,\n",
        "    validation_data=(din_valid_inputs, y_valid),\n",
        "    verbose=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3e0d3c31",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hit_rate@5': 0.213, 'ndcg@5': 0.14482263336251341, 'num_users': 4000}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_valid_din = din.predict(din_valid_inputs, batch_size=4096, verbose=0)\n",
        "hit_ndcg_at_k(valid_df, pred_valid_din, k=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fece7c15",
      "metadata": {},
      "source": [
        "## 6) 保存产物\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4d490ee5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved to: /Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system/artifacts/ranking/deep_models\n"
          ]
        }
      ],
      "source": [
        "deepfm.save(ARTIFACTS_DIR / 'deepfm.keras')\n",
        "din.save(ARTIFACTS_DIR / 'din.keras')\n",
        "\n",
        "# 额外导出 SavedModel（更通用，且避免 .keras 在 safe_mode 下的 Lambda 反序列化限制）\n",
        "deepfm_sm = ARTIFACTS_DIR / 'deepfm_savedmodel'\n",
        "din_sm = ARTIFACTS_DIR / 'din_savedmodel'\n",
        "if not deepfm_sm.exists():\n",
        "    deepfm.save(deepfm_sm)\n",
        "if not din_sm.exists():\n",
        "    din.save(din_sm)\n",
        "\n",
        "import pickle\n",
        "with open(ARTIFACTS_DIR / 'scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "with open(ARTIFACTS_DIR / 'deepfm_factorizers.pkl', 'wb') as f:\n",
        "    pickle.dump({'sparse_cols': SPARSE_COLS, 'dense_cols': DENSE_COLS, 'uniques': factor_uniques}, f)\n",
        "with open(ARTIFACTS_DIR / 'din_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump({'raw_to_item_enc': raw_to_item_enc, 'raw_to_user_enc': raw_to_user_enc}, f)\n",
        "\n",
        "print('saved to:', ARTIFACTS_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "funrec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
