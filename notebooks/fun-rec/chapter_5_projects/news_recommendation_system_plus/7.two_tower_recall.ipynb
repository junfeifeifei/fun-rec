{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4aa2c6de",
      "metadata": {},
      "source": [
        "# 7. 双塔召回（Two-Tower + In-batch Negatives + FAISS）\n",
        "\n",
        "这一节把基础版项目缺失的**双塔召回训练**补齐，并给出一条更贴近工业界的端到端流程：\n",
        "\n",
        "1) 构造训练样本（序列 next-item）\n",
        "\n",
        "2) 训练双塔（用户塔 / 物品塔）\n",
        "\n",
        "3) 导出 item embedding，建立 FAISS 索引\n",
        "\n",
        "4) 离线评估 Recall（HitRate@K / NDCG@K）\n",
        "\n",
        "5) 产出召回候选集（给后续排序模型训练用）\n",
        "\n",
        "## 面试/实习加分点（建议你在 README / 简历里写）\n",
        "\n",
        "- **双塔训练**：in-batch negatives（对比学习 / InfoNCE）\n",
        "- **负样本策略**：in-batch 负样本 +（可扩展：hard negative）\n",
        "- **向量检索**：FAISS IndexFlatIP / IVF（可扩展）\n",
        "- **一致性**：离线 item 向量 + 在线 user 向量 + ANN 检索\n",
        "- **评估**：Recall 指标 + 过滤已看历史（避免数据泄漏）\n",
        "\n",
        "## 产物位置\n",
        "\n",
        "默认写入：`tmp/projects/news_recommendation_system/artifacts/two_tower/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4a47aeff",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('/Users/wangjunfei/Desktop/fun-rec/data/dataset/news_recommendation'),\n",
              " PosixPath('/Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system'),\n",
              " PosixPath('/Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system/artifacts/two_tower/dssm_inbatch'))"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import faiss\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "from tqdm import tqdm\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    cur = start\n",
        "    for _ in range(10):\n",
        "        if (cur / 'pyproject.toml').exists() or (cur / '.git').exists():\n",
        "            return cur\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "\n",
        "# 读取 .env（如果存在）\n",
        "dotenv_path = find_dotenv(usecwd=True)\n",
        "if dotenv_path:\n",
        "    load_dotenv(dotenv_path)\n",
        "\n",
        "# 回退默认路径（保持与基础版一致）\n",
        "os.environ.setdefault('FUNREC_RAW_DATA_PATH', str(REPO_ROOT / 'data'))\n",
        "os.environ.setdefault('FUNREC_PROCESSED_DATA_PATH', str(REPO_ROOT / 'tmp'))\n",
        "\n",
        "RAW_DATA_PATH = Path(os.getenv('FUNREC_RAW_DATA_PATH'))\n",
        "PROCESSED_DATA_PATH = Path(os.getenv('FUNREC_PROCESSED_DATA_PATH'))\n",
        "\n",
        "DATA_PATH = RAW_DATA_PATH / 'dataset' / 'news_recommendation'\n",
        "if not DATA_PATH.exists():\n",
        "    DATA_PATH = RAW_DATA_PATH / 'news_recommendation'\n",
        "\n",
        "PROJECT_PATH = PROCESSED_DATA_PATH / 'projects' / 'news_recommendation_system'\n",
        "ARTIFACTS_DIR = PROJECT_PATH / 'artifacts' / 'two_tower' / 'dssm_inbatch'\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_PATH, PROJECT_PATH, ARTIFACTS_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d3ecdb5",
      "metadata": {},
      "source": [
        "## 1) 数据准备：离线切分（每个用户最后一次点击做验证）\n",
        "\n",
        "如果你已经跑过基础版 2.baseline.ipynb，会存在：\n",
        "\n",
        "- `tmp/projects/news_recommendation_system/train_hist.pkl`\n",
        "- `tmp/projects/news_recommendation_system/valid_last.pkl`\n",
        "\n",
        "这里会优先复用；否则自动从原始点击日志构建。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e4e665c5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(   user_id  click_article_id  click_timestamp  click_environment  \\\n",
              " 0        0             30760    1508211672520                  4   \n",
              " 1        1            289197    1508211316889                  4   \n",
              " 2        2             36162    1508211438695                  4   \n",
              " 3        3             50644    1508211359672                  4   \n",
              " 4        4             42567    1508211625466                  4   \n",
              " \n",
              "    click_deviceGroup  click_os  click_country  click_region  \\\n",
              " 0                  1        17              1            25   \n",
              " 1                  1        17              1            25   \n",
              " 2                  3        20              1            25   \n",
              " 3                  3         2              1            25   \n",
              " 4                  1        12              1            16   \n",
              " \n",
              "    click_referrer_type  \n",
              " 0                    2  \n",
              " 1                    6  \n",
              " 2                    2  \n",
              " 3                    2  \n",
              " 4                    1  ,\n",
              "    user_id  click_article_id  click_timestamp  click_environment  \\\n",
              " 0        0            157507    1508211702520                  4   \n",
              " 1        1             63746    1508211346889                  4   \n",
              " 2        2            168401    1508211468695                  4   \n",
              " 3        3             36162    1508211389672                  4   \n",
              " 4        4             39894    1508211655466                  4   \n",
              " \n",
              "    click_deviceGroup  click_os  click_country  click_region  \\\n",
              " 0                  1        17              1            25   \n",
              " 1                  1        17              1            25   \n",
              " 2                  3        20              1            25   \n",
              " 3                  3         2              1            25   \n",
              " 4                  1        12              1            16   \n",
              " \n",
              "    click_referrer_type  \n",
              " 0                    2  \n",
              " 1                    6  \n",
              " 2                    2  \n",
              " 3                    2  \n",
              " 4                    1  ,\n",
              "    article_id  category_id  created_at_ts  words_count\n",
              " 0           0            0  1513144419000          168\n",
              " 1           1            1  1405341936000          189\n",
              " 2           2            1  1408667706000          250\n",
              " 3           3            1  1408468313000          230\n",
              " 4           4            1  1407071171000          162)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def build_offline_split_last_click(click_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    click_df = click_df.sort_values(['user_id', 'click_timestamp'])\n",
        "    last_click = click_df.groupby('user_id').tail(1)\n",
        "    hist = click_df.drop(last_click.index)\n",
        "\n",
        "    valid_users = hist['user_id'].unique()\n",
        "    hist = hist[hist['user_id'].isin(valid_users)]\n",
        "    last_click = last_click[last_click['user_id'].isin(valid_users)]\n",
        "    return hist.reset_index(drop=True), last_click.reset_index(drop=True)\n",
        "\n",
        "\n",
        "train_hist_path = PROJECT_PATH / 'train_hist.pkl'\n",
        "valid_last_path = PROJECT_PATH / 'valid_last.pkl'\n",
        "\n",
        "if train_hist_path.exists() and valid_last_path.exists():\n",
        "    train_hist = pd.read_pickle(train_hist_path)\n",
        "    valid_last = pd.read_pickle(valid_last_path)\n",
        "else:\n",
        "    train_click = pd.read_csv(DATA_PATH / 'train_click_log.csv')\n",
        "    train_hist, valid_last = build_offline_split_last_click(train_click)\n",
        "    train_hist.to_pickle(train_hist_path)\n",
        "    valid_last.to_pickle(valid_last_path)\n",
        "\n",
        "articles = pd.read_csv(DATA_PATH / 'articles.csv')\n",
        "\n",
        "train_hist.head(), valid_last.head(), articles.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5ab1da7b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20000, 92646, 20000)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 可选：DEBUG 采样（更快）\n",
        "DEBUG = True\n",
        "MAX_USERS = 20000\n",
        "SEED = 42\n",
        "\n",
        "if DEBUG:\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    users = train_hist['user_id'].unique()\n",
        "    if len(users) > MAX_USERS:\n",
        "        sample_users = rng.choice(users, size=MAX_USERS, replace=False)\n",
        "        train_hist = train_hist[train_hist['user_id'].isin(sample_users)]\n",
        "        valid_last = valid_last[valid_last['user_id'].isin(sample_users)]\n",
        "\n",
        "train_hist['user_id'].nunique(), len(train_hist), len(valid_last)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e251d4",
      "metadata": {},
      "source": [
        "## 2) ID 编码与样本构造\n",
        "\n",
        "我们需要把原始 `user_id/article_id/category_id` 编码成从 1 开始的连续整数：\n",
        "\n",
        "- 0 预留给 padding/unknown\n",
        "- embedding table 的 vocab_size = num_classes + 1\n",
        "\n",
        "训练样本采用 next-item：\n",
        "\n",
        "- 输入：用户 id + 历史点击序列（定长 padding）\n",
        "- label：下一个点击 item（物品塔输入）\n",
        "\n",
        "为避免样本爆炸，这里提供 `MAX_SAMPLES_PER_USER` 控制每个用户最多生成多少条样本（面试时可解释：工程上可用采样/重放/窗口裁剪）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3a50ee4a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20001, 364048, 462)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@dataclass(frozen=True)\n",
        "class IdMap:\n",
        "    name: str\n",
        "    classes_: np.ndarray\n",
        "    offset: int = 1\n",
        "    unknown_value: int = 0\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return int(len(self.classes_) + self.offset)\n",
        "\n",
        "    def transform(self, values) -> np.ndarray:\n",
        "        index = pd.Index(self.classes_)\n",
        "        arr = np.asarray(values)\n",
        "        flat = arr.reshape(-1)\n",
        "        idx = index.get_indexer(flat)\n",
        "        out = idx.astype(np.int64) + self.offset\n",
        "        out[idx < 0] = self.unknown_value\n",
        "        return out.reshape(arr.shape).astype(np.int32)\n",
        "\n",
        "    @classmethod\n",
        "    def fit(cls, name: str, values, offset: int = 1) -> 'IdMap':\n",
        "        uniq = pd.unique(pd.Series(list(values)))\n",
        "        try:\n",
        "            uniq = np.array(sorted(uniq))\n",
        "        except Exception:\n",
        "            uniq = np.array(list(uniq))\n",
        "        return cls(name=name, classes_=uniq, offset=offset)\n",
        "\n",
        "\n",
        "def pad_left(seqs: List[List[int]], max_len: int, pad_value: int = 0) -> np.ndarray:\n",
        "    out = np.full((len(seqs), max_len), pad_value, dtype=np.int32)\n",
        "    for i, seq in enumerate(seqs):\n",
        "        if not seq:\n",
        "            continue\n",
        "        seq = seq[-max_len:]\n",
        "        out[i, -len(seq):] = np.asarray(seq, dtype=np.int32)\n",
        "    return out\n",
        "\n",
        "\n",
        "# 物品到类别映射（不存在的置 0）\n",
        "item_to_cat = dict(zip(articles['article_id'].astype(int), articles['category_id'].astype(int)))\n",
        "\n",
        "# 编码器：item 编码用 articles 保证 vocab 稳定\n",
        "user_id_map = IdMap.fit('user_id', train_hist['user_id'].unique(), offset=1)\n",
        "item_id_map = IdMap.fit('article_id', articles['article_id'].unique(), offset=1)\n",
        "cat_id_map = IdMap.fit('category_id', articles['category_id'].unique(), offset=1)\n",
        "\n",
        "user_id_map.vocab_size, item_id_map.vocab_size, cat_id_map.vocab_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6494d143",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "build_samples: 100%|██████████| 20000/20000 [00:00<00:00, 150475.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_samples: 65809\n",
            "num_users: 11924\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==================== 样本构造（滑窗 next-item） ====================\n",
        "MAX_SEQ_LEN = 30\n",
        "MIN_HIST = 1\n",
        "MAX_SAMPLES_PER_USER = 20 if DEBUG else 200\n",
        "\n",
        "train_hist_sorted = train_hist.sort_values(['user_id', 'click_timestamp'])\n",
        "user_hist = train_hist_sorted.groupby('user_id')['click_article_id'].apply(list).to_dict()\n",
        "\n",
        "samples_user: List[int] = []\n",
        "samples_hist: List[List[int]] = []\n",
        "samples_item: List[int] = []\n",
        "samples_cat: List[int] = []\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "for u, seq in tqdm(user_hist.items(), desc='build_samples'):\n",
        "    if len(seq) < (MIN_HIST + 1):\n",
        "        continue\n",
        "    # 每个用户可选：只取最后 MAX_SAMPLES_PER_USER 次点击作为训练目标（更贴近兴趣演化）\n",
        "    positions = list(range(1, len(seq)))\n",
        "    if len(positions) > MAX_SAMPLES_PER_USER:\n",
        "        # 采样一部分位置（偏向最近）\n",
        "        tail = positions[-MAX_SAMPLES_PER_USER * 3 :]\n",
        "        positions = rng.choice(tail, size=MAX_SAMPLES_PER_USER, replace=False).tolist()\n",
        "        positions.sort()\n",
        "\n",
        "    for t in positions:\n",
        "        hist = seq[max(0, t - MAX_SEQ_LEN) : t]\n",
        "        if len(hist) < MIN_HIST:\n",
        "            continue\n",
        "        target_item = int(seq[t])\n",
        "        target_cat = int(item_to_cat.get(target_item, 0))\n",
        "        samples_user.append(int(u))\n",
        "        samples_hist.append([int(x) for x in hist])\n",
        "        samples_item.append(target_item)\n",
        "        samples_cat.append(target_cat)\n",
        "\n",
        "print('num_samples:', len(samples_user))\n",
        "print('num_users:', len(set(samples_user)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96fc69f6",
      "metadata": {},
      "source": [
        "## 3) 双塔模型（In-batch Negatives / InfoNCE）\n",
        "\n",
        "核心做法：\n",
        "\n",
        "- 一个 batch 内，(user_i, item_i) 作为正样本\n",
        "- 其它 (user_i, item_j) 作为负样本（in-batch negatives）\n",
        "- 用 softmax 交叉熵做对比学习（可选双向：user→item + item→user）\n",
        "\n",
        "注意：如果 batch 内存在重复 item，会引入“伪负样本”；工业里会用去重 batch / 多正样本 loss / sampled softmax 等手段缓解。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "01816c89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"two_tower_inbatch\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " user_id (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " hist_article_id (InputLaye  [(None, 30)]                 0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " article_id (InputLayer)     [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " category_id (InputLayer)    [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " emb_user (Embedding)        (None, 32)                   640032    ['user_id[0][0]']             \n",
            "                                                                                                  \n",
            " emb_item (Embedding)        multiple                     1164953   ['hist_article_id[0][0]',     \n",
            "                                                          6          'article_id[0][0]']          \n",
            "                                                                                                  \n",
            " emb_cat (Embedding)         (None, 8)                    3696      ['category_id[0][0]']         \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 32)                   0         ['emb_user[0][0]']            \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 32)                   0         ['emb_item[0][0]',            \n",
            "                                                                     'hist_article_id[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 32)                   0         ['emb_item[1][0]']            \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)         (None, 8)                    0         ['emb_cat[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 64)                   0         ['flatten[0][0]',             \n",
            "                                                                     'lambda[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 40)                   0         ['flatten_1[0][0]',           \n",
            " )                                                                   'flatten_2[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  8320      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 128)                  5248      ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 128)                  0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 128)                  0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 64)                   8256      ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 64)                   8256      ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 64)                   0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 64)                   0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 32)                   2080      ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 32)                   2080      ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 32)                   0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 32)                   0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " user_vec (Lambda)           (None, 32)                   0         ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " item_vec (Lambda)           (None, 32)                   0         ['dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            " logits (Lambda)             (None, None)                 0         ['user_vec[0][0]',            \n",
            "                                                                     'item_vec[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 12327504 (47.03 MB)\n",
            "Trainable params: 12327504 (47.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def masked_mean(emb: tf.Tensor, ids: tf.Tensor) -> tf.Tensor:\n",
        "    # emb: [B, L, D], ids: [B, L]\n",
        "    mask = tf.cast(tf.not_equal(ids, 0), tf.float32)  # [B, L]\n",
        "    mask = tf.expand_dims(mask, axis=-1)  # [B, L, 1]\n",
        "    summed = tf.reduce_sum(emb * mask, axis=1)  # [B, D]\n",
        "    denom = tf.reduce_sum(mask, axis=1)  # [B, 1]\n",
        "    return summed / tf.maximum(denom, 1.0)\n",
        "\n",
        "\n",
        "def build_two_tower_model(\n",
        "    user_vocab_size: int,\n",
        "    item_vocab_size: int,\n",
        "    cat_vocab_size: int,\n",
        "    max_seq_len: int = 30,\n",
        "    emb_dim: int = 32,\n",
        "    dnn_units: List[int] = [128, 64, 32],\n",
        "    temperature: float = 0.05,\n",
        "):\n",
        "    # Inputs\n",
        "    user_id_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='user_id')\n",
        "    hist_item_inp = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name='hist_article_id')\n",
        "    target_item_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='article_id')\n",
        "    target_cat_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='category_id')\n",
        "\n",
        "    # Embeddings\n",
        "    user_emb_layer = tf.keras.layers.Embedding(user_vocab_size, emb_dim, name='emb_user')\n",
        "    item_emb_layer = tf.keras.layers.Embedding(item_vocab_size, emb_dim, name='emb_item')\n",
        "    cat_emb_layer = tf.keras.layers.Embedding(cat_vocab_size, max(4, emb_dim // 4), name='emb_cat')\n",
        "\n",
        "    user_id_emb = tf.keras.layers.Flatten()(user_emb_layer(user_id_inp))  # [B, D]\n",
        "    hist_item_emb = item_emb_layer(hist_item_inp)  # [B, L, D]\n",
        "    hist_mean = tf.keras.layers.Lambda(lambda x: masked_mean(x[0], x[1]))([hist_item_emb, hist_item_inp])\n",
        "\n",
        "    user_feat = tf.keras.layers.Concatenate()([user_id_emb, hist_mean])\n",
        "    x = user_feat\n",
        "    for units in dnn_units:\n",
        "        x = tf.keras.layers.Dense(units, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "    user_vec = tf.keras.layers.Lambda(lambda t: tf.nn.l2_normalize(t, axis=1), name='user_vec')(x)\n",
        "\n",
        "    item_id_emb = tf.keras.layers.Flatten()(item_emb_layer(target_item_inp))\n",
        "    cat_emb = tf.keras.layers.Flatten()(cat_emb_layer(target_cat_inp))\n",
        "    item_feat = tf.keras.layers.Concatenate()([item_id_emb, cat_emb])\n",
        "    y = item_feat\n",
        "    for units in dnn_units:\n",
        "        y = tf.keras.layers.Dense(units, activation='relu')(y)\n",
        "        y = tf.keras.layers.Dropout(0.1)(y)\n",
        "    item_vec = tf.keras.layers.Lambda(lambda t: tf.nn.l2_normalize(t, axis=1), name='item_vec')(y)\n",
        "\n",
        "    # In-batch logits: [B, D] x [B, D]^T => [B, B]\n",
        "    logits = tf.keras.layers.Lambda(lambda z: tf.matmul(z[0], z[1], transpose_b=True) / temperature, name='logits')(\n",
        "        [user_vec, item_vec]\n",
        "    )\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs={'user_id': user_id_inp, 'hist_article_id': hist_item_inp, 'article_id': target_item_inp, 'category_id': target_cat_inp},\n",
        "        outputs=logits,\n",
        "        name='two_tower_inbatch',\n",
        "    )\n",
        "    user_tower = tf.keras.Model(inputs={'user_id': user_id_inp, 'hist_article_id': hist_item_inp}, outputs=user_vec, name='user_tower')\n",
        "    item_tower = tf.keras.Model(inputs={'article_id': target_item_inp, 'category_id': target_cat_inp}, outputs=item_vec, name='item_tower')\n",
        "    return model, user_tower, item_tower\n",
        "\n",
        "\n",
        "TEMPERATURE = 0.05\n",
        "EMB_DIM = 32\n",
        "DNN_UNITS = [128, 64, 32]\n",
        "\n",
        "model, user_tower, item_tower = build_two_tower_model(\n",
        "    user_vocab_size=user_id_map.vocab_size,\n",
        "    item_vocab_size=item_id_map.vocab_size,\n",
        "    cat_vocab_size=cat_id_map.vocab_size,\n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        "    emb_dim=EMB_DIM,\n",
        "    dnn_units=DNN_UNITS,\n",
        "    temperature=TEMPERATURE,\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59bce25f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "65/65 [==============================] - 2s 31ms/step - loss: 7.6184\n",
            "Epoch 2/3\n",
            "65/65 [==============================] - 2s 29ms/step - loss: 6.9840\n",
            "Epoch 3/3\n",
            "65/65 [==============================] - 2s 30ms/step - loss: 6.9355\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
            "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
            "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
            "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
          ]
        }
      ],
      "source": [
        "def inbatch_symmetric_loss(y_true, logits):\n",
        "    # y_true ignored\n",
        "    b = tf.shape(logits)[0]\n",
        "    labels = tf.range(b)\n",
        "    loss_u2i = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "    loss_i2u = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=tf.transpose(logits))\n",
        "    return tf.reduce_mean(loss_u2i + loss_i2u) / 2.0\n",
        "\n",
        "\n",
        "try:\n",
        "    # Mac M1/M2 上 Keras v2.11+ 的新 optimizer 可能会明显变慢；这里直接用 legacy 版本更稳。\n",
        "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-4)\n",
        "except Exception:\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=inbatch_symmetric_loss,\n",
        ")\n",
        "\n",
        "# 组装训练数据\n",
        "X_user = user_id_map.transform(np.asarray(samples_user, dtype=np.int64))\n",
        "X_hist = pad_left([item_id_map.transform(np.asarray(s, dtype=np.int64)).tolist() for s in samples_hist], max_len=MAX_SEQ_LEN)\n",
        "X_item = item_id_map.transform(np.asarray(samples_item, dtype=np.int64))\n",
        "X_cat = cat_id_map.transform(np.asarray(samples_cat, dtype=np.int64))\n",
        "\n",
        "train_X = {\n",
        "    'user_id': X_user,\n",
        "    'hist_article_id': X_hist,\n",
        "    'article_id': X_item,\n",
        "    'category_id': X_cat,\n",
        "}\n",
        "\n",
        "dummy_y = np.zeros(len(X_user), dtype=np.float32)\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "EPOCHS = 3\n",
        "\n",
        "history = model.fit(train_X, dummy_y, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24442981",
      "metadata": {},
      "source": [
        "## 4) 导出 item embedding + FAISS 建索引\n",
        "\n",
        "这里用内积检索（IndexFlatIP），并做 L2 normalize，让内积等价于 cosine。\n",
        "\n",
        "你可以在面试里扩展：\n",
        "\n",
        "- Flat → IVF / HNSW 加速\n",
        "- offline build + online serving（增量更新）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d969ab5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 全量 item 输入（使用 raw article_id 作为 FAISS index id）\n",
        "all_items_raw = articles['article_id'].astype(int).unique()\n",
        "all_items_enc = item_id_map.transform(all_items_raw)\n",
        "all_cats_raw = np.asarray([item_to_cat.get(int(i), 0) for i in all_items_raw], dtype=np.int64)\n",
        "all_cats_enc = cat_id_map.transform(all_cats_raw)\n",
        "\n",
        "item_embs = item_tower.predict(\n",
        "    {'article_id': all_items_enc, 'category_id': all_cats_enc},\n",
        "    batch_size=4096,\n",
        "    verbose=0,\n",
        ").astype('float32')\n",
        "\n",
        "faiss.normalize_L2(item_embs)\n",
        "\n",
        "index = faiss.IndexIDMap2(faiss.IndexFlatIP(item_embs.shape[1]))\n",
        "index.add_with_ids(item_embs, all_items_raw.astype('int64'))\n",
        "\n",
        "faiss.write_index(index, str(ARTIFACTS_DIR / 'faiss_index.bin'))\n",
        "np.save(ARTIFACTS_DIR / 'item_embeddings.npy', item_embs)\n",
        "\n",
        "print('indexed items:', index.ntotal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f02f96",
      "metadata": {},
      "source": [
        "## 5) 离线召回评估（过滤已看历史）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a09045",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_recall_at_k(\n",
        "    user_hist: Dict[int, List[int]],\n",
        "    valid_last: pd.DataFrame,\n",
        "    topk: int = 20,\n",
        "):\n",
        "    # 构建评估样本：每个用户用 train_hist 的最后 MAX_SEQ_LEN 个点击作为输入\n",
        "    users = valid_last['user_id'].astype(int).tolist()\n",
        "    targets = valid_last['click_article_id'].astype(int).tolist()\n",
        "\n",
        "    X_u_raw = []\n",
        "    X_hist_raw = []\n",
        "    y_raw = []\n",
        "    for u, t in zip(users, targets):\n",
        "        seq = user_hist.get(u)\n",
        "        if not seq:\n",
        "            continue\n",
        "        X_u_raw.append(u)\n",
        "        X_hist_raw.append(seq[-MAX_SEQ_LEN:])\n",
        "        y_raw.append(t)\n",
        "\n",
        "    X_u = user_id_map.transform(np.asarray(X_u_raw, dtype=np.int64))\n",
        "    X_hist = pad_left([item_id_map.transform(np.asarray(s, dtype=np.int64)).tolist() for s in X_hist_raw], max_len=MAX_SEQ_LEN)\n",
        "\n",
        "    user_embs = user_tower.predict({'user_id': X_u, 'hist_article_id': X_hist}, batch_size=4096, verbose=0).astype('float32')\n",
        "    faiss.normalize_L2(user_embs)\n",
        "\n",
        "    # 为了过滤历史，检索时多拿一些\n",
        "    search_k = topk + MAX_SEQ_LEN + 10\n",
        "    D, I = index.search(user_embs, search_k)\n",
        "\n",
        "    hit = 0\n",
        "    ndcg = 0.0\n",
        "    for i in range(len(X_u_raw)):\n",
        "        u = X_u_raw[i]\n",
        "        hist_set = set(user_hist.get(u, []))\n",
        "        recs = []\n",
        "        for item_id in I[i].tolist():\n",
        "            item_id = int(item_id)\n",
        "            if item_id <= 0:\n",
        "                continue\n",
        "            if item_id in hist_set:\n",
        "                continue\n",
        "            recs.append(item_id)\n",
        "            if len(recs) >= topk:\n",
        "                break\n",
        "        target = int(y_raw[i])\n",
        "        if target in recs:\n",
        "            hit += 1\n",
        "            rank = recs.index(target)\n",
        "            ndcg += 1.0 / np.log2(rank + 2)\n",
        "\n",
        "    return {\n",
        "        f'hit_rate@{topk}': hit / max(1, len(X_u_raw)),\n",
        "        f'ndcg@{topk}': ndcg / max(1, len(X_u_raw)),\n",
        "        'num_users_eval': len(X_u_raw),\n",
        "    }\n",
        "\n",
        "\n",
        "metrics = evaluate_recall_at_k(user_hist, valid_last, topk=20)\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f8d0ae",
      "metadata": {},
      "source": [
        "## 6) 生成召回候选集（给排序训练用）\n",
        "\n",
        "输出列对齐基础版的 `recall_candidates.pkl`：\n",
        "\n",
        "- `user_id`\n",
        "- `article_id`\n",
        "- `recall_score`\n",
        "- `recall_rank`\n",
        "\n",
        "后续在 `8.deep_ranking.ipynb` 里可以把该候选集与 ItemCF/热门等进行融合，或做 rerank 训练样本。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc7429f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "TOPK_CANDIDATES = 100\n",
        "SEARCH_K = TOPK_CANDIDATES + MAX_SEQ_LEN + 10\n",
        "\n",
        "rows = []\n",
        "users_all = list(user_hist.keys())\n",
        "\n",
        "batch_size = 4096\n",
        "for start in tqdm(range(0, len(users_all), batch_size), desc='recall_all_users'):\n",
        "    end = min(start + batch_size, len(users_all))\n",
        "    u_raw_batch = users_all[start:end]\n",
        "    hist_raw_batch = [user_hist[u][-MAX_SEQ_LEN:] for u in u_raw_batch]\n",
        "\n",
        "    X_u = user_id_map.transform(np.asarray(u_raw_batch, dtype=np.int64))\n",
        "    X_hist = pad_left([item_id_map.transform(np.asarray(s, dtype=np.int64)).tolist() for s in hist_raw_batch], max_len=MAX_SEQ_LEN)\n",
        "\n",
        "    user_embs = user_tower.predict({'user_id': X_u, 'hist_article_id': X_hist}, batch_size=4096, verbose=0).astype('float32')\n",
        "    faiss.normalize_L2(user_embs)\n",
        "    D, I = index.search(user_embs, SEARCH_K)\n",
        "\n",
        "    for local_i, u in enumerate(u_raw_batch):\n",
        "        hist_set = set(user_hist.get(u, []))\n",
        "        rank = 0\n",
        "        for item_id, score in zip(I[local_i].tolist(), D[local_i].tolist()):\n",
        "            item_id = int(item_id)\n",
        "            if item_id <= 0:\n",
        "                continue\n",
        "            if item_id in hist_set:\n",
        "                continue\n",
        "            rank += 1\n",
        "            rows.append((int(u), int(item_id), float(score), int(rank)))\n",
        "            if rank >= TOPK_CANDIDATES:\n",
        "                break\n",
        "\n",
        "two_tower_recall_df = pd.DataFrame(rows, columns=['user_id', 'article_id', 'recall_score', 'recall_rank'])\n",
        "out_path = PROJECT_PATH / 'recall_candidates_two_tower.pkl'\n",
        "two_tower_recall_df.to_pickle(out_path)\n",
        "\n",
        "out_path, two_tower_recall_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "626a0f3b",
      "metadata": {},
      "source": [
        "## 7) 保存模型与编码器\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9502bd69",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(ARTIFACTS_DIR / 'two_tower_model.keras')\n",
        "user_tower.save(ARTIFACTS_DIR / 'user_tower.keras')\n",
        "item_tower.save(ARTIFACTS_DIR / 'item_tower.keras')\n",
        "\n",
        "with open(ARTIFACTS_DIR / 'id_maps.pkl', 'wb') as f:\n",
        "    pickle.dump({'user': user_id_map, 'item': item_id_map, 'cat': cat_id_map}, f)\n",
        "\n",
        "with open(ARTIFACTS_DIR / 'metrics.pkl', 'wb') as f:\n",
        "    pickle.dump(metrics, f)\n",
        "\n",
        "print('saved to:', ARTIFACTS_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "funrec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
