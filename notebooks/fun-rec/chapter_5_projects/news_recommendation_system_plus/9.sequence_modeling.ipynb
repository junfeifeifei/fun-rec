{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. 序列建模（SASRec / DIEN）\n",
        "\n",
        "推荐系统里“序列”几乎是标配：用户兴趣会随时间变化，单纯的静态特征不够。\n",
        "\n",
        "本 notebook 目标：\n",
        "\n",
        "1) 实现一个可训练的 **SASRec（Transformer）召回模型**，并用 FAISS 做 TopK 检索评估\n",
        "\n",
        "2) 实现一个 **DIEN-like（GRU + 注意力）排序模型**，展示“兴趣抽取/演化”的思路\n",
        "\n",
        "## 面试要点\n",
        "\n",
        "- 序列推荐 vs 静态推荐：兴趣演化、短期意图\n",
        "- Transformer 优势：并行、长依赖（但要注意计算/存储）\n",
        "- 训练目标：next-item / contrastive / sampled softmax\n",
        "- 线上：用户向量实时更新 + ANN 检索\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import pickle\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import faiss\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    cur = start\n",
        "    for _ in range(10):\n",
        "        if (cur / 'pyproject.toml').exists() or (cur / '.git').exists():\n",
        "            return cur\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "dotenv_path = find_dotenv(usecwd=True)\n",
        "if dotenv_path:\n",
        "    load_dotenv(dotenv_path)\n",
        "os.environ.setdefault('FUNREC_RAW_DATA_PATH', str(REPO_ROOT / 'data'))\n",
        "os.environ.setdefault('FUNREC_PROCESSED_DATA_PATH', str(REPO_ROOT / 'tmp'))\n",
        "\n",
        "RAW_DATA_PATH = Path(os.getenv('FUNREC_RAW_DATA_PATH'))\n",
        "PROCESSED_DATA_PATH = Path(os.getenv('FUNREC_PROCESSED_DATA_PATH'))\n",
        "\n",
        "DATA_PATH = RAW_DATA_PATH / 'dataset' / 'news_recommendation'\n",
        "if not DATA_PATH.exists():\n",
        "    DATA_PATH = RAW_DATA_PATH / 'news_recommendation'\n",
        "\n",
        "PROJECT_PATH = PROCESSED_DATA_PATH / 'projects' / 'news_recommendation_system'\n",
        "SASREC_DIR = PROJECT_PATH / 'artifacts' / 'sequence' / 'sasrec_inbatch'\n",
        "SASREC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_PATH, PROJECT_PATH, SASREC_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part A：SASRec（召回）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_hist = pd.read_pickle(PROJECT_PATH / 'train_hist.pkl')\n",
        "valid_last = pd.read_pickle(PROJECT_PATH / 'valid_last.pkl')\n",
        "articles = pd.read_csv(DATA_PATH / 'articles.csv')\n",
        "\n",
        "DEBUG = True\n",
        "MAX_USERS = 20000\n",
        "SEED = 42\n",
        "\n",
        "if DEBUG:\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    users = train_hist['user_id'].unique()\n",
        "    if len(users) > MAX_USERS:\n",
        "        sample_users = rng.choice(users, size=MAX_USERS, replace=False)\n",
        "        train_hist = train_hist[train_hist['user_id'].isin(sample_users)]\n",
        "        valid_last = valid_last[valid_last['user_id'].isin(sample_users)]\n",
        "\n",
        "train_hist = train_hist.sort_values(['user_id', 'click_timestamp'])\n",
        "user_hist: Dict[int, List[int]] = train_hist.groupby('user_id')['click_article_id'].apply(list).to_dict()\n",
        "\n",
        "len(user_hist), len(train_hist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class IdMap:\n",
        "    name: str\n",
        "    classes_: np.ndarray\n",
        "    offset: int = 1\n",
        "    unknown_value: int = 0\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return int(len(self.classes_) + self.offset)\n",
        "\n",
        "    def transform(self, values) -> np.ndarray:\n",
        "        index = pd.Index(self.classes_)\n",
        "        arr = np.asarray(values)\n",
        "        flat = arr.reshape(-1)\n",
        "        idx = index.get_indexer(flat)\n",
        "        out = idx.astype(np.int64) + self.offset\n",
        "        out[idx < 0] = self.unknown_value\n",
        "        return out.reshape(arr.shape).astype(np.int32)\n",
        "\n",
        "    @classmethod\n",
        "    def fit(cls, name: str, values, offset: int = 1) -> 'IdMap':\n",
        "        uniq = pd.unique(pd.Series(list(values)))\n",
        "        try:\n",
        "            uniq = np.array(sorted(uniq))\n",
        "        except Exception:\n",
        "            uniq = np.array(list(uniq))\n",
        "        return cls(name=name, classes_=uniq, offset=offset)\n",
        "\n",
        "\n",
        "def pad_left(seqs: List[List[int]], max_len: int, pad_value: int = 0) -> np.ndarray:\n",
        "    out = np.full((len(seqs), max_len), pad_value, dtype=np.int32)\n",
        "    for i, seq in enumerate(seqs):\n",
        "        if not seq:\n",
        "            continue\n",
        "        seq = seq[-max_len:]\n",
        "        out[i, -len(seq):] = np.asarray(seq, dtype=np.int32)\n",
        "    return out\n",
        "\n",
        "\n",
        "item_id_map = IdMap.fit('article_id', articles['article_id'].astype(int).unique(), offset=1)\n",
        "item_id_map.vocab_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== 构造训练样本（next-item） ====================\n",
        "MAX_SEQ_LEN = 50\n",
        "MAX_SAMPLES_PER_USER = 20 if DEBUG else 200\n",
        "\n",
        "seq_samples: List[List[int]] = []\n",
        "pos_items: List[int] = []\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "for u, seq in tqdm(user_hist.items(), desc='build_seq_samples'):\n",
        "    if len(seq) < 2:\n",
        "        continue\n",
        "    positions = list(range(1, len(seq)))\n",
        "    if len(positions) > MAX_SAMPLES_PER_USER:\n",
        "        tail = positions[-MAX_SAMPLES_PER_USER * 3 :]\n",
        "        positions = rng.choice(tail, size=MAX_SAMPLES_PER_USER, replace=False).tolist()\n",
        "        positions.sort()\n",
        "\n",
        "    for t in positions:\n",
        "        hist = seq[max(0, t - MAX_SEQ_LEN) : t]\n",
        "        target = int(seq[t])\n",
        "        seq_samples.append([int(x) for x in hist])\n",
        "        pos_items.append(target)\n",
        "\n",
        "X_seq = pad_left([item_id_map.transform(np.asarray(s, dtype=np.int64)).tolist() for s in seq_samples], max_len=MAX_SEQ_LEN)\n",
        "X_pos = item_id_map.transform(np.asarray(pos_items, dtype=np.int64))\n",
        "\n",
        "print('num_samples:', len(X_pos))\n",
        "X_seq.shape, X_pos.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SASRec（Transformer）实现（in-batch negatives）\n",
        "\n",
        "为了让 notebook 在大 vocab（36w item）下也能跑，这里不做全量 softmax，而是用 **in-batch negatives**：\n",
        "\n",
        "- user 表示来自 Transformer 对序列的编码\n",
        "- item 表示来自 item embedding\n",
        "- batch 内做对比学习（InfoNCE）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_sasrec_inbatch(\n",
        "    item_vocab_size: int,\n",
        "    max_seq_len: int = 50,\n",
        "    emb_dim: int = 64,\n",
        "    num_heads: int = 2,\n",
        "    num_blocks: int = 2,\n",
        "    ff_dim: int = 128,\n",
        "    dropout: float = 0.2,\n",
        "    temperature: float = 0.05,\n",
        "):\n",
        "    if emb_dim % num_heads != 0:\n",
        "        raise ValueError(f'emb_dim ({emb_dim}) must be divisible by num_heads ({num_heads})')\n",
        "\n",
        "    seq_inp = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name='seq_ids')\n",
        "    pos_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='pos_item')\n",
        "\n",
        "    item_emb = tf.keras.layers.Embedding(item_vocab_size, emb_dim, mask_zero=True, name='item_emb')\n",
        "    pos_emb = tf.keras.layers.Embedding(max_seq_len, emb_dim, name='pos_emb')\n",
        "\n",
        "    x = item_emb(seq_inp)  # [B, L, D]\n",
        "    positions = tf.range(max_seq_len)\n",
        "    x = x + pos_emb(positions)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "        attn_in = tf.keras.layers.LayerNormalization()(x)\n",
        "        attn_out = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=emb_dim // num_heads,\n",
        "            dropout=dropout,\n",
        "            name=f'mha_{i}',\n",
        "        )(attn_in, attn_in, use_causal_mask=True)\n",
        "        x = x + attn_out\n",
        "\n",
        "        ffn_in = tf.keras.layers.LayerNormalization()(x)\n",
        "        ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dropout(dropout),\n",
        "            tf.keras.layers.Dense(emb_dim),\n",
        "        ], name=f'ffn_{i}')(ffn_in)\n",
        "        x = x + ffn\n",
        "\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    user_vec = tf.keras.layers.Lambda(lambda t: t[:, -1, :], name='last_state')(x)\n",
        "    user_vec = tf.keras.layers.Dense(emb_dim, activation=None)(user_vec)\n",
        "    user_vec = tf.keras.layers.Lambda(lambda t: tf.nn.l2_normalize(t, axis=1), name='user_vec')(user_vec)\n",
        "\n",
        "    item_vec = tf.keras.layers.Flatten()(item_emb(pos_inp))\n",
        "    item_vec = tf.keras.layers.Dense(emb_dim, activation=None)(item_vec)\n",
        "    item_vec = tf.keras.layers.Lambda(lambda t: tf.nn.l2_normalize(t, axis=1), name='item_vec')(item_vec)\n",
        "\n",
        "    logits = tf.keras.layers.Lambda(lambda z: tf.matmul(z[0], z[1], transpose_b=True) / temperature, name='logits')([user_vec, item_vec])\n",
        "\n",
        "    model = tf.keras.Model(inputs={'seq_ids': seq_inp, 'pos_item': pos_inp}, outputs=logits, name='SASRecInBatch')\n",
        "    user_model = tf.keras.Model(inputs={'seq_ids': seq_inp}, outputs=user_vec, name='sasrec_user')\n",
        "    item_model = tf.keras.Model(inputs={'pos_item': pos_inp}, outputs=item_vec, name='sasrec_item')\n",
        "    return model, user_model, item_model\n",
        "\n",
        "\n",
        "def inbatch_symmetric_loss(y_true, logits):\n",
        "    b = tf.shape(logits)[0]\n",
        "    labels = tf.range(b)\n",
        "    loss_u2i = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "    loss_i2u = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=tf.transpose(logits))\n",
        "    return tf.reduce_mean(loss_u2i + loss_i2u) / 2.0\n",
        "\n",
        "\n",
        "model, sasrec_user, sasrec_item = build_sasrec_inbatch(\n",
        "    item_vocab_size=item_id_map.vocab_size,\n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        "    emb_dim=64,\n",
        "    num_heads=2,\n",
        "    num_blocks=2,\n",
        "    ff_dim=128,\n",
        "    dropout=0.2,\n",
        "    temperature=0.05,\n",
        ")\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(2e-4), loss=inbatch_symmetric_loss)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 1024\n",
        "EPOCHS = 3\n",
        "\n",
        "train_X = {'seq_ids': X_seq, 'pos_item': X_pos}\n",
        "dummy_y = np.zeros(len(X_pos), dtype=np.float32)\n",
        "\n",
        "model.fit(train_X, dummy_y, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FAISS 建索引与 Recall 评估\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 全量 item embedding（用 raw article_id 作为索引 id）\n",
        "all_items_raw = articles['article_id'].astype(int).unique()\n",
        "all_items_enc = item_id_map.transform(all_items_raw)\n",
        "\n",
        "item_embs = sasrec_item.predict({'pos_item': all_items_enc}, batch_size=4096, verbose=0).astype('float32')\n",
        "faiss.normalize_L2(item_embs)\n",
        "\n",
        "index = faiss.IndexIDMap2(faiss.IndexFlatIP(item_embs.shape[1]))\n",
        "index.add_with_ids(item_embs, all_items_raw.astype('int64'))\n",
        "\n",
        "faiss.write_index(index, str(SASREC_DIR / 'faiss_index.bin'))\n",
        "np.save(SASREC_DIR / 'item_embeddings.npy', item_embs)\n",
        "\n",
        "print('indexed items:', index.ntotal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_recall(user_hist: Dict[int, List[int]], valid_last: pd.DataFrame, topk: int = 20) -> Dict[str, float]:\n",
        "    users = valid_last['user_id'].astype(int).tolist()\n",
        "    targets = valid_last['click_article_id'].astype(int).tolist()\n",
        "\n",
        "    X_u = []\n",
        "    X_seq_raw = []\n",
        "    y = []\n",
        "    for u, t in zip(users, targets):\n",
        "        seq = user_hist.get(u)\n",
        "        if not seq:\n",
        "            continue\n",
        "        X_u.append(u)\n",
        "        X_seq_raw.append(seq[-MAX_SEQ_LEN:])\n",
        "        y.append(t)\n",
        "\n",
        "    X_seq_enc = pad_left([item_id_map.transform(np.asarray(s, dtype=np.int64)).tolist() for s in X_seq_raw], max_len=MAX_SEQ_LEN)\n",
        "    user_embs = sasrec_user.predict({'seq_ids': X_seq_enc}, batch_size=4096, verbose=0).astype('float32')\n",
        "    faiss.normalize_L2(user_embs)\n",
        "\n",
        "    search_k = topk + MAX_SEQ_LEN + 10\n",
        "    D, I = index.search(user_embs, search_k)\n",
        "\n",
        "    hit = 0\n",
        "    ndcg = 0.0\n",
        "    total = 0\n",
        "    for i in range(len(X_u)):\n",
        "        hist_set = set(user_hist.get(X_u[i], []))\n",
        "        recs = []\n",
        "        for item_id in I[i].tolist():\n",
        "            item_id = int(item_id)\n",
        "            if item_id < 0:\n",
        "                continue\n",
        "            if item_id in hist_set:\n",
        "                continue\n",
        "            recs.append(item_id)\n",
        "            if len(recs) >= topk:\n",
        "                break\n",
        "        target = int(y[i])\n",
        "        if target in recs:\n",
        "            hit += 1\n",
        "            rank = recs.index(target)\n",
        "            ndcg += 1.0 / np.log2(rank + 2)\n",
        "        total += 1\n",
        "\n",
        "    return {f'hit_rate@{topk}': hit / max(1, total), f'ndcg@{topk}': ndcg / max(1, total), 'num_users': total}\n",
        "\n",
        "\n",
        "metrics = evaluate_recall(user_hist, valid_last, topk=20)\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 生成召回候选集（给排序用）\n",
        "TOPK_CANDIDATES = 100\n",
        "SEARCH_K = TOPK_CANDIDATES + MAX_SEQ_LEN + 10\n",
        "\n",
        "rows = []\n",
        "users_all = list(user_hist.keys())\n",
        "batch_size = 4096\n",
        "for start in tqdm(range(0, len(users_all), batch_size), desc='recall_all_users'):\n",
        "    end = min(start + batch_size, len(users_all))\n",
        "    u_raw_batch = users_all[start:end]\n",
        "    seq_raw_batch = [user_hist[u][-MAX_SEQ_LEN:] for u in u_raw_batch]\n",
        "    X_seq_enc = pad_left([item_id_map.transform(np.asarray(s, dtype=np.int64)).tolist() for s in seq_raw_batch], max_len=MAX_SEQ_LEN)\n",
        "    user_embs = sasrec_user.predict({'seq_ids': X_seq_enc}, batch_size=4096, verbose=0).astype('float32')\n",
        "    faiss.normalize_L2(user_embs)\n",
        "    D, I = index.search(user_embs, SEARCH_K)\n",
        "\n",
        "    for local_i, u in enumerate(u_raw_batch):\n",
        "        hist_set = set(user_hist.get(u, []))\n",
        "        rank = 0\n",
        "        for item_id, score in zip(I[local_i].tolist(), D[local_i].tolist()):\n",
        "            item_id = int(item_id)\n",
        "            if item_id < 0:\n",
        "                continue\n",
        "            if item_id in hist_set:\n",
        "                continue\n",
        "            rank += 1\n",
        "            rows.append((int(u), int(item_id), float(score), int(rank)))\n",
        "            if rank >= TOPK_CANDIDATES:\n",
        "                break\n",
        "\n",
        "sasrec_recall_df = pd.DataFrame(rows, columns=['user_id', 'article_id', 'recall_score', 'recall_rank'])\n",
        "out_path = PROJECT_PATH / 'recall_candidates_sasrec.pkl'\n",
        "sasrec_recall_df.to_pickle(out_path)\n",
        "\n",
        "out_path, sasrec_recall_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存模型与指标\n",
        "model.save(SASREC_DIR / 'sasrec_inbatch.keras')\n",
        "sasrec_user.save(SASREC_DIR / 'user_tower.keras')\n",
        "sasrec_item.save(SASREC_DIR / 'item_tower.keras')\n",
        "\n",
        "with open(SASREC_DIR / 'item_id_map.pkl', 'wb') as f:\n",
        "    pickle.dump(item_id_map, f)\n",
        "with open(SASREC_DIR / 'metrics.pkl', 'wb') as f:\n",
        "    pickle.dump(metrics, f)\n",
        "\n",
        "print('saved to:', SASREC_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part B：DIEN-like（排序）\n",
        "\n",
        "DIEN 的核心：\n",
        "\n",
        "- 用 GRU 从行为序列抽取兴趣状态（Interest Extractor）\n",
        "- 用注意力让兴趣与当前候选 item 对齐\n",
        "- 再做一层“兴趣演化”（这里实现 AIGRU：注意力加权后再 GRU）\n",
        "\n",
        "为了避免重复构造训练集，这里直接复用基础版 5.feature_engineering 的输出：`rank_train.pkl`。\n",
        "\n",
        "如果你还没生成它，请先运行：`news_recommendation_system/5.feature_engineering.ipynb`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rank_path = PROJECT_PATH / 'rank_train.pkl'\n",
        "if not rank_path.exists():\n",
        "    raise FileNotFoundError(f'Missing {rank_path}. Run 5.feature_engineering.ipynb first.')\n",
        "\n",
        "rank_df = pd.read_pickle(rank_path)\n",
        "\n",
        "# 重新加载 train_hist（不要复用上面为了 SASRec 而采样过的 train_hist/user_hist）\n",
        "train_hist_full = pd.read_pickle(PROJECT_PATH / 'train_hist.pkl').sort_values(['user_id', 'click_timestamp'])\n",
        "user_hist_full: Dict[int, List[int]] = train_hist_full.groupby('user_id')['click_article_id'].apply(list).to_dict()\n",
        "\n",
        "# 按 user 划分训练/验证\n",
        "rng = np.random.default_rng(42)\n",
        "users = rank_df['user_id'].unique()\n",
        "rng.shuffle(users)\n",
        "split = int(len(users) * 0.8)\n",
        "train_users = set(users[:split])\n",
        "\n",
        "train_r = rank_df[rank_df['user_id'].isin(train_users)].copy()\n",
        "valid_r = rank_df[~rank_df['user_id'].isin(train_users)].copy()\n",
        "\n",
        "# 可选限制规模\n",
        "MAX_TRAIN_ROWS = 300000\n",
        "if DEBUG and len(train_r) > MAX_TRAIN_ROWS:\n",
        "    train_r = train_r.sample(MAX_TRAIN_ROWS, random_state=42)\n",
        "\n",
        "train_r[['label']].value_counts(), valid_r[['label']].value_counts(), len(train_r), len(valid_r)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 选一些 dense 特征（与 DIN 类似）\n",
        "DENSE_COLS = [\n",
        "    'recall_score', 'recall_rank',\n",
        "    'user_click_count', 'user_unique_items',\n",
        "    'item_click_count', 'words_count',\n",
        "    'item_age_hours', 'time_gap_hours',\n",
        "    'emb_sim_last', 'is_same_category',\n",
        "]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_dense = scaler.fit_transform(train_r[DENSE_COLS].fillna(0).values.astype('float32'))\n",
        "X_valid_dense = scaler.transform(valid_r[DENSE_COLS].fillna(0).values.astype('float32'))\n",
        "\n",
        "y_train = train_r['label'].values.astype('float32')\n",
        "y_valid = valid_r['label'].values.astype('float32')\n",
        "\n",
        "# user/item 编码（用全量 vocab，避免历史 unknown）\n",
        "all_users = pd.unique(pd.concat([train_r['user_id'], valid_r['user_id']]).astype(int))\n",
        "raw_to_user_enc = {int(v): int(i + 1) for i, v in enumerate(np.sort(all_users))}\n",
        "user_vocab_size = int(len(raw_to_user_enc) + 1)\n",
        "\n",
        "train_user = np.asarray([raw_to_user_enc.get(int(x), 0) for x in train_r['user_id'].values], dtype=np.int32)\n",
        "valid_user = np.asarray([raw_to_user_enc.get(int(x), 0) for x in valid_r['user_id'].values], dtype=np.int32)\n",
        "\n",
        "train_item = item_id_map.transform(train_r['article_id'].astype(int).values)\n",
        "valid_item = item_id_map.transform(valid_r['article_id'].astype(int).values)\n",
        "\n",
        "# 历史序列（来自 train_hist）\n",
        "MAX_HIST_LEN = 30\n",
        "hist_map = {int(u): pad_left([item_id_map.transform(np.asarray(seq[-MAX_HIST_LEN:], dtype=np.int64)).tolist()], max_len=MAX_HIST_LEN)[0] for u, seq in user_hist_full.items()}\n",
        "\n",
        "train_hist_mat = np.vstack([hist_map.get(int(u), np.zeros(MAX_HIST_LEN, dtype=np.int32)) for u in train_r['user_id'].values])\n",
        "valid_hist_mat = np.vstack([hist_map.get(int(u), np.zeros(MAX_HIST_LEN, dtype=np.int32)) for u in valid_r['user_id'].values])\n",
        "\n",
        "train_hist_mat.shape, train_item.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_units: List[int] = [80, 40], **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mlp = [tf.keras.layers.Dense(u, activation='relu') for u in hidden_units]\n",
        "        self.out = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, keys, mask = inputs  # query: [B, D], keys: [B, L, H]\n",
        "        q = tf.expand_dims(query, axis=1)\n",
        "        q = tf.tile(q, [1, tf.shape(keys)[1], 1])\n",
        "        x = tf.concat([q, keys, q - keys, q * keys], axis=-1)\n",
        "        for dense in self.mlp:\n",
        "            x = dense(x)\n",
        "        scores = tf.squeeze(self.out(x), axis=-1)\n",
        "        paddings = tf.ones_like(scores) * (-1e9)\n",
        "        scores = tf.where(mask > 0, scores, paddings)\n",
        "        weights = tf.nn.softmax(scores, axis=-1)\n",
        "        return weights  # [B, L]\n",
        "\n",
        "\n",
        "def build_dien_like(user_vocab_size: int, item_vocab_size: int, hist_len: int, dense_dim: int, emb_dim: int = 16, gru_units: int = 32):\n",
        "    user_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='user_id')\n",
        "    hist_inp = tf.keras.layers.Input(shape=(hist_len,), dtype=tf.int32, name='hist_items')\n",
        "    item_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='target_item')\n",
        "    dense_inp = tf.keras.layers.Input(shape=(dense_dim,), dtype=tf.float32, name='dense')\n",
        "\n",
        "    user_emb = tf.keras.layers.Flatten()(tf.keras.layers.Embedding(user_vocab_size, emb_dim)(user_inp))\n",
        "    item_emb_layer = tf.keras.layers.Embedding(item_vocab_size, emb_dim, mask_zero=True)\n",
        "    target_emb = tf.keras.layers.Flatten()(item_emb_layer(item_inp))\n",
        "    hist_emb = item_emb_layer(hist_inp)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(hist_inp, 0), tf.int32)\n",
        "\n",
        "    # Interest Extractor: GRU hidden states\n",
        "    hs = tf.keras.layers.GRU(gru_units, return_sequences=True)(hist_emb)\n",
        "\n",
        "    # Attention weights conditioned on target\n",
        "    att = SimpleAttention()([target_emb, hs, mask])  # [B, L]\n",
        "    att = tf.expand_dims(att, axis=-1)  # [B, L, 1]\n",
        "\n",
        "    # AIGRU: attention * hidden states -> GRU\n",
        "    aigru_inp = hs * att\n",
        "    evolved = tf.keras.layers.GRU(gru_units, return_sequences=False)(aigru_inp)\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()([user_emb, target_emb, evolved, dense_inp])\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    out = tf.keras.layers.Flatten()(out)\n",
        "\n",
        "    model = tf.keras.Model(inputs={'user_id': user_inp, 'hist_items': hist_inp, 'target_item': item_inp, 'dense': dense_inp}, outputs=out, name='DIEN_like')\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "\n",
        "dien = build_dien_like(user_vocab_size=user_vocab_size, item_vocab_size=item_id_map.vocab_size, hist_len=MAX_HIST_LEN, dense_dim=X_train_dense.shape[1])\n",
        "dien.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_inputs = {'user_id': train_user, 'hist_items': train_hist_mat, 'target_item': train_item, 'dense': X_train_dense}\n",
        "valid_inputs = {'user_id': valid_user, 'hist_items': valid_hist_mat, 'target_item': valid_item, 'dense': X_valid_dense}\n",
        "\n",
        "dien.fit(train_inputs, y_train, batch_size=2048, epochs=2, validation_data=(valid_inputs, y_valid), verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hit_ndcg_at_k(df: pd.DataFrame, preds: np.ndarray, k: int = 5) -> Dict[str, float]:\n",
        "    tmp = df[['user_id', 'label']].copy()\n",
        "    tmp['pred'] = preds\n",
        "    hit = 0\n",
        "    ndcg = 0.0\n",
        "    total = 0\n",
        "    for _, g in tmp.groupby('user_id'):\n",
        "        g = g.sort_values('pred', ascending=False).head(k)\n",
        "        if g['label'].max() > 0:\n",
        "            hit += 1\n",
        "            rank = int(g.reset_index(drop=True).index[g['label'] == 1][0])\n",
        "            ndcg += 1.0 / math.log2(rank + 2)\n",
        "        total += 1\n",
        "    return {f'hit_rate@{k}': hit / max(1, total), f'ndcg@{k}': ndcg / max(1, total), 'num_users': total}\n",
        "\n",
        "\n",
        "pred_valid = dien.predict(valid_inputs, batch_size=4096, verbose=0)\n",
        "hit_ndcg_at_k(valid_r, pred_valid, k=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DIEN_DIR = PROJECT_PATH / 'artifacts' / 'ranking' / 'dien_like'\n",
        "DIEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "dien.save(DIEN_DIR / 'dien_like.keras')\n",
        "with open(DIEN_DIR / 'scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "with open(DIEN_DIR / 'encoders.pkl', 'wb') as f:\n",
        "    pickle.dump({'raw_to_user_enc': raw_to_user_enc}, f)\n",
        "\n",
        "print('saved to:', DIEN_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
