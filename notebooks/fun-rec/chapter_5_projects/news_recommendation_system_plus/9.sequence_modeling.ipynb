{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. 序列建模（SASRec / DIEN）\n",
        "\n",
        "推荐系统里“序列”几乎是标配：用户兴趣会随时间变化，单纯的静态特征不够。\n",
        "\n",
        "本 notebook 目标：\n",
        "\n",
        "1) 实现一个可训练的 **SASRec（Transformer）召回模型**，并用 FAISS 做 TopK 检索评估\n",
        "\n",
        "2) 实现一个 **DIEN-like（GRU + 注意力）排序模型**，展示“兴趣抽取/演化”的思路\n",
        "\n",
        "## 面试要点\n",
        "\n",
        "- 序列推荐 vs 静态推荐：兴趣演化、短期意图\n",
        "- Transformer 优势：并行、长依赖（但要注意计算/存储）\n",
        "- 训练目标：next-item / contrastive / sampled softmax\n",
        "- 线上：用户向量实时更新 + ANN 检索\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "51470a5f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('/Users/wangjunfei/Desktop/fun-rec/data/dataset/news_recommendation'),\n",
              " PosixPath('/Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system'),\n",
              " PosixPath('/Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system/artifacts/sequence/sasrec_inbatch'))"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import pickle\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import faiss\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 兼容 Keras 3：尽量用 keras.ops（可处理 KerasTensor）；若没有 keras 包则回退到一个 tf 的轻量包装。\n",
        "try:\n",
        "    import keras  # type: ignore\n",
        "    ops = keras.ops\n",
        "except Exception:\n",
        "    class _Ops:\n",
        "        expand_dims = staticmethod(tf.expand_dims)\n",
        "        zeros_like = staticmethod(tf.zeros_like)\n",
        "        squeeze = staticmethod(tf.squeeze)\n",
        "        ones_like = staticmethod(tf.ones_like)\n",
        "        where = staticmethod(tf.where)\n",
        "        softmax = staticmethod(tf.nn.softmax)\n",
        "        sum = staticmethod(tf.reduce_sum)\n",
        "\n",
        "        @staticmethod\n",
        "        def concatenate(xs, axis=-1):\n",
        "            return tf.concat(xs, axis=axis)\n",
        "\n",
        "    ops = _Ops()\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Mac 上 faiss/tensorflow 偶发 native crash（kernel 直接挂掉）时，先把 faiss 线程数降到 1 往往更稳。\n",
        "try:\n",
        "    faiss.omp_set_num_threads(1)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    cur = start\n",
        "    for _ in range(10):\n",
        "        if (cur / 'pyproject.toml').exists() or (cur / '.git').exists():\n",
        "            return cur\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "dotenv_path = find_dotenv(usecwd=True)\n",
        "if dotenv_path:\n",
        "    load_dotenv(dotenv_path)\n",
        "os.environ.setdefault('FUNREC_RAW_DATA_PATH', str(REPO_ROOT / 'data'))\n",
        "os.environ.setdefault('FUNREC_PROCESSED_DATA_PATH', str(REPO_ROOT / 'tmp'))\n",
        "\n",
        "RAW_DATA_PATH = Path(os.getenv('FUNREC_RAW_DATA_PATH'))\n",
        "PROCESSED_DATA_PATH = Path(os.getenv('FUNREC_PROCESSED_DATA_PATH'))\n",
        "\n",
        "DATA_PATH = RAW_DATA_PATH / 'dataset' / 'news_recommendation'\n",
        "if not DATA_PATH.exists():\n",
        "    DATA_PATH = RAW_DATA_PATH / 'news_recommendation'\n",
        "\n",
        "PROJECT_PATH = PROCESSED_DATA_PATH / 'projects' / 'news_recommendation_system'\n",
        "SASREC_DIR = PROJECT_PATH / 'artifacts' / 'sequence' / 'sasrec_inbatch'\n",
        "SASREC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_PATH, PROJECT_PATH, SASREC_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81c8c91",
      "metadata": {},
      "source": [
        "## Part A：SASRec（召回）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fcfc9d8e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20000, 92646)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_hist = pd.read_pickle(PROJECT_PATH / 'train_hist.pkl')\n",
        "valid_last = pd.read_pickle(PROJECT_PATH / 'valid_last.pkl')\n",
        "articles = pd.read_csv(DATA_PATH / 'articles.csv')\n",
        "\n",
        "DEBUG = True\n",
        "MAX_USERS = 20000\n",
        "SEED = 42\n",
        "\n",
        "if DEBUG:\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    users = train_hist['user_id'].unique()\n",
        "    if len(users) > MAX_USERS:\n",
        "        sample_users = rng.choice(users, size=MAX_USERS, replace=False)\n",
        "        train_hist = train_hist[train_hist['user_id'].isin(sample_users)]\n",
        "        valid_last = valid_last[valid_last['user_id'].isin(sample_users)]\n",
        "\n",
        "train_hist = train_hist.sort_values(['user_id', 'click_timestamp'])\n",
        "user_hist: Dict[int, List[int]] = train_hist.groupby('user_id')['click_article_id'].apply(list).to_dict()\n",
        "\n",
        "len(user_hist), len(train_hist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f8d39a18",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "364048"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@dataclass(frozen=True)\n",
        "class IdMap:\n",
        "    name: str\n",
        "    classes_: np.ndarray\n",
        "    offset: int = 1\n",
        "    unknown_value: int = 0\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return int(len(self.classes_) + self.offset)\n",
        "\n",
        "    def transform(self, values) -> np.ndarray:\n",
        "        index = pd.Index(self.classes_)\n",
        "        arr = np.asarray(values)\n",
        "        flat = arr.reshape(-1)\n",
        "        idx = index.get_indexer(flat)\n",
        "        out = idx.astype(np.int64) + self.offset\n",
        "        out[idx < 0] = self.unknown_value\n",
        "        return out.reshape(arr.shape).astype(np.int32)\n",
        "\n",
        "    @classmethod\n",
        "    def fit(cls, name: str, values, offset: int = 1) -> 'IdMap':\n",
        "        uniq = pd.unique(pd.Series(list(values)))\n",
        "        try:\n",
        "            uniq = np.array(sorted(uniq))\n",
        "        except Exception:\n",
        "            uniq = np.array(list(uniq))\n",
        "        return cls(name=name, classes_=uniq, offset=offset)\n",
        "\n",
        "\n",
        "def pad_left(seqs: List[List[int]], max_len: int, pad_value: int = 0) -> np.ndarray:\n",
        "    out = np.full((len(seqs), max_len), pad_value, dtype=np.int32)\n",
        "    for i, seq in enumerate(seqs):\n",
        "        if not seq:\n",
        "            continue\n",
        "        seq = seq[-max_len:]\n",
        "        out[i, -len(seq):] = np.asarray(seq, dtype=np.int32)\n",
        "    return out\n",
        "\n",
        "\n",
        "item_id_map = IdMap.fit('article_id', articles['article_id'].astype(int).unique(), offset=1)\n",
        "item_id_map.vocab_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5387392f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "build_seq_samples: 100%|██████████| 20000/20000 [00:00<00:00, 336282.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_samples: 65809\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((65809, 50), (65809,))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==================== 构造训练样本（next-item） ====================\n",
        "MAX_SEQ_LEN = 50\n",
        "MAX_SAMPLES_PER_USER = 20 if DEBUG else 200\n",
        "\n",
        "seq_samples: List[List[int]] = []\n",
        "pos_items: List[int] = []\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "for u, seq in tqdm(user_hist.items(), desc='build_seq_samples'):\n",
        "    if len(seq) < 2:\n",
        "        continue\n",
        "    positions = list(range(1, len(seq)))\n",
        "    if len(positions) > MAX_SAMPLES_PER_USER:\n",
        "        tail = positions[-MAX_SAMPLES_PER_USER * 3 :]\n",
        "        positions = rng.choice(tail, size=MAX_SAMPLES_PER_USER, replace=False).tolist()\n",
        "        positions.sort()\n",
        "\n",
        "    for t in positions:\n",
        "        hist = seq[max(0, t - MAX_SEQ_LEN) : t]\n",
        "        target = int(seq[t])\n",
        "        seq_samples.append([int(x) for x in hist])\n",
        "        pos_items.append(target)\n",
        "\n",
        "X_seq = pad_left([item_id_map.transform(np.asarray(s, dtype=np.int64)).tolist() for s in seq_samples], max_len=MAX_SEQ_LEN)\n",
        "X_pos = item_id_map.transform(np.asarray(pos_items, dtype=np.int64))\n",
        "\n",
        "print('num_samples:', len(X_pos))\n",
        "X_seq.shape, X_pos.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "301b0ee7",
      "metadata": {},
      "source": [
        "### SASRec（Transformer）实现（in-batch negatives）\n",
        "\n",
        "为了让 notebook 在大 vocab（36w item）下也能跑，这里不做全量 softmax，而是用 **in-batch negatives**：\n",
        "\n",
        "- user 表示来自 Transformer 对序列的编码\n",
        "- item 表示来自 item embedding\n",
        "- batch 内做对比学习（InfoNCE）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b9574f86",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"SASRecInBatch\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " seq_ids (InputLayer)        [(None, 50)]                 0         []                            \n",
            "                                                                                                  \n",
            " item_emb (Embedding)        multiple                     2329907   ['seq_ids[0][0]',             \n",
            "                                                          2          'pos_item[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 50, 64)               0         ['item_emb[0][0]']            \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 50, 64)               0         ['tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 50, 64)               128       ['dropout[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " mha_0 (MultiHeadAttention)  (None, 50, 64)               16640     ['layer_normalization[0][0]', \n",
            "                                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TF  (None, 50, 64)               0         ['dropout[0][0]',             \n",
            " OpLambda)                                                           'mha_0[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 50, 64)               128       ['tf.__operators__.add_1[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " ffn_0 (Sequential)          (None, 50, 64)               16576     ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TF  (None, 50, 64)               0         ['tf.__operators__.add_1[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'ffn_0[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 50, 64)               128       ['tf.__operators__.add_2[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " mha_1 (MultiHeadAttention)  (None, 50, 64)               16640     ['layer_normalization_2[0][0]'\n",
            "                                                                    , 'layer_normalization_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TF  (None, 50, 64)               0         ['tf.__operators__.add_2[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'mha_1[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 50, 64)               128       ['tf.__operators__.add_3[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " ffn_1 (Sequential)          (None, 50, 64)               16576     ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TF  (None, 50, 64)               0         ['tf.__operators__.add_3[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'ffn_1[0][0]']               \n",
            "                                                                                                  \n",
            " pos_item (InputLayer)       [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 50, 64)               128       ['tf.__operators__.add_4[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " last_state (Lambda)         (None, 64)                   0         ['layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 64)                   0         ['item_emb[1][0]']            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 64)                   4160      ['last_state[0][0]']          \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 64)                   4160      ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " user_vec (Lambda)           (None, 64)                   0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " item_vec (Lambda)           (None, 64)                   0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " logits (Lambda)             (None, None)                 0         ['user_vec[0][0]',            \n",
            "                                                                     'item_vec[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23374464 (89.17 MB)\n",
            "Trainable params: 23374464 (89.17 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_sasrec_inbatch(\n",
        "    item_vocab_size: int,\n",
        "    max_seq_len: int = 50,\n",
        "    emb_dim: int = 64,\n",
        "    num_heads: int = 2,\n",
        "    num_blocks: int = 2,\n",
        "    ff_dim: int = 128,\n",
        "    dropout: float = 0.2,\n",
        "    temperature: float = 0.05,\n",
        "):\n",
        "    if emb_dim % num_heads != 0:\n",
        "        raise ValueError(f'emb_dim ({emb_dim}) must be divisible by num_heads ({num_heads})')\n",
        "\n",
        "    seq_inp = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32, name='seq_ids')\n",
        "    pos_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='pos_item')\n",
        "\n",
        "    item_emb = tf.keras.layers.Embedding(item_vocab_size, emb_dim, mask_zero=True, name='item_emb')\n",
        "    pos_emb = tf.keras.layers.Embedding(max_seq_len, emb_dim, name='pos_emb')\n",
        "\n",
        "    x = item_emb(seq_inp)  # [B, L, D]\n",
        "    positions = tf.range(max_seq_len)\n",
        "    x = x + pos_emb(positions)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "        attn_in = tf.keras.layers.LayerNormalization()(x)\n",
        "        attn_out = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=emb_dim // num_heads,\n",
        "            dropout=dropout,\n",
        "            name=f'mha_{i}',\n",
        "        )(attn_in, attn_in, use_causal_mask=True)\n",
        "        x = x + attn_out\n",
        "\n",
        "        ffn_in = tf.keras.layers.LayerNormalization()(x)\n",
        "        ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dropout(dropout),\n",
        "            tf.keras.layers.Dense(emb_dim),\n",
        "        ], name=f'ffn_{i}')(ffn_in)\n",
        "        x = x + ffn\n",
        "\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    user_vec = tf.keras.layers.Lambda(lambda t: t[:, -1, :], name='last_state')(x)\n",
        "    user_vec = tf.keras.layers.Dense(emb_dim, activation=None)(user_vec)\n",
        "    user_vec = tf.keras.layers.Lambda(lambda t: tf.nn.l2_normalize(t, axis=1), name='user_vec')(user_vec)\n",
        "\n",
        "    item_vec = tf.keras.layers.Flatten()(item_emb(pos_inp))\n",
        "    item_vec = tf.keras.layers.Dense(emb_dim, activation=None)(item_vec)\n",
        "    item_vec = tf.keras.layers.Lambda(lambda t: tf.nn.l2_normalize(t, axis=1), name='item_vec')(item_vec)\n",
        "\n",
        "    logits = tf.keras.layers.Lambda(lambda z: tf.matmul(z[0], z[1], transpose_b=True) / temperature, name='logits')([user_vec, item_vec])\n",
        "\n",
        "    model = tf.keras.Model(inputs={'seq_ids': seq_inp, 'pos_item': pos_inp}, outputs=logits, name='SASRecInBatch')\n",
        "    user_model = tf.keras.Model(inputs={'seq_ids': seq_inp}, outputs=user_vec, name='sasrec_user')\n",
        "    item_model = tf.keras.Model(inputs={'pos_item': pos_inp}, outputs=item_vec, name='sasrec_item')\n",
        "    return model, user_model, item_model\n",
        "\n",
        "\n",
        "def inbatch_symmetric_loss(y_true, logits):\n",
        "    b = tf.shape(logits)[0]\n",
        "    labels = tf.range(b)\n",
        "    loss_u2i = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "    loss_i2u = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=tf.transpose(logits))\n",
        "    return tf.reduce_mean(loss_u2i + loss_i2u) / 2.0\n",
        "\n",
        "\n",
        "model, sasrec_user, sasrec_item = build_sasrec_inbatch(\n",
        "    item_vocab_size=item_id_map.vocab_size,\n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        "    emb_dim=64,\n",
        "    num_heads=2,\n",
        "    num_blocks=2,\n",
        "    ff_dim=128,\n",
        "    dropout=0.2,\n",
        "    temperature=0.05,\n",
        ")\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(2e-4), loss=inbatch_symmetric_loss)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3207c7d3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "65/65 [==============================] - 21s 314ms/step - loss: 7.3925\n",
            "Epoch 2/3\n",
            "65/65 [==============================] - 20s 312ms/step - loss: 6.9820\n",
            "Epoch 3/3\n",
            "65/65 [==============================] - 20s 314ms/step - loss: 6.9478\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x168fd92a0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BATCH_SIZE = 1024\n",
        "EPOCHS = 3\n",
        "\n",
        "train_X = {'seq_ids': X_seq, 'pos_item': X_pos}\n",
        "dummy_y = np.zeros(len(X_pos), dtype=np.float32)\n",
        "\n",
        "model.fit(train_X, dummy_y, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5271afe6",
      "metadata": {},
      "source": [
        "### FAISS 建索引与 Recall 评估\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1cd19886",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "indexed items: 364047\n"
          ]
        }
      ],
      "source": [
        "# 全量 item embedding（用 raw article_id 作为索引 id）\n",
        "all_items_raw = articles['article_id'].astype(int).unique()\n",
        "all_items_enc = item_id_map.transform(all_items_raw)\n",
        "\n",
        "item_embs = sasrec_item.predict({'pos_item': all_items_enc}, batch_size=4096, verbose=0).astype('float32')\n",
        "faiss.normalize_L2(item_embs)\n",
        "\n",
        "index = faiss.IndexIDMap2(faiss.IndexFlatIP(item_embs.shape[1]))\n",
        "index.add_with_ids(item_embs, all_items_raw.astype('int64'))\n",
        "\n",
        "faiss.write_index(index, str(SASREC_DIR / 'faiss_index.bin'))\n",
        "np.save(SASREC_DIR / 'item_embeddings.npy', item_embs)\n",
        "\n",
        "print('indexed items:', index.ntotal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6cbc2be2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hit_rate@20': 0.0, 'ndcg@20': 0.0, 'num_users': 20000}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def evaluate_recall(user_hist: Dict[int, List[int]], valid_last: pd.DataFrame, topk: int = 20) -> Dict[str, float]:\n",
        "    users = valid_last['user_id'].astype(int).tolist()\n",
        "    targets = valid_last['click_article_id'].astype(int).tolist()\n",
        "\n",
        "    X_u = []\n",
        "    X_seq_raw = []\n",
        "    y = []\n",
        "    for u, t in zip(users, targets):\n",
        "        seq = user_hist.get(u)\n",
        "        if not seq:\n",
        "            continue\n",
        "        X_u.append(u)\n",
        "        X_seq_raw.append(seq[-MAX_SEQ_LEN:])\n",
        "        y.append(t)\n",
        "\n",
        "    X_seq_enc = pad_left([item_id_map.transform(np.asarray(s, dtype=np.int64)).tolist() for s in X_seq_raw], max_len=MAX_SEQ_LEN)\n",
        "    user_embs = sasrec_user.predict({'seq_ids': X_seq_enc}, batch_size=4096, verbose=0).astype('float32')\n",
        "    faiss.normalize_L2(user_embs)\n",
        "\n",
        "    search_k = topk + MAX_SEQ_LEN + 10\n",
        "    D, I = index.search(user_embs, search_k)\n",
        "\n",
        "    hit = 0\n",
        "    ndcg = 0.0\n",
        "    total = 0\n",
        "    for i in range(len(X_u)):\n",
        "        hist_set = set(user_hist.get(X_u[i], []))\n",
        "        recs = []\n",
        "        for item_id in I[i].tolist():\n",
        "            item_id = int(item_id)\n",
        "            if item_id < 0:\n",
        "                continue\n",
        "            if item_id in hist_set:\n",
        "                continue\n",
        "            recs.append(item_id)\n",
        "            if len(recs) >= topk:\n",
        "                break\n",
        "        target = int(y[i])\n",
        "        if target in recs:\n",
        "            hit += 1\n",
        "            rank = recs.index(target)\n",
        "            ndcg += 1.0 / np.log2(rank + 2)\n",
        "        total += 1\n",
        "\n",
        "    return {f'hit_rate@{topk}': hit / max(1, total), f'ndcg@{topk}': ndcg / max(1, total), 'num_users': total}\n",
        "\n",
        "\n",
        "metrics = evaluate_recall(user_hist, valid_last, topk=20)\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "67ff3267",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "recall_all_users: 100%|██████████| 5/5 [01:18<00:00, 15.66s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(PosixPath('/Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system/recall_candidates_sasrec.pkl'),\n",
              "    user_id  article_id  recall_score  recall_rank\n",
              " 0        0      209981      0.079361            1\n",
              " 1        0       79492      0.075875            2\n",
              " 2        0      181848      0.075579            3\n",
              " 3        0      252772      0.075436            4\n",
              " 4        0      130407      0.073480            5)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 生成召回候选集（给排序用）\n",
        "TOPK_CANDIDATES = 100\n",
        "SEARCH_K = TOPK_CANDIDATES + MAX_SEQ_LEN + 10\n",
        "\n",
        "rows = []\n",
        "users_all = list(user_hist.keys())\n",
        "batch_size = 4096\n",
        "for start in tqdm(range(0, len(users_all), batch_size), desc='recall_all_users'):\n",
        "    end = min(start + batch_size, len(users_all))\n",
        "    u_raw_batch = users_all[start:end]\n",
        "    seq_raw_batch = [user_hist[u][-MAX_SEQ_LEN:] for u in u_raw_batch]\n",
        "    X_seq_enc = pad_left([item_id_map.transform(np.asarray(s, dtype=np.int64)).tolist() for s in seq_raw_batch], max_len=MAX_SEQ_LEN)\n",
        "    user_embs = sasrec_user.predict({'seq_ids': X_seq_enc}, batch_size=4096, verbose=0).astype('float32')\n",
        "    faiss.normalize_L2(user_embs)\n",
        "    D, I = index.search(user_embs, SEARCH_K)\n",
        "\n",
        "    for local_i, u in enumerate(u_raw_batch):\n",
        "        hist_set = set(user_hist.get(u, []))\n",
        "        rank = 0\n",
        "        for item_id, score in zip(I[local_i].tolist(), D[local_i].tolist()):\n",
        "            item_id = int(item_id)\n",
        "            if item_id < 0:\n",
        "                continue\n",
        "            if item_id in hist_set:\n",
        "                continue\n",
        "            rank += 1\n",
        "            rows.append((int(u), int(item_id), float(score), int(rank)))\n",
        "            if rank >= TOPK_CANDIDATES:\n",
        "                break\n",
        "\n",
        "sasrec_recall_df = pd.DataFrame(rows, columns=['user_id', 'article_id', 'recall_score', 'recall_rank'])\n",
        "out_path = PROJECT_PATH / 'recall_candidates_sasrec.pkl'\n",
        "sasrec_recall_df.to_pickle(out_path)\n",
        "\n",
        "out_path, sasrec_recall_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "03575e0b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved to: /Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system/artifacts/sequence/sasrec_inbatch\n"
          ]
        }
      ],
      "source": [
        "# 保存模型与指标\n",
        "model.save(SASREC_DIR / 'sasrec_inbatch.keras')\n",
        "sasrec_user.save(SASREC_DIR / 'user_tower.keras')\n",
        "sasrec_item.save(SASREC_DIR / 'item_tower.keras')\n",
        "\n",
        "with open(SASREC_DIR / 'item_id_map.pkl', 'wb') as f:\n",
        "    pickle.dump(item_id_map, f)\n",
        "with open(SASREC_DIR / 'metrics.pkl', 'wb') as f:\n",
        "    pickle.dump(metrics, f)\n",
        "\n",
        "print('saved to:', SASREC_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65212258",
      "metadata": {},
      "source": [
        "## Part B：DIEN-like（排序）\n",
        "\n",
        "DIEN 的核心：\n",
        "\n",
        "- 用 GRU 从行为序列抽取兴趣状态（Interest Extractor）\n",
        "- 用注意力让兴趣与当前候选 item 对齐\n",
        "- 再做一层“兴趣演化”（这里实现 AIGRU：注意力加权后再 GRU）\n",
        "\n",
        "为了避免重复构造训练集，这里直接复用基础版 5.feature_engineering 的输出：`rank_train.pkl`。\n",
        "\n",
        "如果你还没生成它，请先运行：`news_recommendation_system/5.feature_engineering.ipynb`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "145d705f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(label\n",
              " 0        297647\n",
              " 1          2353\n",
              " Name: count, dtype: int64,\n",
              " label\n",
              " 0        198443\n",
              " 1          1557\n",
              " Name: count, dtype: int64,\n",
              " 300000,\n",
              " 200000)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rank_path = PROJECT_PATH / 'rank_train.pkl'\n",
        "if not rank_path.exists():\n",
        "    raise FileNotFoundError(f'Missing {rank_path}. Run 5.feature_engineering.ipynb first.')\n",
        "\n",
        "rank_df = pd.read_pickle(rank_path)\n",
        "\n",
        "# 重新加载 train_hist（不要复用上面为了 SASRec 而采样过的 train_hist/user_hist）\n",
        "train_hist_full = pd.read_pickle(PROJECT_PATH / 'train_hist.pkl').sort_values(['user_id', 'click_timestamp'])\n",
        "user_hist_full: Dict[int, List[int]] = train_hist_full.groupby('user_id')['click_article_id'].apply(list).to_dict()\n",
        "\n",
        "# 按 user 划分训练/验证\n",
        "rng = np.random.default_rng(42)\n",
        "users = rank_df['user_id'].unique()\n",
        "rng.shuffle(users)\n",
        "split = int(len(users) * 0.8)\n",
        "train_users = set(users[:split])\n",
        "\n",
        "train_r = rank_df[rank_df['user_id'].isin(train_users)].copy()\n",
        "valid_r = rank_df[~rank_df['user_id'].isin(train_users)].copy()\n",
        "\n",
        "# 可选限制规模\n",
        "MAX_TRAIN_ROWS = 300000\n",
        "if DEBUG and len(train_r) > MAX_TRAIN_ROWS:\n",
        "    train_r = train_r.sample(MAX_TRAIN_ROWS, random_state=42)\n",
        "\n",
        "train_r[['label']].value_counts(), valid_r[['label']].value_counts(), len(train_r), len(valid_r)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6eeb4f72",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((300000, 30), (300000,))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 选一些 dense 特征（与 DIN 类似）\n",
        "DENSE_COLS = [\n",
        "    'recall_score', 'recall_rank',\n",
        "    'user_click_count', 'user_unique_items',\n",
        "    'item_click_count', 'words_count',\n",
        "    'item_age_hours', 'time_gap_hours',\n",
        "    'emb_sim_last', 'is_same_category',\n",
        "]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_dense = scaler.fit_transform(train_r[DENSE_COLS].fillna(0).values.astype('float32'))\n",
        "X_valid_dense = scaler.transform(valid_r[DENSE_COLS].fillna(0).values.astype('float32'))\n",
        "\n",
        "y_train = train_r['label'].values.astype('float32')\n",
        "y_valid = valid_r['label'].values.astype('float32')\n",
        "\n",
        "# user/item 编码（用全量 vocab，避免历史 unknown）\n",
        "all_users = pd.unique(pd.concat([train_r['user_id'], valid_r['user_id']]).astype(int))\n",
        "raw_to_user_enc = {int(v): int(i + 1) for i, v in enumerate(np.sort(all_users))}\n",
        "user_vocab_size = int(len(raw_to_user_enc) + 1)\n",
        "\n",
        "train_user = np.asarray([raw_to_user_enc.get(int(x), 0) for x in train_r['user_id'].values], dtype=np.int32)\n",
        "valid_user = np.asarray([raw_to_user_enc.get(int(x), 0) for x in valid_r['user_id'].values], dtype=np.int32)\n",
        "\n",
        "train_item = item_id_map.transform(train_r['article_id'].astype(int).values)\n",
        "valid_item = item_id_map.transform(valid_r['article_id'].astype(int).values)\n",
        "\n",
        "# 历史序列（来自 train_hist）\n",
        "MAX_HIST_LEN = 30\n",
        "hist_map = {int(u): pad_left([item_id_map.transform(np.asarray(seq[-MAX_HIST_LEN:], dtype=np.int64)).tolist()], max_len=MAX_HIST_LEN)[0] for u, seq in user_hist_full.items()}\n",
        "\n",
        "train_hist_mat = np.vstack([hist_map.get(int(u), np.zeros(MAX_HIST_LEN, dtype=np.int32)) for u in train_r['user_id'].values])\n",
        "valid_hist_mat = np.vstack([hist_map.get(int(u), np.zeros(MAX_HIST_LEN, dtype=np.int32)) for u in valid_r['user_id'].values])\n",
        "\n",
        "train_hist_mat.shape, train_item.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "dbe83b06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"DIEN_like\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " target_item (InputLayer)    [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     multiple                     5824768   ['target_item[0][0]',         \n",
            "                                                                     'hist_items[0][0]']          \n",
            "                                                                                                  \n",
            " hist_items (InputLayer)     [(None, 30)]                 0         []                            \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)         (None, 16)                   0         ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " gru (GRU)                   (None, 30, 32)               4800      ['embedding_1[1][0]']         \n",
            "                                                                                                  \n",
            " target_proj (Dense)         (None, 32)                   544       ['flatten_2[0][0]']           \n",
            "                                                                                                  \n",
            " hist_mask (Lambda)          (None, 30)                   0         ['hist_items[0][0]']          \n",
            "                                                                                                  \n",
            " simple_attention (SimpleAt  (None, 30)                   13601     ['target_proj[0][0]',         \n",
            " tention)                                                            'gru[0][0]',                 \n",
            "                                                                     'hist_mask[0][0]']           \n",
            "                                                                                                  \n",
            " user_id (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " att_expand (Lambda)         (None, 30, 1)                0         ['simple_attention[0][0]']    \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 16)                   320016    ['user_id[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLamb  (None, 30, 32)               0         ['gru[0][0]',                 \n",
            " da)                                                                 'att_expand[0][0]']          \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 16)                   0         ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " gru_1 (GRU)                 (None, 32)                   6336      ['tf.math.multiply[0][0]']    \n",
            "                                                                                                  \n",
            " dense (InputLayer)          [(None, 10)]                 0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 74)                   0         ['flatten_1[0][0]',           \n",
            "                                                                     'flatten_2[0][0]',           \n",
            "                                                                     'gru_1[0][0]',               \n",
            "                                                                     'dense[0][0]']               \n",
            "                                                                                                  \n",
            " dien_dense_0 (Dense)        (None, 128)                  9600      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dien_dropout_0 (Dropout)    (None, 128)                  0         ['dien_dense_0[0][0]']        \n",
            "                                                                                                  \n",
            " dien_dense_1 (Dense)        (None, 64)                   8256      ['dien_dropout_0[0][0]']      \n",
            "                                                                                                  \n",
            " dien_dropout_1 (Dropout)    (None, 64)                   0         ['dien_dense_1[0][0]']        \n",
            "                                                                                                  \n",
            " dien_out (Dense)            (None, 1)                    65        ['dien_dropout_1[0][0]']      \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)         (None, 1)                    0         ['dien_out[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6187986 (23.61 MB)\n",
            "Trainable params: 6187986 (23.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "class SimpleAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_units: List[int] = [80, 40], **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mlp = [tf.keras.layers.Dense(u, activation='relu') for u in hidden_units]\n",
        "        self.out = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, keys, mask = inputs  # query: [B, D], keys: [B, L, H]\n",
        "\n",
        "        # 兼容 Keras 3：尽量使用 ops，且避免 tf.shape + tile\n",
        "        q = ops.expand_dims(query, axis=1)  # [B, 1, D]\n",
        "        q = q + ops.zeros_like(keys)  # broadcast 到 [B, L, D]\n",
        "        x = ops.concatenate([q, keys, q - keys, q * keys], axis=-1)  # [B, L, 4D]\n",
        "\n",
        "        for dense in self.mlp:\n",
        "            x = dense(x)\n",
        "        scores = ops.squeeze(self.out(x), axis=-1)  # [B, L]\n",
        "\n",
        "        paddings = ops.ones_like(scores) * (-1e9)\n",
        "        scores = ops.where(mask > 0, scores, paddings)\n",
        "        weights = ops.softmax(scores, axis=-1)\n",
        "        return weights  # [B, L]\n",
        "\n",
        "\n",
        "def build_dien_like(user_vocab_size: int, item_vocab_size: int, hist_len: int, dense_dim: int, emb_dim: int = 16, gru_units: int = 32):\n",
        "    user_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='user_id')\n",
        "    hist_inp = tf.keras.layers.Input(shape=(hist_len,), dtype=tf.int32, name='hist_items')\n",
        "    item_inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name='target_item')\n",
        "    dense_inp = tf.keras.layers.Input(shape=(dense_dim,), dtype=tf.float32, name='dense')\n",
        "\n",
        "    user_emb = tf.keras.layers.Flatten()(tf.keras.layers.Embedding(user_vocab_size, emb_dim)(user_inp))\n",
        "    item_emb_layer = tf.keras.layers.Embedding(item_vocab_size, emb_dim, mask_zero=True)\n",
        "    target_emb = tf.keras.layers.Flatten()(item_emb_layer(item_inp))\n",
        "    hist_emb = item_emb_layer(hist_inp)\n",
        "\n",
        "    # 新版 Keras 不允许直接对 KerasTensor 调用部分 tf.* 函数，这里用 Lambda 包一层更稳。\n",
        "    mask = tf.keras.layers.Lambda(lambda t: tf.cast(tf.not_equal(t, 0), tf.int32), name='hist_mask')(hist_inp)\n",
        "\n",
        "    # Interest Extractor: GRU hidden states\n",
        "    hs = tf.keras.layers.GRU(gru_units, return_sequences=True)(hist_emb)\n",
        "\n",
        "    # query(target item emb_dim) 与 keys(GRU hidden gru_units) 维度可能不一致；先把 query 投影到 gru_units。\n",
        "    target_for_att = tf.keras.layers.Dense(gru_units, activation=None, name='target_proj')(target_emb)\n",
        "\n",
        "    # Attention weights conditioned on target\n",
        "    att = SimpleAttention()([target_for_att, hs, mask])  # [B, L]\n",
        "    att = tf.keras.layers.Lambda(lambda t: tf.expand_dims(t, axis=-1), name='att_expand')(att)  # [B, L, 1]\n",
        "\n",
        "    # AIGRU: attention * hidden states -> GRU\n",
        "    aigru_inp = hs * att\n",
        "    evolved = tf.keras.layers.GRU(gru_units, return_sequences=False)(aigru_inp)\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()([user_emb, target_emb, evolved, dense_inp])\n",
        "    x = tf.keras.layers.Dense(128, activation='relu', name='dien_dense_0')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2, name='dien_dropout_0')(x)\n",
        "    x = tf.keras.layers.Dense(64, activation='relu', name='dien_dense_1')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2, name='dien_dropout_1')(x)\n",
        "    out = tf.keras.layers.Dense(1, activation='sigmoid', name='dien_out')(x)\n",
        "    out = tf.keras.layers.Flatten()(out)\n",
        "\n",
        "    model = tf.keras.Model(inputs={'user_id': user_inp, 'hist_items': hist_inp, 'target_item': item_inp, 'dense': dense_inp}, outputs=out, name='DIEN_like')\n",
        "    try:\n",
        "        optimizer = tf.keras.optimizers.legacy.Adam(1e-3)\n",
        "    except Exception:\n",
        "        optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "\n",
        "dien = build_dien_like(user_vocab_size=user_vocab_size, item_vocab_size=item_id_map.vocab_size, hist_len=MAX_HIST_LEN, dense_dim=X_train_dense.shape[1])\n",
        "dien.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "56fdd320",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "147/147 [==============================] - 25s 156ms/step - loss: 0.0952 - auc: 0.6477 - val_loss: 0.0389 - val_auc: 0.8094\n",
            "Epoch 2/2\n",
            "147/147 [==============================] - 23s 157ms/step - loss: 0.0366 - auc: 0.8502 - val_loss: 0.0369 - val_auc: 0.8481\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x39562d630>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_inputs = {'user_id': train_user, 'hist_items': train_hist_mat, 'target_item': train_item, 'dense': X_train_dense}\n",
        "valid_inputs = {'user_id': valid_user, 'hist_items': valid_hist_mat, 'target_item': valid_item, 'dense': X_valid_dense}\n",
        "\n",
        "dien.fit(train_inputs, y_train, batch_size=2048, epochs=2, validation_data=(valid_inputs, y_valid), verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "242308e6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hit_rate@5': 0.2245, 'ndcg@5': 0.15424781275328858, 'num_users': 4000}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def hit_ndcg_at_k(df: pd.DataFrame, preds: np.ndarray, k: int = 5) -> Dict[str, float]:\n",
        "    tmp = df[['user_id', 'label']].copy()\n",
        "    tmp['pred'] = preds\n",
        "    hit = 0\n",
        "    ndcg = 0.0\n",
        "    total = 0\n",
        "    for _, g in tmp.groupby('user_id'):\n",
        "        g = g.sort_values('pred', ascending=False).head(k)\n",
        "        if g['label'].max() > 0:\n",
        "            hit += 1\n",
        "            rank = int(g.reset_index(drop=True).index[g['label'] == 1][0])\n",
        "            ndcg += 1.0 / math.log2(rank + 2)\n",
        "        total += 1\n",
        "    return {f'hit_rate@{k}': hit / max(1, total), f'ndcg@{k}': ndcg / max(1, total), 'num_users': total}\n",
        "\n",
        "\n",
        "pred_valid = dien.predict(valid_inputs, batch_size=4096, verbose=0)\n",
        "hit_ndcg_at_k(valid_r, pred_valid, k=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8141c97e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved to: /Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system/artifacts/ranking/dien_like\n"
          ]
        }
      ],
      "source": [
        "DIEN_DIR = PROJECT_PATH / 'artifacts' / 'ranking' / 'dien_like'\n",
        "DIEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "dien.save(DIEN_DIR / 'dien_like.keras')\n",
        "\n",
        "# 额外导出 SavedModel（更通用，且避免 .keras 在 safe_mode 下的 Lambda 反序列化限制）\n",
        "dien_sm = DIEN_DIR / 'dien_savedmodel'\n",
        "if not dien_sm.exists():\n",
        "    dien.save(dien_sm)\n",
        "with open(DIEN_DIR / 'scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "with open(DIEN_DIR / 'encoders.pkl', 'wb') as f:\n",
        "    pickle.dump({'raw_to_user_enc': raw_to_user_enc}, f)\n",
        "\n",
        "print('saved to:', DIEN_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "funrec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
