{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11. 统一评估（Recall / Ranking / 多样性 / GAUC）\n",
        "\n",
        "这一节把前面 7-10 产出的模型与候选集做统一评估，并生成一份可保存的报告。\n",
        "\n",
        "覆盖指标：\n",
        "\n",
        "- Recall：HitRate@K、NDCG@K、MRR@K\n",
        "- Ranking：按 user 分组的 HitRate@K / NDCG@K\n",
        "- 多样性：Coverage、Category Entropy（多类别覆盖）\n",
        "- 多任务：AUC + GAUC（按 user 分组 AUC）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6bb8d1f6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('/Users/wangjunfei/Desktop/fun-rec/data/dataset/news_recommendation'),\n",
              " PosixPath('/Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system'),\n",
              " PosixPath('/Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system/artifacts/evaluation'))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    cur = start\n",
        "    for _ in range(10):\n",
        "        if (cur / 'pyproject.toml').exists() or (cur / '.git').exists():\n",
        "            return cur\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "dotenv_path = find_dotenv(usecwd=True)\n",
        "if dotenv_path:\n",
        "    load_dotenv(dotenv_path)\n",
        "os.environ.setdefault('FUNREC_RAW_DATA_PATH', str(REPO_ROOT / 'data'))\n",
        "os.environ.setdefault('FUNREC_PROCESSED_DATA_PATH', str(REPO_ROOT / 'tmp'))\n",
        "\n",
        "RAW_DATA_PATH = Path(os.getenv('FUNREC_RAW_DATA_PATH'))\n",
        "PROCESSED_DATA_PATH = Path(os.getenv('FUNREC_PROCESSED_DATA_PATH'))\n",
        "\n",
        "DATA_PATH = RAW_DATA_PATH / 'dataset' / 'news_recommendation'\n",
        "if not DATA_PATH.exists():\n",
        "    DATA_PATH = RAW_DATA_PATH / 'news_recommendation'\n",
        "\n",
        "PROJECT_PATH = PROCESSED_DATA_PATH / 'projects' / 'news_recommendation_system'\n",
        "EVAL_DIR = PROJECT_PATH / 'artifacts' / 'evaluation'\n",
        "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_PATH, PROJECT_PATH, EVAL_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ad4a6f60",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(200000, 26343)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_hist = pd.read_pickle(PROJECT_PATH / 'train_hist.pkl')\n",
        "valid_last = pd.read_pickle(PROJECT_PATH / 'valid_last.pkl')\n",
        "articles = pd.read_csv(DATA_PATH / 'articles.csv')\n",
        "\n",
        "valid_last_map = dict(zip(valid_last['user_id'].astype(int), valid_last['click_article_id'].astype(int)))\n",
        "item_to_cat = dict(zip(articles['article_id'].astype(int), articles['category_id'].astype(int)))\n",
        "\n",
        "# 物品热度（用于 novelty/流行度偏置分析）\n",
        "item_pop = train_hist['click_article_id'].value_counts().to_dict()\n",
        "total_clicks = float(len(train_hist))\n",
        "\n",
        "len(valid_last_map), len(item_pop)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1c77e7",
      "metadata": {},
      "source": [
        "## 1) Recall 评估（候选集）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ae624e40",
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_recall_df(recall_df: pd.DataFrame, valid_map: Dict[int, int], k_list: List[int] = [20, 50]) -> Dict[str, float]:\n",
        "    df = recall_df.copy()\n",
        "    if 'recall_rank' in df.columns:\n",
        "        df = df.sort_values(['user_id', 'recall_rank'])\n",
        "    else:\n",
        "        df = df.sort_values(['user_id', 'recall_score'], ascending=[True, False])\n",
        "\n",
        "    metrics = {}\n",
        "    for k in k_list:\n",
        "        hit = 0\n",
        "        ndcg = 0.0\n",
        "        mrr = 0.0\n",
        "        total = 0\n",
        "        for u, g in df.groupby('user_id'):\n",
        "            u = int(u)\n",
        "            target = valid_map.get(u)\n",
        "            if target is None:\n",
        "                continue\n",
        "            items = g['article_id'].astype(int).tolist()[:k]\n",
        "            if target in items:\n",
        "                hit += 1\n",
        "                rank = items.index(target)\n",
        "                ndcg += 1.0 / math.log2(rank + 2)\n",
        "                mrr += 1.0 / float(rank + 1)\n",
        "            total += 1\n",
        "\n",
        "        metrics[f'hit_rate@{k}'] = hit / max(1, total)\n",
        "        metrics[f'ndcg@{k}'] = ndcg / max(1, total)\n",
        "        metrics[f'mrr@{k}'] = mrr / max(1, total)\n",
        "        metrics[f'users@{k}'] = float(total)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def diversity_metrics(recall_df: pd.DataFrame, k: int = 20) -> Dict[str, float]:\n",
        "    df = recall_df.copy()\n",
        "    if 'recall_rank' in df.columns:\n",
        "        df = df.sort_values(['user_id', 'recall_rank'])\n",
        "    else:\n",
        "        df = df.sort_values(['user_id', 'recall_score'], ascending=[True, False])\n",
        "\n",
        "    # Coverage\n",
        "    topk = df.groupby('user_id').head(k)\n",
        "    unique_items = topk['article_id'].nunique()\n",
        "    total_items = articles['article_id'].nunique()\n",
        "    coverage = unique_items / max(1, total_items)\n",
        "\n",
        "    # Category entropy (per user)\n",
        "    ent_list = []\n",
        "    gini_list = []\n",
        "    pop_list = []\n",
        "    novelty_list = []\n",
        "    for u, g in topk.groupby('user_id'):\n",
        "        cats = [item_to_cat.get(int(i), -1) for i in g['article_id'].astype(int).tolist()]\n",
        "        vc = pd.Series(cats).value_counts(normalize=True)\n",
        "        p = vc.values\n",
        "        ent = float(-(p * np.log(p + 1e-12)).sum())\n",
        "        gini = float(1.0 - (p ** 2).sum())\n",
        "        ent_list.append(ent)\n",
        "        gini_list.append(gini)\n",
        "\n",
        "        pops = [item_pop.get(int(i), 0) for i in g['article_id'].astype(int).tolist()]\n",
        "        pop_list.append(float(np.mean(pops)))\n",
        "        novelty = [float(-math.log((item_pop.get(int(i), 0) + 1.0) / (total_clicks + 1.0))) for i in g['article_id'].astype(int).tolist()]\n",
        "        novelty_list.append(float(np.mean(novelty)))\n",
        "\n",
        "    return {\n",
        "        f'coverage@{k}': float(coverage),\n",
        "        f'cat_entropy@{k}': float(np.mean(ent_list)) if ent_list else 0.0,\n",
        "        f'cat_gini@{k}': float(np.mean(gini_list)) if gini_list else 0.0,\n",
        "        f'avg_popularity@{k}': float(np.mean(pop_list)) if pop_list else 0.0,\n",
        "        f'novelty@{k}': float(np.mean(novelty_list)) if novelty_list else 0.0,\n",
        "        f'users@{k}': float(len(ent_list)),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dd693755",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>hit_rate@20</th>\n",
              "      <th>ndcg@20</th>\n",
              "      <th>mrr@20</th>\n",
              "      <th>users@20</th>\n",
              "      <th>hit_rate@50</th>\n",
              "      <th>ndcg@50</th>\n",
              "      <th>mrr@50</th>\n",
              "      <th>users@50</th>\n",
              "      <th>coverage@20</th>\n",
              "      <th>cat_entropy@20</th>\n",
              "      <th>cat_gini@20</th>\n",
              "      <th>avg_popularity@20</th>\n",
              "      <th>novelty@20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>baseline</td>\n",
              "      <td>0.2822</td>\n",
              "      <td>0.142132</td>\n",
              "      <td>0.103172</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.39325</td>\n",
              "      <td>0.164164</td>\n",
              "      <td>0.106705</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.081234</td>\n",
              "      <td>1.930170</td>\n",
              "      <td>0.777242</td>\n",
              "      <td>2174.516187</td>\n",
              "      <td>9.737705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sasrec</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.002986</td>\n",
              "      <td>2.942505</td>\n",
              "      <td>0.946074</td>\n",
              "      <td>0.043968</td>\n",
              "      <td>13.703523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>two_tower</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.001816</td>\n",
              "      <td>2.689413</td>\n",
              "      <td>0.919200</td>\n",
              "      <td>0.161890</td>\n",
              "      <td>13.666247</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       model  hit_rate@20   ndcg@20    mrr@20  users@20  hit_rate@50  \\\n",
              "0   baseline       0.2822  0.142132  0.103172   20000.0      0.39325   \n",
              "2     sasrec       0.0000  0.000000  0.000000   20000.0      0.00000   \n",
              "1  two_tower       0.0000  0.000000  0.000000   20000.0      0.00000   \n",
              "\n",
              "    ndcg@50    mrr@50  users@50  coverage@20  cat_entropy@20  cat_gini@20  \\\n",
              "0  0.164164  0.106705   20000.0     0.081234        1.930170     0.777242   \n",
              "2  0.000000  0.000000   20000.0     0.002986        2.942505     0.946074   \n",
              "1  0.000000  0.000000   20000.0     0.001816        2.689413     0.919200   \n",
              "\n",
              "   avg_popularity@20  novelty@20  \n",
              "0        2174.516187    9.737705  \n",
              "2           0.043968   13.703523  \n",
              "1           0.161890   13.666247  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recall_files = {\n",
        "    'baseline': PROJECT_PATH / 'recall_candidates.pkl',\n",
        "    'two_tower': PROJECT_PATH / 'recall_candidates_two_tower.pkl',\n",
        "    'sasrec': PROJECT_PATH / 'recall_candidates_sasrec.pkl',\n",
        "}\n",
        "\n",
        "recall_reports = []\n",
        "for name, path in recall_files.items():\n",
        "    if not path.exists():\n",
        "        continue\n",
        "    df = pd.read_pickle(path)\n",
        "    r = {'model': name, **eval_recall_df(df, valid_last_map, k_list=[20, 50]), **diversity_metrics(df, k=20)}\n",
        "    recall_reports.append(r)\n",
        "\n",
        "recall_report = pd.DataFrame(recall_reports).sort_values('model')\n",
        "recall_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e538ff",
      "metadata": {},
      "source": [
        "## 2) Ranking 评估（DeepFM / DIN / DIEN-like）\n",
        "\n",
        "这里评估的是“同一个 user 的候选集排序质量”，因此必须按 user 分组计算 TopK 指标。\n",
        "\n",
        "说明：为了严格复现训练时的输入编码，需要保存并复用编码器；\n",
        "如果你中途改过 Notebook8/9 的编码逻辑，请重新训练并刷新 artifacts。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b6a9375e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(200000, 4000)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def hit_ndcg_at_k(df: pd.DataFrame, preds: np.ndarray, k: int = 5) -> Dict[str, float]:\n",
        "    tmp = df[['user_id', 'label']].copy()\n",
        "    tmp['pred'] = preds\n",
        "    hit = 0\n",
        "    ndcg = 0.0\n",
        "    total = 0\n",
        "    for _, g in tmp.groupby('user_id'):\n",
        "        g = g.sort_values('pred', ascending=False).head(k)\n",
        "        if g['label'].max() > 0:\n",
        "            hit += 1\n",
        "            rank = int(g.reset_index(drop=True).index[g['label'] == 1][0])\n",
        "            ndcg += 1.0 / math.log2(rank + 2)\n",
        "        total += 1\n",
        "    return {f'hit_rate@{k}': hit / max(1, total), f'ndcg@{k}': ndcg / max(1, total), 'num_users': total}\n",
        "\n",
        "\n",
        "rank_df = pd.read_pickle(PROJECT_PATH / 'rank_train.pkl')\n",
        "\n",
        "# 与基础版一致：按 user 划分训练/验证（为了评估一致性，这里固定 random seed）\n",
        "rng = np.random.default_rng(42)\n",
        "users = rank_df['user_id'].unique()\n",
        "rng.shuffle(users)\n",
        "split = int(len(users) * 0.8)\n",
        "valid_users = set(users[split:])\n",
        "valid_r = rank_df[rank_df['user_id'].isin(valid_users)].copy()\n",
        "\n",
        "len(valid_r), valid_r['user_id'].nunique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e039f1e9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'model': 'deepfm',\n",
              "  'hit_rate@5': 0.21225,\n",
              "  'ndcg@5': 0.14279763111664995,\n",
              "  'num_users': 4000}]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ------- DeepFM -------\n",
        "deepfm_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models' / 'deepfm.keras'\n",
        "deepfm_scaler_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models' / 'scaler.pkl'\n",
        "deepfm_factorizer_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models' / 'deepfm_factorizers.pkl'\n",
        "\n",
        "def safe_load_model(path: Path, *, custom_objects=None):\n",
        "    # 优先不编译加载：评估只需要 forward\n",
        "    try:\n",
        "        return tf.keras.models.load_model(path, custom_objects=custom_objects, compile=False, safe_mode=False)\n",
        "    except TypeError:\n",
        "        return tf.keras.models.load_model(path, custom_objects=custom_objects, compile=False)\n",
        "\n",
        "ranking_reports = []\n",
        "\n",
        "deepfm_sm_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models' / 'deepfm_savedmodel'\n",
        "deepfm_load_path = deepfm_sm_path if deepfm_sm_path.exists() else deepfm_path\n",
        "\n",
        "if deepfm_load_path.exists() and deepfm_scaler_path.exists() and deepfm_factorizer_path.exists():\n",
        "    try:\n",
        "        deepfm = safe_load_model(deepfm_load_path)\n",
        "    except Exception as e:\n",
        "        print('[skip] failed to load DeepFM:', type(e).__name__, e)\n",
        "        print('提示：可重新运行 8.deep_ranking.ipynb 的保存单元，生成 SavedModel 后再评估')\n",
        "        deepfm = None\n",
        "\n",
        "if deepfm is not None:\n",
        "    with open(deepfm_scaler_path, 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    with open(deepfm_factorizer_path, 'rb') as f:\n",
        "        fac = pickle.load(f)\n",
        "\n",
        "    sparse_cols = fac['sparse_cols']\n",
        "    dense_cols = fac['dense_cols']\n",
        "    uniques = fac['uniques']  # dict(col -> list[str])\n",
        "\n",
        "    def encode_with_uniques(series: pd.Series, uniq_list: List[str]) -> np.ndarray:\n",
        "        idx = {v: i + 1 for i, v in enumerate(uniq_list)}\n",
        "        return np.asarray([idx.get(str(x), 0) for x in series.values], dtype=np.int32)\n",
        "\n",
        "    inputs = {}\n",
        "    for c in sparse_cols:\n",
        "        inputs[c] = encode_with_uniques(valid_r[c].fillna(0).astype(str), uniques[c])\n",
        "    inputs['dense'] = scaler.transform(valid_r[dense_cols].fillna(0).values.astype('float32'))\n",
        "\n",
        "    preds = deepfm.predict(inputs, batch_size=4096, verbose=0)\n",
        "    m = hit_ndcg_at_k(valid_r, preds, k=5)\n",
        "    ranking_reports.append({'model': 'deepfm', **m})\n",
        "\n",
        "ranking_reports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c6109983",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'model': 'deepfm',\n",
              "  'hit_rate@5': 0.21225,\n",
              "  'ndcg@5': 0.14279763111664995,\n",
              "  'num_users': 4000},\n",
              " {'model': 'din',\n",
              "  'hit_rate@5': 0.1875,\n",
              "  'ndcg@5': 0.1259067905901265,\n",
              "  'num_users': 4000}]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ------- DIN -------\n",
        "din_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models' / 'din.keras'\n",
        "din_scaler_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models' / 'scaler.pkl'\n",
        "din_enc_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models' / 'din_encoders.pkl'\n",
        "\n",
        "# 用于加载 DIN（notebook8 中的自定义 attention）\n",
        "class DinAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_units: List[int] = [80, 40], **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_units = hidden_units\n",
        "        self.mlp = [tf.keras.layers.Dense(u, activation='relu') for u in hidden_units]\n",
        "        self.out = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, keys, mask = inputs\n",
        "        q = tf.expand_dims(query, axis=1)\n",
        "        q = q + tf.zeros_like(keys)\n",
        "        x = tf.concat([q, keys, q - keys, q * keys], axis=-1)\n",
        "        for dense in self.mlp:\n",
        "            x = dense(x)\n",
        "        scores = tf.squeeze(self.out(x), axis=-1)\n",
        "        paddings = tf.ones_like(scores) * (-1e9)\n",
        "        scores = tf.where(mask > 0, scores, paddings)\n",
        "        weights = tf.nn.softmax(scores, axis=-1)\n",
        "        weights = tf.expand_dims(weights, axis=-1)\n",
        "        return tf.reduce_sum(weights * keys, axis=1)\n",
        "\n",
        "\n",
        "din_sm_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models' / 'din_savedmodel'\n",
        "din_load_path = din_sm_path if din_sm_path.exists() else din_path\n",
        "\n",
        "din = None\n",
        "if din_load_path.exists() and din_scaler_path.exists() and din_enc_path.exists():\n",
        "    try:\n",
        "        din = safe_load_model(din_load_path, custom_objects={'DinAttention': DinAttention})\n",
        "    except Exception as e:\n",
        "        print('[skip] failed to load DIN:', type(e).__name__, e)\n",
        "        print('提示：可重新运行 8.deep_ranking.ipynb 的保存单元，生成 SavedModel 后再评估')\n",
        "\n",
        "if din is not None:\n",
        "    with open(din_scaler_path, 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    with open(din_enc_path, 'rb') as f:\n",
        "        enc = pickle.load(f)\n",
        "    raw_to_item_enc = enc['raw_to_item_enc']\n",
        "    raw_to_user_enc = enc['raw_to_user_enc']\n",
        "\n",
        "    dense_cols = ['recall_score','recall_rank','user_click_count','user_unique_items','item_click_count','words_count','item_age_hours','time_gap_hours','emb_sim_last','is_same_category']\n",
        "    X_dense = scaler.transform(valid_r[dense_cols].fillna(0).values.astype('float32'))\n",
        "\n",
        "    # 构造 hist 序列\n",
        "    train_hist_full = pd.read_pickle(PROJECT_PATH / 'train_hist.pkl').sort_values(['user_id','click_timestamp'])\n",
        "    user_hist_full = train_hist_full.groupby('user_id')['click_article_id'].apply(list).to_dict()\n",
        "    # hist_len 与训练时保持一致（从模型输入推断）\n",
        "    MAX_HIST_LEN = 30\n",
        "    try:\n",
        "        MAX_HIST_LEN = int([t.shape[1] for t in din.inputs if 'hist_items' in t.name][0])\n",
        "    except Exception:\n",
        "        pass\n",
        "    def encode_seq(seq: List[int]) -> np.ndarray:\n",
        "        enc = [raw_to_item_enc.get(int(x), 0) for x in seq][-MAX_HIST_LEN:]\n",
        "        out = np.zeros(MAX_HIST_LEN, dtype=np.int32)\n",
        "        if enc:\n",
        "            out[-len(enc):] = np.asarray(enc, dtype=np.int32)\n",
        "        return out\n",
        "\n",
        "    hist_mat = np.vstack([encode_seq(user_hist_full.get(int(u), [])) for u in valid_r['user_id'].values])\n",
        "    user_enc = np.asarray([raw_to_user_enc.get(int(u), 0) for u in valid_r['user_id'].values], dtype=np.int32)\n",
        "    item_enc = np.asarray([raw_to_item_enc.get(int(i), 0) for i in valid_r['article_id'].values], dtype=np.int32)\n",
        "\n",
        "    inputs = {'user_id': user_enc, 'hist_items': hist_mat, 'target_item': item_enc, 'dense': X_dense}\n",
        "    preds = din.predict(inputs, batch_size=4096, verbose=0)\n",
        "    m = hit_ndcg_at_k(valid_r, preds, k=5)\n",
        "    ranking_reports.append({'model': 'din', **m})\n",
        "\n",
        "ranking_reports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "53ca7bea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[warn] failed to load sasrec item_id_map.pkl, fallback to rebuild from articles: AttributeError Can't get attribute 'IdMap' on <module '__main__'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'model': 'deepfm',\n",
              "  'hit_rate@5': 0.21225,\n",
              "  'ndcg@5': 0.14279763111664995,\n",
              "  'num_users': 4000},\n",
              " {'model': 'din',\n",
              "  'hit_rate@5': 0.1875,\n",
              "  'ndcg@5': 0.1259067905901265,\n",
              "  'num_users': 4000},\n",
              " {'model': 'dien_like',\n",
              "  'hit_rate@5': 0.2245,\n",
              "  'ndcg@5': 0.15424781275328858,\n",
              "  'num_users': 4000}]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ------- DIEN-like -------\n",
        "dien_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'dien_like' / 'dien_like.keras'\n",
        "dien_scaler_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'dien_like' / 'scaler.pkl'\n",
        "dien_enc_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'dien_like' / 'encoders.pkl'\n",
        "\n",
        "# 用于加载 DIEN-like（notebook9 中的自定义 attention）\n",
        "class SimpleAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_units: List[int] = [80, 40], **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_units = hidden_units\n",
        "        self.mlp = [tf.keras.layers.Dense(u, activation='relu') for u in hidden_units]\n",
        "        self.out = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, keys, mask = inputs\n",
        "        q = tf.expand_dims(query, axis=1)\n",
        "        q = q + tf.zeros_like(keys)\n",
        "        x = tf.concat([q, keys, q - keys, q * keys], axis=-1)\n",
        "        for dense in self.mlp:\n",
        "            x = dense(x)\n",
        "        scores = tf.squeeze(self.out(x), axis=-1)\n",
        "        paddings = tf.ones_like(scores) * (-1e9)\n",
        "        scores = tf.where(mask > 0, scores, paddings)\n",
        "        weights = tf.nn.softmax(scores, axis=-1)\n",
        "        return weights\n",
        "\n",
        "\n",
        "dien_sm_path = PROJECT_PATH / 'artifacts' / 'ranking' / 'dien_like' / 'dien_savedmodel'\n",
        "dien_load_path = dien_sm_path if dien_sm_path.exists() else dien_path\n",
        "\n",
        "dien = None\n",
        "if dien_load_path.exists() and dien_scaler_path.exists() and dien_enc_path.exists():\n",
        "    try:\n",
        "        dien = safe_load_model(dien_load_path, custom_objects={'SimpleAttention': SimpleAttention})\n",
        "    except Exception as e:\n",
        "        print('[skip] failed to load DIEN-like:', type(e).__name__, e)\n",
        "        print('提示：可重新运行 9.sequence_modeling.ipynb 的保存单元，生成 SavedModel 后再评估')\n",
        "\n",
        "if dien is not None:\n",
        "    with open(dien_scaler_path, 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    with open(dien_enc_path, 'rb') as f:\n",
        "        enc = pickle.load(f)\n",
        "    raw_to_user_enc = enc['raw_to_user_enc']\n",
        "\n",
        "    dense_cols = ['recall_score','recall_rank','user_click_count','user_unique_items','item_click_count','words_count','item_age_hours','time_gap_hours','emb_sim_last','is_same_category']\n",
        "    X_dense = scaler.transform(valid_r[dense_cols].fillna(0).values.astype('float32'))\n",
        "\n",
        "    # item_id_map（来自 SASRec artifacts，若不存在则用 articles 重建）\n",
        "    sas_item_map_path = PROJECT_PATH / 'artifacts' / 'sequence' / 'sasrec_inbatch' / 'item_id_map.pkl'\n",
        "    if sas_item_map_path.exists():\n",
        "        try:\n",
        "            with open(sas_item_map_path, 'rb') as f:\n",
        "                item_id_map = pickle.load(f)\n",
        "            if hasattr(item_id_map, 'transform'):\n",
        "                def enc_item(arr):\n",
        "                    return item_id_map.transform(np.asarray(arr, dtype=np.int64))\n",
        "            else:\n",
        "                raise TypeError('item_id_map.pkl is not a compatible object (missing transform)')\n",
        "        except Exception as e:\n",
        "            # notebook9 里 IdMap 是在 notebook 中定义的 dataclass，pickle 在其它 notebook 反序列化会失败。\n",
        "            print('[warn] failed to load sasrec item_id_map.pkl, fallback to rebuild from articles:', type(e).__name__, e)\n",
        "            all_items = articles['article_id'].astype(int).unique()\n",
        "            idx = {int(v): int(i + 1) for i, v in enumerate(np.sort(all_items))}\n",
        "            def enc_item(arr):\n",
        "                return np.asarray([idx.get(int(x), 0) for x in arr], dtype=np.int32)\n",
        "    else:\n",
        "        # 简单重建\n",
        "        all_items = articles['article_id'].astype(int).unique()\n",
        "        idx = {int(v): int(i + 1) for i, v in enumerate(np.sort(all_items))}\n",
        "        def enc_item(arr):\n",
        "            return np.asarray([idx.get(int(x), 0) for x in arr], dtype=np.int32)\n",
        "\n",
        "    train_hist_full = pd.read_pickle(PROJECT_PATH / 'train_hist.pkl').sort_values(['user_id','click_timestamp'])\n",
        "    user_hist_full = train_hist_full.groupby('user_id')['click_article_id'].apply(list).to_dict()\n",
        "    MAX_HIST_LEN = 30\n",
        "    try:\n",
        "        MAX_HIST_LEN = int([t.shape[1] for t in dien.inputs if 'hist_items' in t.name][0])\n",
        "    except Exception:\n",
        "        pass\n",
        "    def encode_seq_raw(seq: List[int]) -> np.ndarray:\n",
        "        enc = enc_item(seq)[-MAX_HIST_LEN:]\n",
        "        out = np.zeros(MAX_HIST_LEN, dtype=np.int32)\n",
        "        if len(enc) > 0:\n",
        "            out[-len(enc):] = enc.astype(np.int32)\n",
        "        return out\n",
        "\n",
        "    hist_mat = np.vstack([encode_seq_raw(user_hist_full.get(int(u), [])) for u in valid_r['user_id'].values])\n",
        "    user_enc = np.asarray([raw_to_user_enc.get(int(u), 0) for u in valid_r['user_id'].values], dtype=np.int32)\n",
        "    item_enc = enc_item(valid_r['article_id'].values)\n",
        "\n",
        "    inputs = {'user_id': user_enc, 'hist_items': hist_mat, 'target_item': item_enc, 'dense': X_dense}\n",
        "    preds = dien.predict(inputs, batch_size=4096, verbose=0)\n",
        "    m = hit_ndcg_at_k(valid_r, preds, k=5)\n",
        "    ranking_reports.append({'model': 'dien_like', **m})\n",
        "\n",
        "ranking_reports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>hit_rate@5</th>\n",
              "      <th>ndcg@5</th>\n",
              "      <th>num_users</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>deepfm</td>\n",
              "      <td>0.21225</td>\n",
              "      <td>0.142798</td>\n",
              "      <td>4000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dien_like</td>\n",
              "      <td>0.22450</td>\n",
              "      <td>0.154248</td>\n",
              "      <td>4000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>din</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>0.125907</td>\n",
              "      <td>4000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       model  hit_rate@5    ndcg@5  num_users\n",
              "0     deepfm     0.21225  0.142798       4000\n",
              "2  dien_like     0.22450  0.154248       4000\n",
              "1        din     0.18750  0.125907       4000"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ranking_report = pd.DataFrame(ranking_reports).sort_values('model') if ranking_reports else pd.DataFrame()\n",
        "ranking_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) 多任务评估（AUC / GAUC）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[skip] missing multitask preprocess/scaler, run 10.multi_task.ipynb first\n"
          ]
        }
      ],
      "source": [
        "def gauc(user_ids: np.ndarray, labels: np.ndarray, preds: np.ndarray) -> float:\n",
        "    user_ids = np.asarray(user_ids)\n",
        "    labels = np.asarray(labels)\n",
        "    preds = np.asarray(preds)\n",
        "    uniq = np.unique(user_ids)\n",
        "    aucs = []\n",
        "    weights = []\n",
        "    for u in uniq:\n",
        "        m = user_ids == u\n",
        "        y = labels[m]\n",
        "        if len(np.unique(y)) < 2:\n",
        "            continue\n",
        "        p = preds[m]\n",
        "        aucs.append(roc_auc_score(y, p))\n",
        "        weights.append(np.sum(m))\n",
        "    if not aucs:\n",
        "        return 0.0\n",
        "    return float(np.average(aucs, weights=weights))\n",
        "\n",
        "\n",
        "mt_dir = PROJECT_PATH / 'artifacts' / 'multitask'\n",
        "mt_models = {\n",
        "    'shared_bottom': mt_dir / 'shared_bottom.keras',\n",
        "    'mmoe': mt_dir / 'mmoe.keras',\n",
        "    'esmm': mt_dir / 'esmm.keras',\n",
        "    'ple': mt_dir / 'ple.keras',\n",
        "}\n",
        "\n",
        "mt_reports = []\n",
        "rank_path = PROJECT_PATH / 'rank_train.pkl'\n",
        "if not rank_path.exists():\n",
        "    print('[skip] missing rank_train.pkl, run 6.ranking.ipynb (or 8.deep_ranking.ipynb) first')\n",
        "    mt_report = pd.DataFrame()\n",
        "else:\n",
        "    df = pd.read_pickle(rank_path)\n",
        "\n",
        "    preprocess_path = mt_dir / 'preprocess.pkl'\n",
        "    scaler_path = mt_dir / 'scaler.pkl'\n",
        "    if not preprocess_path.exists() or not scaler_path.exists():\n",
        "        print('[skip] missing multitask preprocess/scaler, run 10.multi_task.ipynb first')\n",
        "        mt_report = pd.DataFrame()\n",
        "    else:\n",
        "        with open(preprocess_path, 'rb') as f:\n",
        "            prep = pickle.load(f)\n",
        "        with open(scaler_path, 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "\n",
        "        thresh_words = int(prep.get('thresh_words', 300))\n",
        "        df['ctr'] = df['label'].astype(int)\n",
        "        df['ctcvr'] = ((df['label'] == 1) & (df['words_count'].fillna(0) >= thresh_words)).astype(int)\n",
        "\n",
        "        # 固定 split（与 Notebook10 一致）\n",
        "        rng = np.random.default_rng(42)\n",
        "        users = df['user_id'].unique()\n",
        "        rng.shuffle(users)\n",
        "        split = int(len(users) * 0.8)\n",
        "        valid_users = set(users[split:])\n",
        "        valid_df = df[df['user_id'].isin(valid_users)].copy()\n",
        "\n",
        "        sparse_cols = prep['sparse_cols']\n",
        "        dense_cols = prep['dense_cols']\n",
        "        uniques = prep['uniques']\n",
        "\n",
        "        def encode_with_uniques(series: pd.Series, uniq_list: List[str]) -> np.ndarray:\n",
        "            idx = {v: i + 1 for i, v in enumerate(uniq_list)}\n",
        "            return np.asarray([idx.get(str(x), 0) for x in series.values], dtype=np.int32)\n",
        "\n",
        "        inputs = {c: encode_with_uniques(valid_df[c].fillna(0).astype(str), uniques[c]) for c in sparse_cols}\n",
        "        inputs['dense'] = scaler.transform(valid_df[dense_cols].fillna(0).values.astype('float32'))\n",
        "\n",
        "        y_ctr = valid_df['ctr'].values.astype(int)\n",
        "        y_ctcvr = valid_df['ctcvr'].values.astype(int)\n",
        "\n",
        "        for name, path in mt_models.items():\n",
        "            if not path.exists():\n",
        "                continue\n",
        "            m = tf.keras.models.load_model(path)\n",
        "            pred = m.predict(inputs, batch_size=4096, verbose=0)\n",
        "            p_ctr = pred['ctr'] if isinstance(pred, dict) else pred[0]\n",
        "            p_ctcvr = pred['ctcvr'] if isinstance(pred, dict) else pred[1]\n",
        "            p_ctr = np.asarray(p_ctr).reshape(-1)\n",
        "            p_ctcvr = np.asarray(p_ctcvr).reshape(-1)\n",
        "\n",
        "            ctr_auc = roc_auc_score(y_ctr, p_ctr) if len(np.unique(y_ctr)) > 1 else float('nan')\n",
        "            ctcvr_auc = roc_auc_score(y_ctcvr, p_ctcvr) if len(np.unique(y_ctcvr)) > 1 else float('nan')\n",
        "            mt_reports.append({\n",
        "                'model': name,\n",
        "                'ctr_auc': ctr_auc,\n",
        "                'ctr_gauc': gauc(valid_df['user_id'].values, y_ctr, p_ctr),\n",
        "                'ctcvr_auc': ctcvr_auc,\n",
        "                'ctcvr_gauc': gauc(valid_df['user_id'].values, y_ctcvr, p_ctcvr),\n",
        "            })\n",
        "\n",
        "        mt_report = pd.DataFrame(mt_reports).sort_values('model') if mt_reports else pd.DataFrame()\n",
        "        mt_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) 保存报告\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved to: /Users/wangjunfei/Desktop/fun-rec/tmp/projects/news_recommendation_system/artifacts/evaluation\n"
          ]
        }
      ],
      "source": [
        "recall_report.to_csv(EVAL_DIR / 'recall_report.csv', index=False)\n",
        "ranking_report.to_csv(EVAL_DIR / 'ranking_report.csv', index=False)\n",
        "mt_report.to_csv(EVAL_DIR / 'multitask_report.csv', index=False)\n",
        "\n",
        "print('saved to:', EVAL_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "funrec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
