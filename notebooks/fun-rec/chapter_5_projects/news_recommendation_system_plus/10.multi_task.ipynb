{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. 多任务学习（Shared-Bottom / MMoE / ESMM / PLE）\n",
        "\n",
        "工业推荐里经常是多目标：点击（CTR）、转化（CVR）、深度阅读/停留、点赞、收藏、分享……\n",
        "\n",
        "这节我们在新闻数据上把“多任务建模”模块补齐：\n",
        "\n",
        "- **Shared-Bottom**：共享底座 + 任务塔（基线）\n",
        "- **MMoE**：多专家 + gate（常见面试题）\n",
        "- **ESMM**：CTR/CVR 去偏建模（需要转化类标签）\n",
        "- **PLE**：分层专家抽取（多任务更强的结构）\n",
        "\n",
        "## 重要说明（关于标签）\n",
        "\n",
        "公开新闻数据集通常只有点击日志，没有真实的“转化/停留/点赞”等多目标反馈。\n",
        "\n",
        "为了让项目具备可运行的多任务训练流程，这里构造一个**可解释的 proxy 标签**：\n",
        "\n",
        "- `ctr`：是否为验证目标点击（来自 `rank_train.pkl` 的 label）\n",
        "- `ctcvr`：是否为“高价值点击”= `ctr==1` 且 `words_count >= THRESH_WORDS`\n",
        "\n",
        "你在简历/面试里要明确写清楚：\n",
        "\n",
        "- 这是 proxy label（用于展示方法与工程能力）\n",
        "- 真实业务应使用真实转化/停留等日志，并做样本选择偏差（SSB）处理\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    cur = start\n",
        "    for _ in range(10):\n",
        "        if (cur / 'pyproject.toml').exists() or (cur / '.git').exists():\n",
        "            return cur\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "dotenv_path = find_dotenv(usecwd=True)\n",
        "if dotenv_path:\n",
        "    load_dotenv(dotenv_path)\n",
        "os.environ.setdefault('FUNREC_RAW_DATA_PATH', str(REPO_ROOT / 'data'))\n",
        "os.environ.setdefault('FUNREC_PROCESSED_DATA_PATH', str(REPO_ROOT / 'tmp'))\n",
        "\n",
        "RAW_DATA_PATH = Path(os.getenv('FUNREC_RAW_DATA_PATH'))\n",
        "PROCESSED_DATA_PATH = Path(os.getenv('FUNREC_PROCESSED_DATA_PATH'))\n",
        "PROJECT_PATH = PROCESSED_DATA_PATH / 'projects' / 'news_recommendation_system'\n",
        "\n",
        "ARTIFACTS_DIR = PROJECT_PATH / 'artifacts' / 'multitask'\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PROJECT_PATH, ARTIFACTS_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rank_path = PROJECT_PATH / 'rank_train.pkl'\n",
        "if not rank_path.exists():\n",
        "    raise FileNotFoundError(f'Missing {rank_path}. Run 5.feature_engineering.ipynb first.')\n",
        "\n",
        "df = pd.read_pickle(rank_path)\n",
        "\n",
        "THRESH_WORDS = 300\n",
        "df['ctr'] = df['label'].astype(int)\n",
        "df['ctcvr'] = ((df['label'] == 1) & (df['words_count'].fillna(0) >= THRESH_WORDS)).astype(int)\n",
        "\n",
        "# 按 user 划分训练/验证\n",
        "rng = np.random.default_rng(42)\n",
        "users = df['user_id'].unique()\n",
        "rng.shuffle(users)\n",
        "split = int(len(users) * 0.8)\n",
        "train_users = set(users[:split])\n",
        "\n",
        "train_df = df[df['user_id'].isin(train_users)].copy()\n",
        "valid_df = df[~df['user_id'].isin(train_users)].copy()\n",
        "\n",
        "DEBUG = True\n",
        "MAX_TRAIN_ROWS = 300000\n",
        "if DEBUG and len(train_df) > MAX_TRAIN_ROWS:\n",
        "    train_df = train_df.sample(MAX_TRAIN_ROWS, random_state=42)\n",
        "\n",
        "train_df[['ctr', 'ctcvr']].mean(), valid_df[['ctr', 'ctcvr']].mean(), len(train_df), len(valid_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 特征准备\n",
        "\n",
        "和深度排序一致：\n",
        "\n",
        "- sparse：user/item/category 等\n",
        "- dense：召回分数 + 统计/时间/相似度等\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SPARSE_COLS = ['user_id', 'article_id', 'category_id', 'user_top_category', 'last_click_article_id']\n",
        "DENSE_COLS = [\n",
        "    'recall_score', 'recall_rank',\n",
        "    'user_click_count', 'user_unique_items',\n",
        "    'item_click_count', 'words_count',\n",
        "    'item_age_hours', 'time_gap_hours',\n",
        "    'emb_sim_last', 'is_same_category',\n",
        "]\n",
        "\n",
        "\n",
        "def encode_categorical(train_s: pd.Series, valid_s: pd.Series) -> Tuple[np.ndarray, np.ndarray, int, List[str]]:\n",
        "    all_s = pd.concat([train_s, valid_s], axis=0)\n",
        "    codes, uniques = pd.factorize(all_s.astype(str), sort=True)\n",
        "    codes = codes.astype(np.int32) + 1  # 0 reserved\n",
        "    train_codes = codes[: len(train_s)]\n",
        "    valid_codes = codes[len(train_s) :]\n",
        "    vocab_size = int(len(uniques) + 1)\n",
        "    return train_codes, valid_codes, vocab_size, list(uniques)\n",
        "\n",
        "\n",
        "train_sparse = {}\n",
        "valid_sparse = {}\n",
        "vocab_sizes = {}\n",
        "factor_uniques = {}\n",
        "for col in SPARSE_COLS:\n",
        "    tr, va, vs, uniq = encode_categorical(train_df[col].fillna(0), valid_df[col].fillna(0))\n",
        "    train_sparse[col] = tr\n",
        "    valid_sparse[col] = va\n",
        "    vocab_sizes[col] = vs\n",
        "    factor_uniques[col] = uniq\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_dense = scaler.fit_transform(train_df[DENSE_COLS].fillna(0).values.astype('float32'))\n",
        "X_valid_dense = scaler.transform(valid_df[DENSE_COLS].fillna(0).values.astype('float32'))\n",
        "\n",
        "y_train = {\n",
        "    'ctr': train_df['ctr'].values.astype('float32'),\n",
        "    'ctcvr': train_df['ctcvr'].values.astype('float32'),\n",
        "}\n",
        "y_valid = {\n",
        "    'ctr': valid_df['ctr'].values.astype('float32'),\n",
        "    'ctcvr': valid_df['ctcvr'].values.astype('float32'),\n",
        "}\n",
        "\n",
        "train_inputs = {**train_sparse, 'dense': X_train_dense}\n",
        "valid_inputs = {**valid_sparse, 'dense': X_valid_dense}\n",
        "\n",
        "len(y_train['ctr']), len(y_train['ctcvr'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gauc(user_ids: np.ndarray, labels: np.ndarray, preds: np.ndarray) -> float:\n",
        "    user_ids = np.asarray(user_ids)\n",
        "    labels = np.asarray(labels)\n",
        "    preds = np.asarray(preds)\n",
        "    uniq = np.unique(user_ids)\n",
        "    aucs = []\n",
        "    weights = []\n",
        "    for u in uniq:\n",
        "        m = user_ids == u\n",
        "        y = labels[m]\n",
        "        if len(np.unique(y)) < 2:\n",
        "            continue\n",
        "        p = preds[m]\n",
        "        aucs.append(roc_auc_score(y, p))\n",
        "        weights.append(np.sum(m))\n",
        "    if not aucs:\n",
        "        return 0.0\n",
        "    return float(np.average(aucs, weights=weights))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Shared-Bottom（基线）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_shared_bottom(sparse_vocab_sizes: Dict[str, int], dense_dim: int, task_names: List[str], emb_dim: int = 16, shared_units: List[int] = [128, 64], tower_units: List[int] = [64]):\n",
        "    inputs = {}\n",
        "    embed_vecs = []\n",
        "    for feat, vocab_size in sparse_vocab_sizes.items():\n",
        "        inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name=feat)\n",
        "        inputs[feat] = inp\n",
        "        emb = tf.keras.layers.Flatten()(tf.keras.layers.Embedding(vocab_size, emb_dim)(inp))\n",
        "        embed_vecs.append(emb)\n",
        "    dense_inp = tf.keras.layers.Input(shape=(dense_dim,), dtype=tf.float32, name='dense')\n",
        "    inputs['dense'] = dense_inp\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()(embed_vecs + [dense_inp])\n",
        "    for u in shared_units:\n",
        "        x = tf.keras.layers.Dense(u, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "    outputs = {}\n",
        "    for t in task_names:\n",
        "        h = x\n",
        "        for u in tower_units:\n",
        "            h = tf.keras.layers.Dense(u, activation='relu')(h)\n",
        "        out = tf.keras.layers.Dense(1, activation='sigmoid', name=t)(h)\n",
        "        outputs[t] = tf.keras.layers.Flatten()(out)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='SharedBottom')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss={t: 'binary_crossentropy' for t in task_names},\n",
        "        metrics={t: [tf.keras.metrics.AUC(name='auc')] for t in task_names},\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "shared_bottom = build_shared_bottom({k: vocab_sizes[k] for k in SPARSE_COLS}, dense_dim=X_train_dense.shape[1], task_names=['ctr', 'ctcvr'])\n",
        "shared_bottom.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shared_bottom.fit(train_inputs, y_train, batch_size=2048, epochs=2, validation_data=(valid_inputs, y_valid), verbose=1)\n",
        "\n",
        "pred = shared_bottom.predict(valid_inputs, batch_size=4096, verbose=0)\n",
        "metrics_shared = {\n",
        "    'ctr_auc': roc_auc_score(y_valid['ctr'], pred['ctr']),\n",
        "    'ctr_gauc': gauc(valid_df['user_id'].values, y_valid['ctr'], pred['ctr']),\n",
        "    'ctcvr_auc': roc_auc_score(y_valid['ctcvr'], pred['ctcvr']),\n",
        "    'ctcvr_gauc': gauc(valid_df['user_id'].values, y_valid['ctcvr'], pred['ctcvr']),\n",
        "}\n",
        "metrics_shared\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) MMoE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_mmoe(\n",
        "    sparse_vocab_sizes: Dict[str, int],\n",
        "    dense_dim: int,\n",
        "    task_names: List[str],\n",
        "    emb_dim: int = 16,\n",
        "    num_experts: int = 4,\n",
        "    expert_units: List[int] = [128, 64],\n",
        "    tower_units: List[int] = [64],\n",
        "):\n",
        "    inputs = {}\n",
        "    embed_vecs = []\n",
        "    for feat, vocab_size in sparse_vocab_sizes.items():\n",
        "        inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name=feat)\n",
        "        inputs[feat] = inp\n",
        "        emb = tf.keras.layers.Flatten()(tf.keras.layers.Embedding(vocab_size, emb_dim)(inp))\n",
        "        embed_vecs.append(emb)\n",
        "    dense_inp = tf.keras.layers.Input(shape=(dense_dim,), dtype=tf.float32, name='dense')\n",
        "    inputs['dense'] = dense_inp\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()(embed_vecs + [dense_inp])\n",
        "\n",
        "    # Experts\n",
        "    expert_outputs = []\n",
        "    for i in range(num_experts):\n",
        "        h = x\n",
        "        for u in expert_units:\n",
        "            h = tf.keras.layers.Dense(u, activation='relu')(h)\n",
        "        expert_outputs.append(h)\n",
        "    experts = tf.keras.layers.Lambda(lambda z: tf.stack(z, axis=1), name='experts_stack')(expert_outputs)  # [B, E, H]\n",
        "\n",
        "    outputs = {}\n",
        "    for t in task_names:\n",
        "        gate = tf.keras.layers.Dense(num_experts, activation='softmax', name=f'gate_{t}')(x)  # [B, E]\n",
        "        gate = tf.keras.layers.Lambda(lambda g: tf.expand_dims(g, axis=-1))(gate)  # [B, E, 1]\n",
        "        task_inp = tf.keras.layers.Lambda(lambda z: tf.reduce_sum(z[0] * z[1], axis=1))([experts, gate])  # [B, H]\n",
        "\n",
        "        h = task_inp\n",
        "        for u in tower_units:\n",
        "            h = tf.keras.layers.Dense(u, activation='relu')(h)\n",
        "        out = tf.keras.layers.Dense(1, activation='sigmoid', name=t)(h)\n",
        "        outputs[t] = tf.keras.layers.Flatten()(out)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='MMoE')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss={t: 'binary_crossentropy' for t in task_names},\n",
        "        metrics={t: [tf.keras.metrics.AUC(name='auc')] for t in task_names},\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "mmoe = build_mmoe({k: vocab_sizes[k] for k in SPARSE_COLS}, dense_dim=X_train_dense.shape[1], task_names=['ctr', 'ctcvr'])\n",
        "mmoe.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mmoe.fit(train_inputs, y_train, batch_size=2048, epochs=2, validation_data=(valid_inputs, y_valid), verbose=1)\n",
        "\n",
        "pred = mmoe.predict(valid_inputs, batch_size=4096, verbose=0)\n",
        "metrics_mmoe = {\n",
        "    'ctr_auc': roc_auc_score(y_valid['ctr'], pred['ctr']),\n",
        "    'ctr_gauc': gauc(valid_df['user_id'].values, y_valid['ctr'], pred['ctr']),\n",
        "    'ctcvr_auc': roc_auc_score(y_valid['ctcvr'], pred['ctcvr']),\n",
        "    'ctcvr_gauc': gauc(valid_df['user_id'].values, y_valid['ctcvr'], pred['ctcvr']),\n",
        "}\n",
        "metrics_mmoe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) ESMM（CTR + CVR → CTCVR）\n",
        "\n",
        "ESMM 经典用于解决 CVR 的样本选择偏差（只在点击后才观测转化）。\n",
        "\n",
        "这里为了能跑通，我们用 `ctcvr` proxy 标签；结构仍然与 ESMM 一致：\n",
        "\n",
        "- 输出 `pCTR`\n",
        "- 输出 `pCVR`\n",
        "- 输出 `pCTCVR = pCTR * pCVR`\n",
        "- 用 `ctr` 和 `ctcvr` 两个 loss 训练\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_esmm(sparse_vocab_sizes: Dict[str, int], dense_dim: int, emb_dim: int = 16, shared_units: List[int] = [128, 64]):\n",
        "    inputs = {}\n",
        "    embed_vecs = []\n",
        "    for feat, vocab_size in sparse_vocab_sizes.items():\n",
        "        inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name=feat)\n",
        "        inputs[feat] = inp\n",
        "        emb = tf.keras.layers.Flatten()(tf.keras.layers.Embedding(vocab_size, emb_dim)(inp))\n",
        "        embed_vecs.append(emb)\n",
        "    dense_inp = tf.keras.layers.Input(shape=(dense_dim,), dtype=tf.float32, name='dense')\n",
        "    inputs['dense'] = dense_inp\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()(embed_vecs + [dense_inp])\n",
        "    for u in shared_units:\n",
        "        x = tf.keras.layers.Dense(u, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "    ctr_logit = tf.keras.layers.Dense(1, activation=None)(x)\n",
        "    cvr_logit = tf.keras.layers.Dense(1, activation=None)(x)\n",
        "    pctr = tf.keras.layers.Activation('sigmoid', name='ctr')(ctr_logit)\n",
        "    pcvr = tf.keras.layers.Activation('sigmoid', name='cvr')(cvr_logit)\n",
        "    pctcvr = tf.keras.layers.Multiply(name='ctcvr')([pctr, pcvr])\n",
        "\n",
        "    outputs = {\n",
        "        'ctr': tf.keras.layers.Flatten()(pctr),\n",
        "        'ctcvr': tf.keras.layers.Flatten()(pctcvr),\n",
        "    }\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='ESMM')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss={'ctr': 'binary_crossentropy', 'ctcvr': 'binary_crossentropy'},\n",
        "        metrics={'ctr': [tf.keras.metrics.AUC(name='auc')], 'ctcvr': [tf.keras.metrics.AUC(name='auc')]},\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "esmm = build_esmm({k: vocab_sizes[k] for k in SPARSE_COLS}, dense_dim=X_train_dense.shape[1])\n",
        "esmm.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "esmm.fit(train_inputs, y_train, batch_size=2048, epochs=2, validation_data=(valid_inputs, y_valid), verbose=1)\n",
        "\n",
        "pred = esmm.predict(valid_inputs, batch_size=4096, verbose=0)\n",
        "metrics_esmm = {\n",
        "    'ctr_auc': roc_auc_score(y_valid['ctr'], pred['ctr']),\n",
        "    'ctr_gauc': gauc(valid_df['user_id'].values, y_valid['ctr'], pred['ctr']),\n",
        "    'ctcvr_auc': roc_auc_score(y_valid['ctcvr'], pred['ctcvr']),\n",
        "    'ctcvr_gauc': gauc(valid_df['user_id'].values, y_valid['ctcvr'], pred['ctcvr']),\n",
        "}\n",
        "metrics_esmm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) PLE（简化版：1 层）\n",
        "\n",
        "PLE 比 MMoE 更常见于多任务强耦合场景；这里实现一个最小可用版本：\n",
        "\n",
        "- shared experts + task experts\n",
        "- task gate 在 shared+task experts 上做混合\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_ple(\n",
        "    sparse_vocab_sizes: Dict[str, int],\n",
        "    dense_dim: int,\n",
        "    task_names: List[str],\n",
        "    emb_dim: int = 16,\n",
        "    num_shared_experts: int = 2,\n",
        "    num_task_experts: int = 2,\n",
        "    expert_units: List[int] = [128, 64],\n",
        "    tower_units: List[int] = [64],\n",
        "):\n",
        "    inputs = {}\n",
        "    embed_vecs = []\n",
        "    for feat, vocab_size in sparse_vocab_sizes.items():\n",
        "        inp = tf.keras.layers.Input(shape=(), dtype=tf.int32, name=feat)\n",
        "        inputs[feat] = inp\n",
        "        emb = tf.keras.layers.Flatten()(tf.keras.layers.Embedding(vocab_size, emb_dim)(inp))\n",
        "        embed_vecs.append(emb)\n",
        "    dense_inp = tf.keras.layers.Input(shape=(dense_dim,), dtype=tf.float32, name='dense')\n",
        "    inputs['dense'] = dense_inp\n",
        "\n",
        "    x = tf.keras.layers.Concatenate()(embed_vecs + [dense_inp])\n",
        "\n",
        "    def make_expert(name: str):\n",
        "        h = x\n",
        "        for u in expert_units:\n",
        "            h = tf.keras.layers.Dense(u, activation='relu', name=f'{name}_dense_{u}')(h)\n",
        "        return h\n",
        "\n",
        "    shared_experts = [make_expert(f'shared_{i}') for i in range(num_shared_experts)]\n",
        "\n",
        "    outputs = {}\n",
        "    for t in task_names:\n",
        "        task_experts = [make_expert(f'{t}_expert_{i}') for i in range(num_task_experts)]\n",
        "        all_experts = shared_experts + task_experts\n",
        "        experts_stack = tf.keras.layers.Lambda(lambda z: tf.stack(z, axis=1), name=f'{t}_experts_stack')(all_experts)\n",
        "\n",
        "        gate = tf.keras.layers.Dense(len(all_experts), activation='softmax', name=f'{t}_gate')(x)\n",
        "        gate = tf.keras.layers.Lambda(lambda g: tf.expand_dims(g, axis=-1))(gate)\n",
        "        task_inp = tf.keras.layers.Lambda(lambda z: tf.reduce_sum(z[0] * z[1], axis=1))([experts_stack, gate])\n",
        "\n",
        "        h = task_inp\n",
        "        for u in tower_units:\n",
        "            h = tf.keras.layers.Dense(u, activation='relu')(h)\n",
        "        out = tf.keras.layers.Dense(1, activation='sigmoid', name=t)(h)\n",
        "        outputs[t] = tf.keras.layers.Flatten()(out)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='PLE_1layer')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss={t: 'binary_crossentropy' for t in task_names},\n",
        "        metrics={t: [tf.keras.metrics.AUC(name='auc')] for t in task_names},\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "ple = build_ple({k: vocab_sizes[k] for k in SPARSE_COLS}, dense_dim=X_train_dense.shape[1], task_names=['ctr', 'ctcvr'])\n",
        "ple.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ple.fit(train_inputs, y_train, batch_size=2048, epochs=2, validation_data=(valid_inputs, y_valid), verbose=1)\n",
        "\n",
        "pred = ple.predict(valid_inputs, batch_size=4096, verbose=0)\n",
        "metrics_ple = {\n",
        "    'ctr_auc': roc_auc_score(y_valid['ctr'], pred['ctr']),\n",
        "    'ctr_gauc': gauc(valid_df['user_id'].values, y_valid['ctr'], pred['ctr']),\n",
        "    'ctcvr_auc': roc_auc_score(y_valid['ctcvr'], pred['ctcvr']),\n",
        "    'ctcvr_gauc': gauc(valid_df['user_id'].values, y_valid['ctcvr'], pred['ctcvr']),\n",
        "}\n",
        "metrics_ple\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) 总结与保存\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = pd.DataFrame([\n",
        "    {'model': 'shared_bottom', **metrics_shared},\n",
        "    {'model': 'mmoe', **metrics_mmoe},\n",
        "    {'model': 'esmm', **metrics_esmm},\n",
        "    {'model': 'ple', **metrics_ple},\n",
        "])\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shared_bottom.save(ARTIFACTS_DIR / 'shared_bottom.keras')\n",
        "mmoe.save(ARTIFACTS_DIR / 'mmoe.keras')\n",
        "esmm.save(ARTIFACTS_DIR / 'esmm.keras')\n",
        "ple.save(ARTIFACTS_DIR / 'ple.keras')\n",
        "\n",
        "with open(ARTIFACTS_DIR / 'scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "with open(ARTIFACTS_DIR / 'vocab_sizes.pkl', 'wb') as f:\n",
        "    pickle.dump(vocab_sizes, f)\n",
        "with open(ARTIFACTS_DIR / 'preprocess.pkl', 'wb') as f:\n",
        "    pickle.dump({'sparse_cols': SPARSE_COLS, 'dense_cols': DENSE_COLS, 'uniques': factor_uniques, 'thresh_words': THRESH_WORDS}, f)\n",
        "summary.to_csv(ARTIFACTS_DIR / 'multi_task_report.csv', index=False)\n",
        "\n",
        "print('saved to:', ARTIFACTS_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
