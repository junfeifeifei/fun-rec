{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 12. 模型部署与推理（Serving Demo）\n",
        "\n",
        "这个 Notebook 的目标：把前面训练好的模型（召回 + 排序）串起来，做一个**可本地运行的“推荐服务”Demo**。\n",
        "\n",
        "你可以把它当成面试/实习时解释“线上推理链路”的样板：\n",
        "\n",
        "1. **召回（Retrieval）**：双塔 / SASRec → FAISS 检索 TopK 候选\n",
        "2. **特征（Features）**：复用离线特征表 + 计算实时交互特征\n",
        "3. **排序（Ranking）**：LGBM / DeepFM / DIN 预测分数并 rerank\n",
        "4. **冷启动与兜底**：无历史用户回落到热门\n",
        "5. **导出（Export）**：SavedModel（可选 ONNX，若环境有依赖）\n",
        "\n",
        "> 注：本仓库默认依赖里没有 `onnxruntime/tf2onnx`，所以 ONNX 部分会做成可选跳过，不影响 notebook 运行。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import faiss\n",
        "import lightgbm as lgb\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    cur = start\n",
        "    for _ in range(10):\n",
        "        if (cur / 'pyproject.toml').exists() or (cur / '.git').exists():\n",
        "            return cur\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "\n",
        "RAW_DATA_PATH = Path(os.getenv('FUNREC_RAW_DATA_PATH', REPO_ROOT / 'data'))\n",
        "PROCESSED_DATA_PATH = Path(os.getenv('FUNREC_PROCESSED_DATA_PATH', REPO_ROOT / 'tmp'))\n",
        "\n",
        "DATA_PATH = RAW_DATA_PATH / 'dataset' / 'news_recommendation'\n",
        "if not DATA_PATH.exists():\n",
        "    DATA_PATH = RAW_DATA_PATH / 'news_recommendation'\n",
        "\n",
        "PROJECT_PATH = PROCESSED_DATA_PATH / 'projects' / 'news_recommendation_system'\n",
        "PROJECT_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('DATA_PATH   :', DATA_PATH)\n",
        "print('PROJECT_PATH:', PROJECT_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 加载基础产物（历史、特征表、热门兜底）\n",
        "\n",
        "Serving 里最关键的是 **offline/online 一致性**：\n",
        "- 训练和推理要用**一致的特征定义**、**一致的 ID 编码**\n",
        "- 召回与排序的候选构建、过滤（比如去掉已看）要一致\n",
        "\n",
        "这里默认复用基础版（1-6）产物：\n",
        "- `train_hist.pkl` / `valid_last.pkl`\n",
        "- `user_features.pkl` / `item_features.pkl` / `user_last_click.pkl`\n",
        "- `popular_items.pkl`（冷启动兜底）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "required = [\n",
        "    'train_hist.pkl',\n",
        "    'valid_last.pkl',\n",
        "    'user_features.pkl',\n",
        "    'item_features.pkl',\n",
        "    'user_last_click.pkl',\n",
        "    'popular_items.pkl',\n",
        "    'feature_cols.pkl',\n",
        "    'lgb_ranker.txt',\n",
        "]\n",
        "missing = [f for f in required if not (PROJECT_PATH / f).exists()]\n",
        "if missing:\n",
        "    print('[warn] missing baseline artifacts:', missing)\n",
        "    print('请先运行基础版 notebooks 1-6（尤其是 5.feature_engineering 与 6.ranking）')\n",
        "\n",
        "train_hist = pd.read_pickle(PROJECT_PATH / 'train_hist.pkl') if (PROJECT_PATH / 'train_hist.pkl').exists() else pd.DataFrame()\n",
        "valid_last = pd.read_pickle(PROJECT_PATH / 'valid_last.pkl') if (PROJECT_PATH / 'valid_last.pkl').exists() else pd.DataFrame()\n",
        "user_features = pd.read_pickle(PROJECT_PATH / 'user_features.pkl') if (PROJECT_PATH / 'user_features.pkl').exists() else pd.DataFrame()\n",
        "item_features = pd.read_pickle(PROJECT_PATH / 'item_features.pkl') if (PROJECT_PATH / 'item_features.pkl').exists() else pd.DataFrame()\n",
        "user_last_click = pd.read_pickle(PROJECT_PATH / 'user_last_click.pkl') if (PROJECT_PATH / 'user_last_click.pkl').exists() else pd.DataFrame()\n",
        "popular_items = pickle.load(open(PROJECT_PATH / 'popular_items.pkl', 'rb')) if (PROJECT_PATH / 'popular_items.pkl').exists() else []\n",
        "feature_cols = pickle.load(open(PROJECT_PATH / 'feature_cols.pkl', 'rb')) if (PROJECT_PATH / 'feature_cols.pkl').exists() else []\n",
        "\n",
        "if not train_hist.empty:\n",
        "    train_hist = train_hist.sort_values(['user_id', 'click_timestamp'])\n",
        "    user_hist = train_hist.groupby('user_id')['click_article_id'].apply(list).to_dict()\n",
        "else:\n",
        "    user_hist = {}\n",
        "\n",
        "articles = pd.read_csv(DATA_PATH / 'articles.csv') if (DATA_PATH / 'articles.csv').exists() else pd.DataFrame()\n",
        "\n",
        "print('train_hist:', train_hist.shape)\n",
        "print('valid_last:', valid_last.shape)\n",
        "print('user_features:', user_features.shape)\n",
        "print('item_features:', item_features.shape)\n",
        "print('popular_items:', len(popular_items))\n",
        "print('articles:', articles.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### （可选）加载文章向量计算 `emb_sim_last`\n",
        "\n",
        "`articles_emb.csv` 很大（36w × 250），Serving/面试里重点是链路与一致性：\n",
        "- 默认 `LOAD_EMB=False`，直接把 `emb_sim_last=0`\n",
        "- 如果你想更真实地复用基础特征，可以改成 `True` 加载并计算相似度\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LOAD_EMB = False\n",
        "\n",
        "emb_path = DATA_PATH / 'articles_emb.csv'\n",
        "article_emb = None\n",
        "id2idx = None\n",
        "emb_matrix = None\n",
        "\n",
        "if LOAD_EMB and emb_path.exists():\n",
        "    t0 = time.time()\n",
        "    article_emb = pd.read_csv(emb_path)\n",
        "    emb_cols = [c for c in article_emb.columns if c.startswith('emb_')]\n",
        "    emb_matrix = article_emb[emb_cols].values.astype('float32')\n",
        "    emb_matrix /= np.linalg.norm(emb_matrix, axis=1, keepdims=True) + 1e-12\n",
        "    article_ids = article_emb['article_id'].values.astype(int)\n",
        "    id2idx = {int(aid): int(i) for i, aid in enumerate(article_ids)}\n",
        "    print('loaded articles_emb:', article_emb.shape, 'time:', round(time.time() - t0, 2), 's')\n",
        "else:\n",
        "    print('[info] skip loading articles_emb.csv (LOAD_EMB=False)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 召回 Serving（Two-Tower / SASRec / Popular）\n",
        "\n",
        "召回阶段的“线上形态”通常是：\n",
        "- **用户塔**：输入用户特征/序列 → 输出 user embedding\n",
        "- **向量检索**：FAISS/ANN → TopK item_id\n",
        "- **过滤规则**：去掉已看、黑名单、时间窗口等\n",
        "\n",
        "下面我们提供 3 种候选生成：\n",
        "- `two_tower`：Notebook 7 训练的双塔 + FAISS\n",
        "- `sasrec`：Notebook 9 训练的 SASRec user encoder + FAISS\n",
        "- `popular`：冷启动兜底\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class IdMap:\n",
        "    name: str\n",
        "    classes_: np.ndarray\n",
        "    offset: int = 1\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return int(len(self.classes_) + self.offset)\n",
        "\n",
        "    def transform(self, values: np.ndarray) -> np.ndarray:\n",
        "        values = np.asarray(values)\n",
        "        idx = pd.Index(self.classes_)\n",
        "        # get_indexer returns -1 for missing\n",
        "        pos = idx.get_indexer(values)\n",
        "        out = np.where(pos >= 0, pos + self.offset, 0).astype('int32')\n",
        "        return out\n",
        "\n",
        "\n",
        "def pad_left(seqs: List[List[int]], max_len: int, pad: int = 0) -> np.ndarray:\n",
        "    out = np.full((len(seqs), max_len), pad, dtype=np.int32)\n",
        "    for i, s in enumerate(seqs):\n",
        "        if not s:\n",
        "            continue\n",
        "        s = s[-max_len:]\n",
        "        out[i, -len(s):] = np.asarray(s, dtype=np.int32)\n",
        "    return out\n",
        "\n",
        "\n",
        "def load_two_tower_runtime() -> Optional[Tuple[Dict[str, IdMap], tf.keras.Model, faiss.Index]]:\n",
        "    two_dir = PROJECT_PATH / 'artifacts' / 'two_tower' / 'dssm_inbatch'\n",
        "    maps_path = two_dir / 'id_maps.pkl'\n",
        "    user_path = two_dir / 'user_tower.keras'\n",
        "    index_path = two_dir / 'faiss_index.bin'\n",
        "    if not (maps_path.exists() and user_path.exists() and index_path.exists()):\n",
        "        return None\n",
        "    id_maps = pickle.load(open(maps_path, 'rb'))\n",
        "    user_tower = tf.keras.models.load_model(user_path, compile=False)\n",
        "    index = faiss.read_index(str(index_path))\n",
        "    return id_maps, user_tower, index\n",
        "\n",
        "\n",
        "def load_sasrec_runtime() -> Optional[Tuple[IdMap, tf.keras.Model, faiss.Index]]:\n",
        "    seq_dir = PROJECT_PATH / 'artifacts' / 'sequence' / 'sasrec_inbatch'\n",
        "    map_path = seq_dir / 'item_id_map.pkl'\n",
        "    user_path = seq_dir / 'user_tower.keras'\n",
        "    index_path = seq_dir / 'faiss_index.bin'\n",
        "    if not (map_path.exists() and user_path.exists() and index_path.exists()):\n",
        "        return None\n",
        "    item_id_map = pickle.load(open(map_path, 'rb'))\n",
        "    user_tower = tf.keras.models.load_model(user_path, compile=False)\n",
        "    index = faiss.read_index(str(index_path))\n",
        "    return item_id_map, user_tower, index\n",
        "\n",
        "\n",
        "def retrieve_two_tower(user_id: int, topk: int = 100, max_seq_len: int = 30) -> pd.DataFrame:\n",
        "    runtime = load_two_tower_runtime()\n",
        "    if runtime is None:\n",
        "        print('[skip] missing two_tower artifacts, run 7.two_tower_recall.ipynb first')\n",
        "        return pd.DataFrame(columns=['user_id', 'article_id', 'recall_score', 'recall_rank'])\n",
        "    id_maps, user_tower, index = runtime\n",
        "    user_id_map: IdMap = id_maps['user']\n",
        "    item_id_map: IdMap = id_maps['item']\n",
        "\n",
        "    hist = user_hist.get(int(user_id), [])\n",
        "    if not hist:\n",
        "        return retrieve_popular(user_id=user_id, topk=topk)\n",
        "\n",
        "    u_enc = user_id_map.transform(np.asarray([int(user_id)], dtype=np.int64))\n",
        "    h_enc = item_id_map.transform(np.asarray(hist, dtype=np.int64)).tolist()\n",
        "    X_hist = pad_left([h_enc], max_len=max_seq_len)\n",
        "\n",
        "    feats = {'user_id': u_enc, 'hist_article_id': X_hist}\n",
        "    u_vec = user_tower.predict(feats, verbose=0).astype('float32')\n",
        "    faiss.normalize_L2(u_vec)\n",
        "\n",
        "    search_k = int(topk + max_seq_len + 10)\n",
        "    D, I = index.search(u_vec, search_k)\n",
        "    hist_set = set(hist)\n",
        "\n",
        "    rows = []\n",
        "    for score, aid in zip(D[0].tolist(), I[0].tolist()):\n",
        "        aid = int(aid)\n",
        "        if aid <= 0:\n",
        "            continue\n",
        "        if aid in hist_set:\n",
        "            continue\n",
        "        rows.append((int(user_id), aid, float(score)))\n",
        "        if len(rows) >= topk:\n",
        "            break\n",
        "\n",
        "    out = pd.DataFrame(rows, columns=['user_id', 'article_id', 'recall_score'])\n",
        "    out['recall_rank'] = np.arange(1, len(out) + 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "def retrieve_sasrec(user_id: int, topk: int = 100, max_seq_len: int = 50) -> pd.DataFrame:\n",
        "    runtime = load_sasrec_runtime()\n",
        "    if runtime is None:\n",
        "        print('[skip] missing sasrec artifacts, run 9.sequence_modeling.ipynb first')\n",
        "        return pd.DataFrame(columns=['user_id', 'article_id', 'recall_score', 'recall_rank'])\n",
        "    item_id_map, user_tower, index = runtime\n",
        "\n",
        "    hist = user_hist.get(int(user_id), [])\n",
        "    if not hist:\n",
        "        return retrieve_popular(user_id=user_id, topk=topk)\n",
        "\n",
        "    h_enc = item_id_map.transform(np.asarray(hist, dtype=np.int64)).tolist()\n",
        "    X_seq = pad_left([h_enc], max_len=max_seq_len)\n",
        "\n",
        "    u_vec = user_tower.predict({'seq_ids': X_seq}, verbose=0).astype('float32')\n",
        "    faiss.normalize_L2(u_vec)\n",
        "\n",
        "    search_k = int(topk + max_seq_len + 10)\n",
        "    D, I = index.search(u_vec, search_k)\n",
        "    hist_set = set(hist)\n",
        "\n",
        "    rows = []\n",
        "    for score, aid in zip(D[0].tolist(), I[0].tolist()):\n",
        "        aid = int(aid)\n",
        "        if aid <= 0:\n",
        "            continue\n",
        "        if aid in hist_set:\n",
        "            continue\n",
        "        rows.append((int(user_id), aid, float(score)))\n",
        "        if len(rows) >= topk:\n",
        "            break\n",
        "\n",
        "    out = pd.DataFrame(rows, columns=['user_id', 'article_id', 'recall_score'])\n",
        "    out['recall_rank'] = np.arange(1, len(out) + 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "def retrieve_popular(user_id: int, topk: int = 100) -> pd.DataFrame:\n",
        "    hist = set(user_hist.get(int(user_id), []))\n",
        "    rows = []\n",
        "    for i, aid in enumerate(popular_items[: max(0, topk * 2)]):\n",
        "        aid = int(aid)\n",
        "        if aid in hist:\n",
        "            continue\n",
        "        # popular 没有自然相似度分数，这里用一个递减分数占位\n",
        "        rows.append((int(user_id), aid, float(1.0 / (i + 1))))\n",
        "        if len(rows) >= topk:\n",
        "            break\n",
        "    out = pd.DataFrame(rows, columns=['user_id', 'article_id', 'recall_score'])\n",
        "    out['recall_rank'] = np.arange(1, len(out) + 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "def retrieve_candidates(user_id: int, method: str = 'two_tower', topk: int = 100) -> pd.DataFrame:\n",
        "    method = method.lower()\n",
        "    if method == 'two_tower':\n",
        "        return retrieve_two_tower(user_id=user_id, topk=topk)\n",
        "    if method == 'sasrec':\n",
        "        return retrieve_sasrec(user_id=user_id, topk=topk)\n",
        "    if method == 'popular':\n",
        "        return retrieve_popular(user_id=user_id, topk=topk)\n",
        "    raise ValueError(f'unknown retrieval method: {method}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) 构建排序特征（复用离线特征定义）\n",
        "\n",
        "这里复用基础版 notebook 5 的特征逻辑，保证一致性：\n",
        "- merge：`user_features` + `user_last_click` + `item_features`\n",
        "- 计算：`is_same_category` / `item_age_hours` / `time_gap_hours` / `emb_sim_last`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_emb_sim_last(df: pd.DataFrame) -> np.ndarray:\n",
        "    if emb_matrix is None or id2idx is None:\n",
        "        return np.zeros(len(df), dtype='float32')\n",
        "    cand_idx = df['article_id'].map(id2idx)\n",
        "    last_idx = df['last_click_article_id'].map(id2idx)\n",
        "    mask = cand_idx.notna() & last_idx.notna()\n",
        "    sim = np.zeros(len(df), dtype='float32')\n",
        "    if mask.any():\n",
        "        a = emb_matrix[cand_idx[mask].astype(int)]\n",
        "        b = emb_matrix[last_idx[mask].astype(int)]\n",
        "        sim[mask.values] = (a * b).sum(axis=1)\n",
        "    return sim\n",
        "\n",
        "\n",
        "def build_ranking_features(cand_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if cand_df.empty:\n",
        "        return cand_df\n",
        "    df = (\n",
        "        cand_df.merge(user_features, on='user_id', how='left')\n",
        "        .merge(user_last_click, on='user_id', how='left')\n",
        "        .merge(item_features, on='article_id', how='left')\n",
        "    )\n",
        "\n",
        "    df['is_same_category'] = (df['category_id'] == df['user_top_category']).astype(int)\n",
        "    df['item_age_hours'] = (df['last_click_timestamp'] - df['created_at_ts']) / 3600_000\n",
        "    df['time_gap_hours'] = (df['last_click_timestamp'] - df['item_last_click_ts']) / 3600_000\n",
        "    df[['item_age_hours', 'time_gap_hours']] = df[['item_age_hours', 'time_gap_hours']].fillna(0)\n",
        "\n",
        "    df['emb_sim_last'] = compute_emb_sim_last(df)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) 排序 Serving（LGBM / DeepFM / DIN）\n",
        "\n",
        "- **LGBMRanker**：基础版 notebook 6 的模型（最稳的 fallback）\n",
        "- **DeepFM / DIN**：Plus notebook 8 的深度排序模型（更贴近工业面试）\n",
        "\n",
        "DeepFM/DIN 的推理需要：\n",
        "- 和训练一致的 `scaler.pkl`（dense 标准化）\n",
        "- DeepFM 的 `deepfm_factorizers.pkl`（稀疏特征编码表）\n",
        "- DIN 的 `din_encoders.pkl`（user/item 的 id 编码）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_lgbm_ranker() -> Optional[lgb.Booster]:\n",
        "    path = PROJECT_PATH / 'lgb_ranker.txt'\n",
        "    if not path.exists():\n",
        "        return None\n",
        "    return lgb.Booster(model_file=str(path))\n",
        "\n",
        "\n",
        "def rank_with_lgbm(df: pd.DataFrame, topn: int = 10) -> pd.DataFrame:\n",
        "    booster = load_lgbm_ranker()\n",
        "    if booster is None or not feature_cols:\n",
        "        print('[skip] missing lgb_ranker.txt or feature_cols.pkl')\n",
        "        return df.head(topn)\n",
        "    X = df[feature_cols].fillna(0).values\n",
        "    s = booster.predict(X)\n",
        "    out = df.copy()\n",
        "    out['rank_score'] = s\n",
        "    out = out.sort_values('rank_score', ascending=False).head(topn)\n",
        "    return out\n",
        "\n",
        "\n",
        "def load_deep_ranking_artifacts():\n",
        "    art_dir = PROJECT_PATH / 'artifacts' / 'ranking' / 'deep_models'\n",
        "    deepfm_path = art_dir / 'deepfm.keras'\n",
        "    din_path = art_dir / 'din.keras'\n",
        "    scaler_path = art_dir / 'scaler.pkl'\n",
        "    deepfm_fac_path = art_dir / 'deepfm_factorizers.pkl'\n",
        "    din_enc_path = art_dir / 'din_encoders.pkl'\n",
        "    if not art_dir.exists():\n",
        "        return None\n",
        "    if not (scaler_path.exists() and deepfm_fac_path.exists() and din_enc_path.exists()):\n",
        "        return None\n",
        "    scaler = pickle.load(open(scaler_path, 'rb'))\n",
        "    deepfm_fac = pickle.load(open(deepfm_fac_path, 'rb'))\n",
        "    din_enc = pickle.load(open(din_enc_path, 'rb'))\n",
        "    deepfm = tf.keras.models.load_model(deepfm_path) if deepfm_path.exists() else None\n",
        "    din = tf.keras.models.load_model(din_path) if din_path.exists() else None\n",
        "    return {'dir': art_dir, 'scaler': scaler, 'deepfm_factorizers': deepfm_fac, 'din_encoders': din_enc, 'deepfm': deepfm, 'din': din}\n",
        "\n",
        "\n",
        "def encode_with_uniques(series: pd.Series, uniq_list: List[str]) -> np.ndarray:\n",
        "    idx = {v: i + 1 for i, v in enumerate(uniq_list)}\n",
        "    return np.asarray([idx.get(str(x), 0) for x in series.values], dtype=np.int32)\n",
        "\n",
        "\n",
        "def rank_with_deepfm(df: pd.DataFrame, topn: int = 10) -> pd.DataFrame:\n",
        "    art = load_deep_ranking_artifacts()\n",
        "    if art is None or art['deepfm'] is None:\n",
        "        print('[skip] missing deepfm artifacts, run 8.deep_ranking.ipynb first')\n",
        "        return df.head(topn)\n",
        "\n",
        "    sparse_cols = art['deepfm_factorizers']['sparse_cols']\n",
        "    dense_cols = art['deepfm_factorizers']['dense_cols']\n",
        "    uniques = art['deepfm_factorizers']['uniques']\n",
        "    scaler = art['scaler']\n",
        "\n",
        "    inputs = {c: encode_with_uniques(df[c].fillna(0).astype(str), uniques[c]) for c in sparse_cols}\n",
        "    inputs['dense'] = scaler.transform(df[dense_cols].fillna(0).values.astype('float32'))\n",
        "\n",
        "    p = art['deepfm'].predict(inputs, batch_size=4096, verbose=0)\n",
        "    p = np.asarray(p).reshape(-1)\n",
        "    out = df.copy()\n",
        "    out['rank_score'] = p\n",
        "    out = out.sort_values('rank_score', ascending=False).head(topn)\n",
        "    return out\n",
        "\n",
        "\n",
        "def rank_with_din(df: pd.DataFrame, topn: int = 10, max_hist_len: int = 50) -> pd.DataFrame:\n",
        "    art = load_deep_ranking_artifacts()\n",
        "    if art is None or art['din'] is None:\n",
        "        print('[skip] missing din artifacts, run 8.deep_ranking.ipynb first')\n",
        "        return df.head(topn)\n",
        "    scaler = art['scaler']\n",
        "    raw_to_user_enc = art['din_encoders']['raw_to_user_enc']\n",
        "    raw_to_item_enc = art['din_encoders']['raw_to_item_enc']\n",
        "\n",
        "    # DIN 输入：user_id_enc / hist_items_enc / target_item_enc / dense\n",
        "    user_id = int(df['user_id'].iloc[0])\n",
        "    hist = user_hist.get(user_id, [])[-max_hist_len:]\n",
        "    hist_enc = [raw_to_item_enc.get(int(x), 0) for x in hist]\n",
        "    hist_mat = pad_left([hist_enc] * len(df), max_len=max_hist_len)\n",
        "\n",
        "    user_enc = np.full(len(df), raw_to_user_enc.get(user_id, 0), dtype=np.int32)\n",
        "    item_enc = np.asarray([raw_to_item_enc.get(int(x), 0) for x in df['article_id'].values], dtype=np.int32)\n",
        "\n",
        "    dense_cols = ['recall_score','recall_rank','user_click_count','user_unique_items','item_click_count','words_count','item_age_hours','time_gap_hours','emb_sim_last','is_same_category']\n",
        "    dense = scaler.transform(df[dense_cols].fillna(0).values.astype('float32'))\n",
        "\n",
        "    inputs = {\n",
        "        'user_id': user_enc,\n",
        "        'hist_items': hist_mat,\n",
        "        'target_item': item_enc,\n",
        "        'dense': dense,\n",
        "    }\n",
        "    p = art['din'].predict(inputs, batch_size=4096, verbose=0)\n",
        "    p = np.asarray(p).reshape(-1)\n",
        "    out = df.copy()\n",
        "    out['rank_score'] = p\n",
        "    out = out.sort_values('rank_score', ascending=False).head(topn)\n",
        "    return out\n",
        "\n",
        "\n",
        "def rerank(df: pd.DataFrame, ranker: str = 'lgbm', topn: int = 10) -> pd.DataFrame:\n",
        "    ranker = ranker.lower()\n",
        "    if df.empty:\n",
        "        return df\n",
        "    if ranker == 'lgbm':\n",
        "        return rank_with_lgbm(df, topn=topn)\n",
        "    if ranker == 'deepfm':\n",
        "        return rank_with_deepfm(df, topn=topn)\n",
        "    if ranker == 'din':\n",
        "        return rank_with_din(df, topn=topn)\n",
        "    raise ValueError(f'unknown ranker: {ranker}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) End-to-End：`recommend(user_id)`\n",
        "\n",
        "这个函数体现了典型线上链路：\n",
        "1) 召回 TopK\n",
        "2) 构建特征\n",
        "3) 排序 TopN\n",
        "\n",
        "你可以在面试里用它解释：\n",
        "- 召回/排序分工\n",
        "- Feature 的 offline/online 一致性\n",
        "- 冷启动兜底策略\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend(user_id: int, recall: str = 'two_tower', ranker: str = 'lgbm', topk: int = 100, topn: int = 10) -> pd.DataFrame:\n",
        "    cand = retrieve_candidates(user_id=user_id, method=recall, topk=topk)\n",
        "    feats = build_ranking_features(cand)\n",
        "    ranked = rerank(feats, ranker=ranker, topn=topn)\n",
        "    # 方便展示：拼回文章元信息\n",
        "    if not articles.empty:\n",
        "        ranked = ranked.merge(articles, on='article_id', how='left', suffixes=('', '_meta'))\n",
        "    return ranked\n",
        "\n",
        "\n",
        "# 选择一个有历史的用户做 demo\n",
        "if not valid_last.empty:\n",
        "    demo_user = int(valid_last['user_id'].sample(1, random_state=42).iloc[0])\n",
        "elif user_hist:\n",
        "    demo_user = int(next(iter(user_hist.keys())))\n",
        "else:\n",
        "    demo_user = 1\n",
        "\n",
        "print('demo_user:', demo_user, 'hist_len:', len(user_hist.get(demo_user, [])))\n",
        "\n",
        "# 你可以切换 recall={'two_tower','sasrec','popular'}，ranker={'lgbm','deepfm','din'}\n",
        "res = recommend(demo_user, recall='two_tower', ranker='lgbm', topk=100, topn=10)\n",
        "res[['user_id','article_id','recall_score','recall_rank','rank_score','category_id','words_count']].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) 导出（SavedModel / 可选 ONNX）与简要性能测试\n",
        "\n",
        "工业里常见做法：\n",
        "- 训练产物：`*.keras` / SavedModel\n",
        "- 检索产物：item embedding + ANN index（FAISS/HNSW/ScaNN）\n",
        "- 推理服务：TensorFlow Serving / 自研服务 + 特征服务\n",
        "\n",
        "下面演示：\n",
        "- 如何把 Keras 模型导出为 SavedModel\n",
        "- 如果环境装了 `tf2onnx`，可选导出 ONNX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SERVING_DIR = PROJECT_PATH / 'artifacts' / 'serving'\n",
        "SERVING_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 示例：导出 DeepFM / DIN（如果已经训练过）\n",
        "art = load_deep_ranking_artifacts()\n",
        "if art is None:\n",
        "    print('[skip] deep ranking artifacts not found, run 8.deep_ranking.ipynb first')\n",
        "else:\n",
        "    if art['deepfm'] is not None:\n",
        "        export_path = SERVING_DIR / 'deepfm_savedmodel'\n",
        "        art['deepfm'].save(export_path)\n",
        "        print('saved:', export_path)\n",
        "    if art['din'] is not None:\n",
        "        export_path = SERVING_DIR / 'din_savedmodel'\n",
        "        art['din'].save(export_path)\n",
        "        print('saved:', export_path)\n",
        "\n",
        "# 可选：ONNX（依赖缺失时直接跳过）\n",
        "try:\n",
        "    import tf2onnx  # type: ignore\n",
        "except Exception as e:\n",
        "    tf2onnx = None\n",
        "\n",
        "if tf2onnx is None:\n",
        "    print('[optional] tf2onnx not installed, skip ONNX export')\n",
        "else:\n",
        "    if art is not None and art['din'] is not None:\n",
        "        spec = (\n",
        "            tf.TensorSpec((None,), tf.int32, name='user_id'),\n",
        "            tf.TensorSpec((None, 50), tf.int32, name='hist_items'),\n",
        "            tf.TensorSpec((None,), tf.int32, name='target_item'),\n",
        "            tf.TensorSpec((None, 10), tf.float32, name='dense'),\n",
        "        )\n",
        "        onnx_path = SERVING_DIR / 'din.onnx'\n",
        "        _ = tf2onnx.convert.from_keras(art['din'], input_signature=spec, output_path=str(onnx_path))\n",
        "        print('saved:', onnx_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

